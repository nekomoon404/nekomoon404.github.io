{"meta":{"title":"nekomoon的个人小站","subtitle":"","description":"","author":"nekomoon","url":"http://nekomoon404.github.io","root":"/"},"pages":[{"title":"about","date":"2020-01-17T12:16:07.000Z","updated":"2020-01-17T12:45:38.714Z","comments":false,"path":"about/index.html","permalink":"http://nekomoon404.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-01-17T12:15:51.000Z","updated":"2020-01-17T12:45:26.408Z","comments":false,"path":"categories/index.html","permalink":"http://nekomoon404.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-01-17T12:15:21.000Z","updated":"2020-01-17T12:45:12.630Z","comments":false,"path":"tags/index.html","permalink":"http://nekomoon404.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"算法基础（5）","slug":"算法基础（5）","date":"2020-10-07T12:31:03.000Z","updated":"2020-10-09T09:20:35.369Z","comments":true,"path":"2020/10/07/算法基础（5）/","link":"","permalink":"http://nekomoon404.github.io/2020/10/07/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%EF%BC%885%EF%BC%89/","excerpt":"","text":"Trie树Trie树是用来快速存储和查找字符串集合的数据结构。 Trie树的基础知识可以参考： Trie树例题：Trie字符串统计（Acwing 835） 维护一个字符串集合，支持两种操作： “I x”向集合中插入一个字符串x； “Q x”询问一个字符串在集合中出现了多少次。 共有N个操作，输入的字符串总长度不超过 $10^5$，字符串仅包含小写英文字母。 输入格式：第一行包含整数N，表示操作数。接下来N行，每行包含一个操作指令，指令为”I x”或”Q x”中的一种。 输出格式：对于每个询问指令”Q x”，都要输出一个整数作为结果，表示x在集合中出现的次数。每个结果占一行。 数据范围：$1 \\le N \\le 2 \\times 10^4$。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include&lt;iostream&gt;using namespace std;const int N = 100010;//son[N][26]存Trie树中每个节点的儿子//cnt[]表示以当前节点为结尾的单词的个数//idx表示当前用到的下标，注意：下标是0的点，即是根节点，又是空节点int son[N][26], cnt[N], idx;char str[N];void insert(char str[]) //插入操作：在当前的Trie树中插入一个字符串&#123; int p = 0; for(int i = 0; str[i]; i++) &#123; int u = str[i] - 'a'; if(!son[p][u]) son[p][u] = ++ idx; p = son[p][u]; &#125; cnt[p] ++;&#125;int query(char str[]) //返回字符串出现的次数&#123; int p = 0; for(int i = 0; str[i]; i++) &#123; int u = str[i] - 'a'; if(!son[p][u]) return 0; p = son[p][u]; &#125; return cnt[p];&#125;int main()&#123; int n; scanf(\"%d\", &amp;n); while(n--) &#123; char op[2]; scanf(\"%s%s\", op, str); if(op[0] == 'I') insert(str); else printf(\"%d\\n\", query(str)); &#125; return 0;&#125; Trie树例题2：最大异或对 在给定的N个整数$A_1，A_2……A_N$中选出两个进行xor（异或）运算，得到的结果最大是多少？ 输入格式：第一行输入一个整数N。第二行输入N个整数$A_1$～$A_N$。 输出格式：输出一个整数表示答案。 数据范围：$1 \\le N \\le 10^5$，$0 \\le A_i \\le 2^31$。 异或运算是按位运算（二进制），相同得0，不同得1。同样先考虑暴力做法，可以用两重循环来解决。 123456int res = 0;for(int i = 0; i &lt; n; i++) //枚举第一个数&#123; for(int j = 0; j &lt; i; j++) //枚举第二个数 res = max(res, a[i] ^ a[j]);&#125; 接下来考虑用Trie树来优化内层循环，即从$a_0$到$ a_{i-1}$中找到与$a_i$异或最大的数。本题的数据范围是$0 \\le a_i \\le 2^31$，因此每个数可以看出一个31位的二进制串，假设$a_i=101110…1$，要找到与$a_i$异或最大的数，就要异或结果高位是1比较好，因此按$a_i$的位数从左往右找，在$a_0$到$ a_{i-1}$中，先找第30位是0的数，再从这部分数中找第29位是1的数，依次类推。在Trie树中，对于每个$a_i$，先查找再插入。 Trie树 不光可以存储字符串，也可以存储整数，也可以存储二进制数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;const int N = 100010, M = 31 * N;int n;int a[N];int son[M][2], idx;void insert(int x) //在Trie树中插入一个数&#123; int p = 0; for(int i = 30; i &gt;= 0; i--) //从最高位开始存 &#123; int u = x &gt;&gt; i &amp; 1; //取二进制的每一位数 if(!son[p][u]) son[p][u] = ++ idx; p = son[p][u]; &#125;&#125;int query(int x) //在当前的树中找出与x异或结果最大的数&#123; int p = 0, res = 0; for(int i = 30; i &gt;= 0; i--) &#123; int u = x &gt;&gt; i &amp; 1; if(son[p][!u]) //优先寻找是否有与当前位数相反的子节点 &#123; p = son[p][!u]; res = res * 2 + !u; &#125; else &#123; p = son[p][u]; res = res * 2 + u; &#125; &#125; return res;&#125;int main()&#123; scanf(\"%d\", &amp;n); int res = 0; for(int i = 0; i &lt; n; i++) &#123; scanf(\"%d\", &amp;a[i]); insert(a[i]); //这里先插入再查找是为了避免处理边界问题，少写判断，即一开始树是空的 int t = query(a[i]); res = max(res, a[i] ^ t); &#125; printf(\"%d\\n\", res); return 0;&#125; 并查集并查集的作用： 将两个集合合并 询问两个元素是否在一个集合当中 并查集可以在近乎$O(1)$的时间内完成这两个操作。 基本思想：每一个集合用一棵树（不一定是二叉树）来维护，树根的编号就是整个集合的编号，对于每一点都存储其父节点是谁，p[x]表示x的父节点。 问题1：如何判断树根： if(p[x] == x) 问题2：如何求x的集合编号：while(p[x] != x) x = p[x]; 问题3：如何合并两个集合：设p[x]是x的集合编号，p[y]是y的集合编号，p[x]=y 针对问题2的优化：路径压缩。一旦从一个点往上找找到了根节点，就把这条路径上所有的点都指向根节点。 并查集例题1（Acwing 836）： 一共有n个数，编号是1~n，最开始每个数各自在一个集合中。 现在要进行m个操作，操作共有两种： “M a b”，将编号为a和b的两个数所在的集合合并，如果两个数已经在同一个集合中，则忽略这个操作 “Q a b”，询问编号为a和b的两个数是否在同一个集合中； 输入格式：第一行输入整数n和m。接下来m行，每行包含一个操作指令，指令为“M a b”或“Q a b”中的一种。 数据范围：$1 \\le n, m \\le 10^5$ 1234567891011121314151617181920212223242526272829303132333435#include&lt;iostream&gt;using namespace std;const int N = 100010;int n, m;int p[N];int find(int x) //返回x的祖宗节点 + 路劲压缩&#123; if(p[x] != x) p[x] = find(p[x]); return p[x];&#125;int main()&#123; scanf(\"%d%d\", &amp;n, &amp;m); for(int i = 0; i &lt; n; i++) p[i] = i; //最开始每个数各自在一个集合中 while(m--) &#123; char op[2]; int a, b; scanf(\"%s%d%d\", op, &amp;a, &amp;b); if(op[0] == 'M') p[find(a)] = find(b); //将数a和b所在的两个集合合并，即让find(a)的父节点是find(b) else &#123; if(find(a) == find(b)) puts(\"Yes\"); //询问数a和b是否在同一个集合内，即find(a)是否等于find(b) else puts(\"No\"); &#125; &#125; return 0;&#125; 并查集例题2：连通块中点的数量（Acwing 837） 给定一个包含n个点（编号为1~n）的无向图，初始时图中没有边。现在要进行m个操作，操作共有三种： “C a b”，在点a和点b之间连一条边，a和b可能相等； “Q1 a b”，询问点a和点b是否在同一个连通块中，a和b可能相等； “Q2 a”，询问点a所在连通块中点的数量。 输入格式：第一行输入整数n和m。接下来m行，每行包含一个操作指令，指令为“C a b”，“Q1 a b”或“Q2 a”中的一种。 数据范围：$1 \\le n, m \\le 10^5$ 可以发现这道题的前两个操作和并查集是一样的，我们可以用一个集合维护连通块，一个连通块中的点就在一个集合当中。当在两个连通块之间连一条边时，起到的作用就是把两个集合合并。额外的操作是统计一个集合中点的数量。 用cnt[N]表示每个集合中点的数量，我们只保证根节点的cnt[]是有意义的。将数a和b所在的两个集合合并时，即让find(a)的父节点是find(b)，然后让find(b)的cnt等于find(a)的cnt加上find(b)的cnt。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include&lt;iostream&gt;using namespace std;const int N = 100010;int n, m;int p[N], cnt[N]; //size[]存放每个集合中点的数量int find(int x) //返回x的祖宗节点 + 路劲压缩&#123; if(p[x] != x) p[x] = find(p[x]); return p[x];&#125;int main()&#123; scanf(\"%d%d\", &amp;n, &amp;m); for(int i = 0; i &lt; n; i++) //最开始每个数各自在一个集合中 &#123; p[i] = i; cnt[i] = 1; &#125; while(m--) &#123; char op[4]; int a, b; scanf(\"%s\", op); if(op[0] == 'C') &#123; scanf(\"%d%d\", &amp;a, &amp;b); if(find(a) == find(b)) continue; //如果a和b在同一个集合中，那么就忽略这次合并操作 cnt[find(b)] += cnt[find(a)]; //合并集合时，更新集合中点的数量，即find(b)的size[] p[find(a)] = find(b); //将数a和b所在的两个集合合并，即让find(a)的父节点是find(b) &#125; else if(op[1] == '1') &#123; scanf(\"%d%d\", &amp;a, &amp;b); if(find(a) == find(b)) puts(\"Yes\"); //询问数a和b是否在同一个集合内，即find(a)是否等于find(b) else puts(\"No\"); &#125; else &#123; scanf(\"%d\", &amp;a); printf(\"%d\\n\", cnt[find(a)]); //询问a所在的集合中点的数量 &#125; &#125; return 0;&#125; 并查集例题3：食物链（Acwing 240） 动物王国中有三类动物A,B,C，这三类动物的食物链构成了有趣的环形。A吃B， B吃C，C吃A。现有N个动物，以1－N编号。每个动物都是A,B,C中的一种，但是我们并不知道它到底是哪一种。有人用两种说法对这N个动物所构成的食物链关系进行描述： 第一种说法是”1 X Y”，表示X和Y是同类。 第二种说法是”2 X Y”，表示X吃Y。 此人对N个动物，用上述两种说法，一句接一句地说出K句话，这K句话有的是真的，有的是假的。当一句话满足下列三条之一时，这句话就是假话，否则就是真话。 1） 当前的话与前面的某些真的话冲突，就是假话； 2） 当前的话中X或Y比N大，就是假话； 3） 当前的话表示X吃X，就是假话。 你的任务是根据给定的N和K句话，输出假话的总数。 输入格式：第一行是两个整数N和K，以一个空格分隔。以下K行每行是三个正整数 D，X，Y，两数之间用一个空格隔开，其中D表示说法的种类。若D=1，则表示X和Y是同类。若D=2，则表示X吃Y。 输出格式：只有一个整数，表示假话的数目。 数据范围：$1 \\le N \\le 50000, 0 \\le K \\le100000$。 通过确定每个点与根节点之间的关系，来确定任意两个点之间的关系。由于三种动物的关系是循环被吃，就用每个点到根节点之间的距离来表示它与根节点之间的关系。如果一个点到根节点的距离是1，表示它可以吃根节点；如果一个点到根节点的距离是2，表示它被根节点吃（1吃根，2吃1，根吃2）；如果一个点到根节点的距离是3，表示它和根节点是同类…….如此类推，每3个 一循环。因此可以把集合中所有的点归为三类，用并查集维护每个点到根节点的距离，按点到根节点的距离对3取模： 余1——可以吃根节点； 余2——可以被根节点吃； 余0——与根节点是同类。 在用并查集维护时，每个点存的是其到父节点的距离，做路径压缩时，就更新为其到根节点的距离。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include&lt;iostream&gt;using namespace std;const int N = 50010;int n, m;int p[N], d[N];int find(int x)&#123; if(p[x] != x) &#123; int t = find(p[x]); d[x] += d[p[x]]; p[x] = t; &#125; return p[x];&#125;int main()&#123; scanf(\"%d%d\", &amp;n, &amp;m); for(int i = 1; i &lt;= n; i++) p[i] = i; //有n个动物，以1-N编号 int res = 0; while(m--) &#123; int t, x, y; scanf(\"%d%d%d\", &amp;t, &amp;x, &amp;y); if(x &gt; n || y &gt; n) res ++; //如果x大于n，或者y大于n，则一定是假话； else &#123; int px = find(x), py = find(y); //先找到x和y的根节点 if(t == 1) &#123; if(px == py &amp;&amp; (d[x] - d[y]) % 3) res ++; //当x 和 y 在同一个集合内时，x，y到根节点的距离模3不相等时一定是假话 else if(px != py) //当x 和 y不在同一个集合内时，就要合并,q且要让两者是同类（这时还没法判断，因为不在一个集合内） &#123; p[px] = py; //不妨让x的根节点的父节点 等于 y的根节点的父节点 d[px] = d[y] - d[x]; //这样(d[x] + d[px] - d[y]) % 3就等于0了，即x和y是同类 &#125; &#125; else &#123; if(px == py &amp;&amp; (d[x] - d[y] - 1) % 3) res ++; //当x和y在同一个集合内时，x到根节点距离模3比y的多1就是真话，否则就是假话 else if(px != py) //当x和y不在同一个集合内时，合并集合，更新距离 &#123; p[px] = py; d[px] = d[y] - d[x] + 1; &#125; &#125; &#125; &#125; printf(\"%d\\n\", res); return 0;&#125; 堆堆，要支持的操作： 插入一个数 求集合当中的最小值 删除最小值 删除任意一个元素 修改任意一个元素 堆是一棵完全二叉树。小根堆：每个点的值都小于等于左右儿子的。 堆的基础知识可以参考：数据结构与算法（19）优先级队列 完全二叉树的基础知识可以参考： 用数组模拟堆，堆的存储方式：用一个一维数组，1号点是根节点（下标从1开始），节点x的左儿子是2x，右儿子是2x+1。有两个基本操作：down(x)，把一个节点往下移，如果把一个节点的值变大了，它有可能就要往下移；up(x)，把一个节点往上移，如果把一个数变小了，它有可能就要往上移 。用这两个基本操作就可以实现上面的五个操作： 插入一个数：heap[++ size] = x; up[x]; 求集合当中的最小值：heap[1]; 删除最小值：（用堆的最后一个元素覆盖掉堆顶元素）heap[1] = heap[size]; size--; down(1); 删除任意一个元素：heap[k] = heap[size]; size --; down(k); up(k);（每次donw和up只会执行一个） 修改任意一个元素：heap[k] = x; down(k); up(x); 堆例题1：堆排序（Acwing 838） 输入一个长度为n的整数数列，从小到大输出前m小的数。 输入格式：第一行包含整数n和m。第二行包含n个整数，表示整数数列。 输出格式：共一行，包含m个整数，表示整数数列中前m小的数。 数据范围：$1 \\le m, n \\le 10^5$，$1 \\le 数列中元素 \\le 10^9$ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;const int N = 100010;int n, m;int h[N], cnt;void down(int u)&#123; int t = u; //t表示当前节点u，它的左右孩子，三者之间的最小值的下标 if(u * 2 &lt;= cnt &amp;&amp; h[u * 2] &lt; h[t]) t = u * 2; //t和左孩子比较 if(u * 2 + 1 &lt;= cnt &amp;&amp; h[u * 2 + 1] &lt; h[t]) t = u * 2 + 1; //t和右孩子比较 if(u != t) //当前节点不是最小值时，down操作要求它与左右孩子中的最小值交换 &#123; swap(h[u], h[t]); down(t); &#125;&#125;void up(int u)&#123; while(u / 2 &amp;&amp; h[u / 2] &gt; h[u]) &#123; swap(h[u / 2], h[u]); u /= 2; &#125;&#125; int main()&#123; scanf(\"%d%d\", &amp;n, &amp;m); for(int i = 1; i &lt;= n; i++) scanf(\"%d\", &amp;h[i]); //下标从1开始 cnt = n; //建堆，后n / 2 个元素是在最低层的，它们没有左右孩子，保持不动就行；让i = n / 2开始倒着进行down操作即可建堆 //从 i = n / 2 开始，是保证每次down的时候，下面的孩子节点已经是堆了；这样建堆的时间复杂度是O(n) for(int i = n / 2; i ; i--) down(i); while(m--) &#123; printf(\"%d \", h[1]); //取堆顶元素，即堆中的最小值 //删除堆顶元素 h[1] = h[cnt]; cnt --; down(1); &#125; return 0;&#125; 堆例题2：模拟堆（Acwing 839） 维护一个集合，初始时集合为空，支持如下几种操作： “I x”，插入一个数x； “PM”，输出当前集合中的最小值； “DM”，删除当前集合中的最小值（数据保证此时的最小值唯一）； “D k”，删除第k个插入的数； “C k x”，修改第k个插入的数，将其变为x； 输入格式：第一行包含整数N。接下来N行，每行包含一个操作指令，操作指令为”I x”，”PM”，”DM”，”D k”或”C k x”中的一种。 输出格式：对于每个输出指令“PM”，输出一个结果，表示当前集合中的最小值。每个结果占一行。 数据范围：$1 \\le N \\le 10^5$，$-10^9 \\le x \\le 10^9$ 这道题的难点在于操作4和5，怎么快速地找到第k个插入的数。这就需要额外维护两个数组，用ph[k]存第k个插入的点在堆中的位置(下标)，hp[k]表示堆中的位置为k的点是第几个插入的点；如ph[j] = k, hp[k] = j; 可以理解为映射。这样例题1代码中普通的swap()操作就要改成这里的堆的特有的交换操作heap_swap()。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;string.h&gt;using namespace std;const int N = 100010;int h[N], cnt; int ph[N], hp[N]; //ph[k]存第k个插入的点在堆中的位置(下标)，hp[k]表示堆中的位置为k的点是第几个插入的点；如ph[j] = k, hp[k] = j; 可以理解为映射void heap_swap(int a, int b) //a，b是数在堆中的位置，这个需要注意一下&#123; swap(ph[hp[a]], ph[hp[b]]); swap(hp[a], hp[b]); swap(h[a], h[b]);&#125;void down(int u)&#123; int t = u; //t表示当前节点u，它的左右孩子，三者之间的最小值的下标 if(u * 2 &lt;= cnt &amp;&amp; h[u * 2] &lt; h[t]) t = u * 2; //t和左孩子比较 if(u * 2 + 1 &lt;= cnt &amp;&amp; h[u * 2 + 1] &lt; h[t]) t = u * 2 + 1; //t和右孩子比较 if(u != t) //当前节点不是最小值时，down操作要求它与左右孩子中的最小值交换 &#123; heap_swap(u, t); down(t); &#125;&#125;void up(int u)&#123; while(u / 2 &amp;&amp; h[u / 2] &gt; h[u]) &#123; heap_swap(u / 2, u); u /= 2; &#125;&#125; int main()&#123; int n, m = 0; //m记录插入了多少个数 scanf(\"%d\", &amp;n); while(n--) &#123; char op[5]; int k, x; scanf(\"%s\", op); if(!strcmp(op, \"I\")) //插入一个数x &#123; scanf(\"%d\", &amp;x); cnt ++; m ++; ph[m] = cnt, hp[cnt] = m; h[cnt] = x; up(cnt); &#125; else if(!strcmp(op, \"PM\")) printf(\"%d\\n\", h[1]); //输出集合中最小的数，即堆顶 else if(!strcmp(op, \"DM\")) //删除集合中最小的数，即堆顶 &#123; heap_swap(1, cnt); cnt --; down(1); &#125; else if(!strcmp(op, \"D\")) //删除插入的第k个数 &#123; scanf(\"%d\", &amp;k); k = ph[k]; heap_swap(k, cnt); cnt --; down(k); up(k); &#125; else //修改第k个插入的数，将其变为x &#123; scanf(\"%d%d\", &amp;k, &amp;x); k = ph[k]; h[k] = x; down(k); up(k); &#125; &#125; return 0;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"算法基础（4）","slug":"算法基础（4）","date":"2020-10-05T10:30:14.000Z","updated":"2020-10-07T12:28:52.689Z","comments":true,"path":"2020/10/05/算法基础（4）/","link":"","permalink":"http://nekomoon404.github.io/2020/10/05/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%EF%BC%884%EF%BC%89/","excerpt":"","text":"第二章 数据结构 链表 栈与队列 kmp 这节课主要讲如何用数组模拟链表，栈与队列。 链表我们知道链表可以通过结构体+指针来实现，但每次创建一个新节点就要通过new Node; 来实现，这一过程是很慢的，在做笔试题时一般不会采用这样的动态链表的方式，常用的是用数组来模拟链表，又分为两种： 单链表，其中在算法题中用的最多的是邻接表，它最主要的应用是存储图和树 双链表，主要作用是优化某些问题 关于链表的基础知识可以参考之前写的 Cpp基础（6）结构体与链表 数组模拟单链表：用两个数组e[N]和ne[N]，它们通过下标关联起来，下标从0开始，e[i]用来存放第i个节点的值，ne[i]用来存放第i个节点指向的next节点的下标，空节点的下标用-1来表示。 单链表例题（Acwing 826）： 实现一个单链表，链表初始为空，支持三种操作： (1) 向链表头插入一个数； (2) 删除第k个插入的数后面的数； (3) 在第k个插入的数后插入一个数 现在要对该链表进行M次操作，进行完所有操作后，从头到尾输出整个链表。 注意:题目中第k个插入的数并不是指当前链表的第k个数。例如操作过程中一共插入了n个数，则按照插入的时间顺序，这n个数依次为：第1个插入的数，第2个插入的数，…第n个插入的数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include&lt;iostream&gt;using namespace std;const int N = 100010;//head 表示头节点的下标//e[i] 表示节点i的值//ne[i] 表示节点i的next指针是多少（指向的下一个节点的下标）//idx 表示当前已经用到了哪个点int head, e[N], ne[N], idx;//初始化void init()&#123; head = -1; idx = 0;&#125;//将x插到头节点void add_to_head(int x)&#123; e[idx] = x, ne[idx] = head, head = idx ++;&#125;//将x插到下标是k的点的后面（下标是k表示第k+1个插入链表中的数，与其位置无关）void add(int k, int x)&#123; e[idx] = x, ne[idx] = ne[k], ne[k] = idx ++;&#125;//将下标是k的点后面的点从链表中删除void remove(int k)&#123; ne[k] = ne[ne[k]];&#125;int main()&#123; int m; cin &gt;&gt; m; init(); while( m-- ) &#123; int k, x; char op; cin &gt;&gt; op; if(op == 'H') &#123; cin &gt;&gt; x; add_to_head(x); &#125; else if(op == 'D') &#123; cin &gt;&gt; k; if(!k) head = ne[head]; //若要删除的节点是头节点 remove(k - 1); //下标从0开始，第k个插入的数下标是k-1 &#125; else &#123; cin &gt;&gt; k &gt;&gt; x; add(k - 1, x); &#125; &#125; for(int i = head; i != -1; i = ne[i]) cout &lt;&lt; e[i] &lt;&lt; ' '; cout &lt;&lt; endl; return 0;&#125; 双链表的每个节点有两个指针，一个指向前面的点 ，一个指向后面的点，用数组l[]存放节点左边的点的下标，用数组r[]存放节点右边的点的下标。这里我们偷个懒，让下标是0的点是head，让下标是1的点是tail。 双链表例题（Acwing 827） 实现一个双链表，双链表初始为空，支持5种操作： (1) 在最左侧插入一个数； (2) 在最右侧插入一个数； (3) 将第k个插入的数删除； (4) 在第k个插入的数左侧插入一个数； (5) 在第k个插入的数右侧插入一个数 现在要对该链表进行M次操作，进行完所有操作后，从左到右输出整个链表。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#include&lt;iostream&gt;using namespace std;const int N = 100010;int e[N], l[N], r[N], idx;//初始化void init()&#123; //0表示head， 1表示tail r[0] = 1, l[1] = 0; idx = 2; //第k的插入的数下标就是k + 1&#125;//在下标是k的点的右边插入xvoid add(int k, int x)&#123; e[idx] = x; l[idx] = k, r[idx] = r[k]; l[r[k]] = idx, r[k] = idx ++; //注意这里的顺序不能变&#125;//删除下标是k的节点void remove(int k)&#123; r[l[k]] = r[k]; l[r[k]] = l[k];&#125;int main()&#123; int m; cin &gt;&gt; m; init(); while(m --) &#123; string op; cin &gt;&gt; op; int k, x; if(op == \"L\") //注意这里要用双引号 &#123; cin &gt;&gt; x; add(0, x); &#125; else if(op == \"R\") &#123; cin &gt;&gt; x; add(l[1], x); &#125; else if(op == \"D\") &#123; cin &gt;&gt; k; remove(k + 1); &#125; else if(op == \"IL\") &#123; cin &gt;&gt; k &gt;&gt; x; add(l[k + 1], x); &#125; else &#123; cin &gt;&gt; k &gt;&gt; x; add(k + 1, x); &#125; &#125; for(int i = r[0]; i != 1; i = r[i]) cout &lt;&lt; e[i] &lt;&lt; ' '; cout &lt;&lt; endl; return 0;&#125; 栈与队列 栈：先进后出 队列：先进先出 栈与队列的基础知识可以参考之前写的 数据结构与算法（7）栈与队列 12345678910111213141516//****************栈const int N = 100010;int stk[N], tt;//读入stk[ ++ tt] = x;//弹出tt --;//判断栈是否为空if(tt &gt; 0) not empty;else empty;//取栈顶元素stk[tt]; 12345678910111213141516171819//******************队列//在队尾插入元素，在队头弹出元素int q[N], hh, tt = -1;//插入q[ ++ tt] = x;//弹出hh ++;//判断队列是否为空if(hh &lt;= tt) not empty;else empty;//取队头元素q[hh];//当然用数组模拟的队列还可以取队尾元素 q[tt]; 数组模拟栈例题（Acwing 828） 实现一个栈，栈初始为空，支持四种操作： (1) “push x” – 向栈顶插入一个数x； (2) “pop” – 从栈顶弹出一个数； (3) “empty” – 判断栈是否为空； (4) “query” – 查询栈顶元素。 现在要对栈进行M个操作，其中的每个操作3和操作4都要输出相应的结果。数据范围：$1 \\le M \\le 100000$，$1 \\le x \\le 10^9$。所有操作保证合法。 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;using namespace std;const int N = 100010;int m;int stk[N], tt;int main()&#123; cin &gt;&gt; m; while(m --) &#123; string op; int x; cin &gt;&gt; op; if(op == \"push\") &#123; cin &gt;&gt; x; stk[++ tt] = x; &#125; else if(op == \"pop\") tt --; else if(op == \"empty\") cout &lt;&lt; (tt ? \"NO\" : \"YES\") &lt;&lt; endl; else cout &lt;&lt; stk[tt] &lt;&lt; endl; &#125; return 0;&#125; 数组模拟队列例题（ACwing 829） 实现一个队列，队列初始为空，支持四种操作： (1) “push x” – 向队尾插入一个数x； (2) “pop” – 从队头弹出一个数； (3) “empty” – 判断队列是否为空； (4) “query” – 查询队头元素。 现在要对队列进行M个操作，其中的每个操作3和操作4都要输出相应的结果。数据范围：$1 \\le M \\le 100000$，$1 \\le x \\le 10^9$。所有操作保证合法。 12345678910111213141516171819202122232425262728#include&lt;iostream&gt;using namespace std;const int N = 100010;int m;int q[N], hh, tt = -1;int main()&#123; cin &gt;&gt; m; while(m --) &#123; string op; int x; cin &gt;&gt; op; if(op == \"push\") &#123; cin &gt;&gt; x; q[++ tt] = x; &#125; else if(op == \"pop\") hh ++; else if(op == \"empty\") cout &lt;&lt; (hh &lt;= tt ? \"NO\" : \"YES\") &lt;&lt; endl; else cout &lt;&lt; q[hh] &lt;&lt; endl; &#125; return 0;&#125; 单调栈与单调队列单调栈的常见题型：给定一个序列，求序列中每一个数左边离它最近的且比它小的数是多少。 例题：给定一个长度为N的整数数列，输出每个数左边第一个比它小的数，如果不存在则输出-1。 保证栈中的数是单调递增的，遍历数列中的数，对于数列中第i个数，若栈非空且栈顶元素大于等于它，就弹出栈顶，直到栈顶比它小，则栈顶即为所求；若栈空，说明不存所求。之后再把第i个数存入栈顶。 12345678910111213141516171819202122232425#include&lt;iostream&gt;using namespace std;const int N = 100010;int stk[N], tt;int main()&#123; int n; cin &gt;&gt; n; //输入数据比较多时还是用scanf比较快 while(n --) &#123; int x; cin &gt;&gt; x; while(tt &amp;&amp; stk[tt] &gt;= x) tt --; if(tt) cout &lt;&lt; stk[tt] &lt;&lt; ' '; else cout &lt;&lt; -1 &lt;&lt; ' '; stk[++ tt] = x; &#125; return 0;&#125; 对于每个元素，最多只会进栈一次出栈一次，因此时间复杂度为$O(n)$。 单调队列的常见题型：求滑动窗口中的最大值/最小值 例题：滑动窗口（Acwing 154） 遍历序列中的元素，i表示滑动窗口右端点元素的下标，用一个单调队列来维护当前的滑动窗口，保证队列中的元素是单调递增的，即每插入一个新元素，便判断队尾是否大于等于它，若是就弹出队尾，再把新元素插入队尾，这样每次求滑动窗口中的最小值只需取出队头即可。求滑动窗口中的最大值同理。 123456789101112131415161718192021222324252627282930313233343536373839#include&lt;iostream&gt;using namespace std;int n, k;const int N = 1000010;int a[N], q[N]; //单调队列q[N]，存放下标int main()&#123; scanf(\"%d%d\", &amp;n, &amp;k); for(int i = 0; i &lt; n; i++) scanf(\"%d\", &amp;a[i]); //求滑动窗口中的最小值 int hh = 0, tt = -1; //队头和队尾 for(int i = 0; i &lt; n; i++) &#123; //判断队头是否已经弹出窗口 if(hh &lt;= tt &amp;&amp; i - k + 1 &gt; q[hh]) hh++; //每次只会出现一次，因此可以用if while(hh &lt;= tt &amp;&amp; a[q[tt]] &gt;= a[i]) tt--; q[++ tt] = i; if(i &gt;= k - 1) printf(\"%d \", a[q[hh]]); &#125; puts(\"\"); //求滑动窗口中的最大值 hh = 0, tt = -1; //队头和队尾 for(int i = 0; i &lt; n; i++) &#123; //判断队头是否已经弹出窗口 if(hh &lt;= tt &amp;&amp; i - k + 1 &gt; q[hh]) hh++; while(hh &lt;= tt &amp;&amp; a[q[tt]] &lt;= a[i]) tt--; q[++ tt] = i; if(i &gt;= k - 1) printf(\"%d \", a[q[hh]]); &#125; puts(\"\"); return 0;&#125; 用单调栈或者单调队列解题的思路都是：先用暴力做法求解，再考虑暴力做法的栈或队列中哪些元素是没有用的，删掉这些没用的元素，再看剩下的元素有没有单调性，如果有就可以考虑用栈或队列做优化。 KMP给定一个模式串S（长度为M），以及一个模板串P（长度为N），所有字符串中只包含大小写英文字母以及阿拉伯数字。模板串P在模式串S中多次作为子串出现。求出模板串P在模式串S中所有出现的位置的起始下标。 数据范围：$1 \\le N \\le 10^5$，$1 \\le M \\le 10^6$。 KMP算法的知识可以参考之前写的 数据结构与算法（20）串——3.KMP算法 对于模板串要处理出一个：它的一个以第i个元素为右端点的后缀和一个前缀相等，相等的最大长度是多少。如next[i]=j，则说明p[1,j]=p[i-j+1]。 123456789101112131415161718192021222324252627282930313233343536#include&lt;iostream&gt;using namespace std;const int N = 100010, M = 1000010;int n, m;char p[N], s[M];int ne[N]; int main()&#123; cin &gt;&gt; n &gt;&gt; p + 1 &gt;&gt; m &gt;&gt; s + 1; //求模板串P的next[]数组的过程，相当于是把P也当成模式 串，两个P进行kmp匹配 for(int i = 2, j = 0; i &lt;= n; i++) &#123; while(j &amp;&amp; p[i] != p[j + 1]) j = ne[j]; if(p[i] == p[j + 1]) j++; ne[i] = j; &#125; //kmp匹配过程 for(int i = 1, j = 0; i &lt;= m; i++) //每次是s[i]和p[j+1]匹配 &#123; while(j &amp;&amp; s[i] != p[j + 1]) j = ne[j]; if(s[i] == p[j + 1]) j++; if(j == n) &#123; //匹配成功 printf(\"%d \", i - n); //输出模板串P在模式串S中出现的位置的起始下标 j = ne[j]; &#125; &#125; return 0;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"算法基础（3）","slug":"算法基础（3）","date":"2020-10-02T07:21:03.000Z","updated":"2020-10-03T15:29:59.894Z","comments":true,"path":"2020/10/02/算法基础（3）/","link":"","permalink":"http://nekomoon404.github.io/2020/10/02/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%EF%BC%883%EF%BC%89/","excerpt":"","text":"第一章 基础算法 双指针算法6：双指针，通常有两种情况： 两个指针分别指向两个序列，如归并排序 两个指针指向同一个序列，可以是一个在首一个在尾，两个指针共同维护一段区间，如快速排序 代码结构一般是： 1234567for(i = 0, j = 0; i &lt; n; i++)&#123; while(j &lt; i &amp;&amp; check(i, j)) //i和j满足某种性质 j++; //下面就是每道题目的具体逻辑&#125; 每个指针在所有循环中总共移动的次数不超过$n$，双指针算法就是将朴素的两层for遍历（时间复杂度为$O(n^2)$）优化到$O(n)$。 解题的思路一般是：从朴素做法入手，从中发现一些问题的性质，如单调性等，将原来的$O(n^2)$时间复杂度优化到$O(n)$。 双指针的几个例子： 读入一行字符串，其中有若干个单词，每个单词中间有一个空格隔开，要求输出每一个单词。 1234567891011121314151617181920212223#include&lt;iostream&gt;using namespace std;int main()&#123; char str[1000]; gets(str); int n = strlen(str); for(int i = 0; i &lt; n; i++) &#123; int j = i; while(j &lt; n &amp;&amp; str[j] != ' ') j++; //这道题的具体逻辑 for(int k = i; k &lt; j; k++) cout &lt;&lt; str[k]; cout &lt;&lt; endl; i = j; &#125; return 0;&#125; 最长连续不重复子序列：给定一个长度为n的整数序列，请找出最长的不包含重复的数的连续区间，输出它的长度。 i表示一段区间的右边界，j表示这段区间的左边界。遍历i，找到这段区间最左能到多远，即j的位置，当i向右移时，j一定是向右移或者是不动，一定不会向左移，即是这个问题的单调性，i和j在循环走过的长度都不会超过n，因此时间复杂度是$O(n)$。 1234567891011121314151617181920212223242526#include&lt;iostream&gt;using namespace std;const int N = 100010;int a[N], s[N]; //s[]用来存放区间内每个数字出现的次数int n;int main()&#123; cin &gt;&gt; n; for(int i = 0; i &lt; n; i++) cin &gt;&gt; a[i]; int res = 0; for(int i = 0, j = 0; i &lt; n; i++) &#123; s[a[i]] ++; while(s[a[i]] &gt; 1) &#123; s[a[j]] --; j++; &#125; res = max(res, i - j + 1); &#125; cout &lt;&lt; res &lt;&lt; endl; return 0;&#125; 数组元素的目标和：给定两个升序排序的有序数组A和B，以及一个目标值x。数组下标从0开始。请你求出满足A[i] + B[j] = x的数对(i, j)。数据保证有唯一解。 12345678910111213141516171819202122232425#include&lt;iostream&gt;using namespace std;const int N = 100010;int a[N], b[N];int main()&#123; int n, m, x; cin &gt;&gt; n &gt;&gt; m &gt;&gt; x; for(int i = 0; i &lt; n; i++) scanf(\"%d\", &amp;a[i]); for(int i = 0; i &lt; m; i++) scanf(\"%d\", &amp;b[i]); for(int i = 0, j = m - 1; i &lt; n; i++) &#123; while(a[i] + b[j] &gt; x) j--; if(j &gt;= 0 &amp;&amp; a[i] + b[j] == x) &#123; cout &lt;&lt; i &lt;&lt; ' ' &lt;&lt; j &lt;&lt; endl; break; &#125; &#125; return 0;&#125; 位运算算法7：位运算 算法题中位运算的常用操作： 求一个整数$n$的二进制表示中第$k$位数字是几，如$n=15=(1111)_2$，个位是第0位开始算。 先把第$k$位移到最后一位，n &gt;&gt; k； 看个位数字是几，n &amp; 1；两步合并就是：n &gt;&gt; k &amp; 1 lowbit(x)操作：返回x的最后一位1，如x = 1010，lowbit(x) = 10；x = 101000，lowbit(x)=1000 lowbit(x)的实现：x &amp; -x，相当于是x &amp; (~x + 1)。（~表示取反） lowbit(x)的应用：统计x中1的个数。 12345678910111213141516171819202122232425#include&lt;iostream&gt;using namespace std;int lowbit(int x)&#123; return x &amp; -x;&#125;int main()&#123; int n; cin &gt;&gt; n; while(n--) &#123; int x; cin &gt;&gt; x; int res = 0; while(x) x -= lowbit(x), res++; cout &lt;&lt; res &lt;&lt; ' ' ; &#125; return 0;&#125; 离散化算法8：离散化 这里特指整数的离散化，它所针对的问题是：假设有一个数组，其元素个数很少，但元素的值域很大，如a[5]={1, 3, 100, 2000, 500000}，我们又需要以这些元素为下标进行一些其他的操作。再开一个500000长度的数组显然是不明智的，因此就需要离散化，将这些值域很大的数字映射到从0开始的连续的自然数。要进行离散化需要考虑两个问题： a[]中可能有重复元素 $\\to$ 去重； 如何快速地算出a[i]离散化后的值 $\\to$ a[]是有序的，即找到数字的下标即可 $\\to$ 二分 代码模板： 12345678910111213141516vector&lt;int&gt; alls; //存储所有待离散化的值sort(alls.begin(), alls.end()); //将所有值排序alls.eras(unique(alls.begin(), alls.end()), alls.end()); //去掉重复元素//二分求出x对应的离散化的值int find(int x) //找到第一个大于等于x的数的下标 &#123; int l = 0, r = alls.size() - 1; while(l &lt; r) &#123; int mid = l + r &gt;&gt; 1; if(alls[mid] &gt;= x) r = mid; else l = mid + 1; &#125; return r + 1; //映射到1,2,3,...；若要映射到0, 1, 2, 3, ....就return r;&#125; 例子：假定有一个无限长的数轴，数轴上每个坐标上的数都是0。现在，我们首先进行 n 次操作，每次操作将某一位置x上的数加c。接下来，进行 m 次询问，每个询问包含两个整数l和r，你需要求出在区间[l, r]之间的所有数的和。 数据范围：$-10^9 \\le x \\le 10^9$，$1\\le n, m \\le 10^5$，$-10^9 \\le l \\le r \\le 10^9$，$-10000 \\le c \\le 10000$。 可见相比于整个数轴的范围，要取的数是比较稀疏的，但是跨度很大，这些数的下标就可以用离散化的思想来处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;typedef pair&lt;int, int&gt; PII;const int N = 300010; //n, l, r分别需要10^5，所以需要用到的下标个数就是300000int n, m;vector&lt;int&gt; alls; //alls存放所要用到的数的下标，包括赋予了值的数的下标，以及待计算的区间的左右边界的下标vector&lt;PII&gt; add, query; //add存放取出的数的下标和赋予的值，query存放待计算的区间的左右边界int a[N], s[N]; //a[]存放之前赋予过的值，s[]是a[]的前缀和，方便计算区间和int find(int x)&#123; int l = 0, r = alls.size() - 1; while(l &lt; r) &#123; int mid = l + r &gt;&gt; 1; if(alls[mid] &gt;= x) r = mid; else l = mid + 1; &#125; return r + 1; //映射到1, 2, 3, ......方便后续的前缀和操作&#125;int main()&#123; cin &gt;&gt; n &gt;&gt; m; for(int i = 0; i &lt; n; i++) //读入n行，每行包含两个整数x和c &#123; int x, c; cin &gt;&gt; x &gt;&gt; c; add.push_back(&#123;x, c&#125;); alls.push_back(x); &#125; for(int i = 0; i &lt; m; i++) &#123; int l, r; cin &gt;&gt; l &gt;&gt; r; query.push_back(&#123;l, r&#125;); alls.push_back(l); alls.push_back(r); &#125; //对alls序列排序和去重 sort(alls.begin(), alls.end()); alls.erase(unique(alls.begin(), alls.end()), alls.end()); //将add中存放的下标和值的数对，下标按照alls排序后的顺序，值填入到数组a[]中，方便后续的前缀和计算 for(auto item : add) &#123; int x = find(item.first); a[x] += item.second; &#125; //预处理前缀和，到这一步就和之前讲过的前缀和一样了 for(int i = 1; i &lt;= alls.size(); i++) s[i] += s[i-1] + a[i]; //处理待计算的区间的和 for(auto item : query) &#123; int l = find(item.first), r = find(item.second); cout &lt;&lt; s[r] - s[l-1] &lt;&lt; endl; &#125; return 0;&#125; 当然可以自己实现unique函数。 12345678910vector&lt;int&gt;::iterator unique(vector&lt;int&gt; &amp;a)&#123; int j = 0; for(int i = 0; i &lt; a.size(); i++) if(!i || a[i] != a[i-1]) a[j++] = a[i]; //a[0]~a[j-1]中就存好了a[]中所有不重复的数字 return a.begin() + j;&#125; 区间合并算法9：区间合并 若两个区间有交集，那就可以把它们合并到一个较长的区间，可以扩展到多个区间。 例子：给定$n$个区间$[l_i,r_i]$，要求合并所有有交集的区间。注意如果在端点处相交，也算有交集。 输出合并完成后的区间个数。例如：[1,3]和[2,6]可以合并为一个区间[1,6]。 数据范围：$1\\le n \\le 100000$，$-10^9 \\le l_i \\le r_i \\le 10^9$。 解题思路： 按区间左端点排序； 扫描整个区间，扫描的过程中把所有有交集的区间合并。每次维护一个当前的区间$[st, ed]$，设已扫描到第$i$个区间$[st_i, ed_i]$，那第$i$个区间和当前的区间的关系有： 第$i$个区间在当前的区间的内部；$\\to$ 当前区间不变 第$i$个区间与当前的区间有交集，但不全在其内部；$\\to$ 当前区间更新成$[st, ed_i]$ 第$i$个区间与当前的区间没有有交集。$\\to$ 当前区间更新成$[st_i, ed_i]$ （与区间有关的题目的思路大多都是贪心） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;typedef pair&lt;int, int&gt; PII; //定义一个pair，存放区间的左右端点void merge(vector&lt;PII&gt; &amp;segs)&#123; vector&lt;PII&gt; res; sort(segs.begin(), segs.end()); //sort先按segs.first排序，即先按区间左端点排序 int st = -2e9, ed = -2e9; //当前维护的区间的左右端点为st, ed for(auto seg : segs) if(ed &lt; seg.first) //第i个区间与当前区间没有交集，则可以把当前区间存入res，然后更新当前区间为第i个区间 &#123; if(st != -2e9) res.push_back(&#123;st, ed&#125;); st = seg.first, ed = seg.second; &#125; else ed = max(ed, seg.second); //第i个区间与当前区间有交集，那就更新当前区间的右端点为两者右端点的最大值 if(st != -2e9) res.push_back(&#123;st, ed&#125;); //加了一个st != -2e9的判断是为了确保开始输入的区间数量不为0，即初始的st已经变化过了 segs = res;&#125;int main()&#123; int n; cin &gt;&gt; n; vector&lt;PII&gt; segs; for(int i = 0; i &lt; n; i++) &#123; int l, r; cin &gt;&gt; l &gt;&gt; r; segs.push_back(&#123;l, r&#125;); &#125; merge(segs); cout &lt;&lt; segs.size() &lt;&lt; endl; return 0;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"算法基础（2）","slug":"算法基础（2）","date":"2020-09-29T12:27:11.000Z","updated":"2020-10-03T15:28:54.452Z","comments":true,"path":"2020/09/29/算法基础（2）/","link":"","permalink":"http://nekomoon404.github.io/2020/09/29/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%EF%BC%882%EF%BC%89/","excerpt":"","text":"第一章 基础算法 高精度算法4：高精度（只有C++需要），一般有四种情况： 两个大整数相加 A + B，A、B的位数 $\\le 10^6$； 两个大整数相减 A - B，A、B的位数 $\\le 10^6$； 一个大整数乘以一个小整数 A * a，A的位数$\\le 10^6$，a$\\le 10000$； 一个大整数除以一个小整数 A / a，A的位数$\\le 10^6$，a$\\le 10000$； 首先要考虑的是一个大整数如何存储？方法是可以将其中的每一位保存在一个数组中，为了方便运算，让a[0]存数字的个位，a[1]存数字的十位……依次存高位。如数字以string类型输入a = &quot;123456&quot;，用vector&lt;int&gt;来存储，A=[6,5,4,3,2,1]。 （1）加法 两个数组的加法运算就是来模拟人工手算的过程，从个位开始逐位相加，逢十进一。 1234567891011121314151617181920212223242526272829303132333435363738#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;//C = A + Bvector&lt;int&gt; add(vector&lt;int&gt; &amp;A, vector&lt;int&gt; &amp;B)&#123; vector&lt;int&gt; C; if(A.size() &lt; B.size()) return add(B, A); int t = 0; //进位 for(int i = 0; i &lt; A.size(); i++) &#123; t += A[i]; if(i &lt; B.size()) t += B[i]; C.push_back(t % 10); t /= 10; &#125; if(t) C.push_back(t); //结束后检查下最高位是否需要进位 return C;&#125;int main()&#123; string a, b; vector&lt;int&gt; A, B; cin &gt;&gt; a &gt;&gt; b; //a=\"123456\" for(int i = a.size() - 1; i &gt;= 0; i--) A.push_back(a[i] - '0'); //A=[6,5,4,3,2,1] for(int i = b.size() - 1; i &gt;= 0; i--) B.push_back(b[i] - '0'); vector&lt;int&gt; C = add(A, B); for(int i = C.size() - 1; i &gt;= 0; i--) printf(\"%d\", C[i]); return 0;&#125; 还可以在空间上做进一步的优化，即进行压位处理，数组中的每一个元素不止存放数字的一位，而是多位。（不常用） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;const int base = 1000000000;vector&lt;int&gt; add(vector&lt;int&gt; &amp;A, vector&lt;int&gt; &amp;B)&#123; if (A.size() &lt; B.size()) return add(B, A); vector&lt;int&gt; C; int t = 0; for (int i = 0; i &lt; A.size(); i ++ ) &#123; t += A[i]; if (i &lt; B.size()) t += B[i]; C.push_back(t % base); t /= base; &#125; if (t) C.push_back(t); return C;&#125;int main()&#123; string a, b; vector&lt;int&gt; A, B; cin &gt;&gt; a &gt;&gt; b; for (int i = a.size() - 1, s = 0, j = 0, t = 1; i &gt;= 0; i -- ) &#123; s += (a[i] - '0') * t; j ++, t *= 10; if (j == 9 || i == 0) &#123; A.push_back(s); s = j = 0; t = 1; &#125; &#125; for (int i = b.size() - 1, s = 0, j = 0, t = 1; i &gt;= 0; i -- ) &#123; s += (b[i] - '0') * t; j ++, t *= 10; if (j == 9 || i == 0) &#123; B.push_back(s); s = j = 0; t = 1; &#125; &#125; auto C = add(A, B); cout &lt;&lt; C.back(); for (int i = C.size() - 2; i &gt;= 0; i -- ) printf(\"%09d\", C[i]); cout &lt;&lt; endl; return 0;&#125; （2）减法 两个数组的减法运算也是来模拟人工手算的过程，从个位开始逐位相减，不够就向前一位借1，加10。 算法的思路： 如果A$\\ge$B，就计算$A - B$；如果A$&lt;$B，就计算 $-(B - A)$。 在每一位上，若$A_i-B_i-t \\ge 0$，就计算$A_i-B_i-t$；若$A_i-B_i-t &lt; 0$，就计算$A_i-B_i+10-t $，其中$t$代表借位。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;//判断是否有A &gt;= Bbool cmp(vector&lt;int&gt; &amp;A, vector&lt;int&gt; &amp;B)&#123; if(A.size() != B.size()) return A.size() &gt; B.size(); for(int i = A.size() - 1; i &gt;= 0; i--) //数组中最后一个元素存放的是数字的最高位，从最高位开始比较 if(A[i] != B[i]) return A[i] &gt; B[i]; return true;&#125;//C = A - Bvector&lt;int&gt; sub(vector&lt;int&gt; &amp;A, vector&lt;int&gt; &amp;B)&#123; vector&lt;int&gt; C; int t = 0; //进位 for(int i = 0; i &lt; A.size(); i++) &#123; t = A[i] - t; if(i &lt; B.size()) t -= B[i]; C.push_back( (t + 10) % 10); if(t &lt; 0) t = 1; //当前的t小于0，需要借位 else t = 0; &#125; while(C.size() &gt; 1 &amp;&amp; C.back() == 0) C.pop_back(); //去掉前导0 return C;&#125;int main()&#123; string a, b; vector&lt;int&gt; A, B; cin &gt;&gt; a &gt;&gt; b; //a=\"123456\" for(int i = a.size() - 1; i &gt;= 0; i--) A.push_back(a[i] - '0'); //A=[6,5,4,3,2,1] for(int i = b.size() - 1; i &gt;= 0; i--) B.push_back(b[i] - '0'); if(cmp(A, B)) &#123; vector&lt;int&gt; C = sub(A, B); for(int i = C.size() - 1; i &gt;= 0; i--) printf(\"%d\", C[i]); &#125; else &#123; vector&lt;int&gt; C = sub(B, A); printf(\"-\"); for(int i = C.size() - 1; i &gt;= 0; i--) printf(\"%d\", C[i]); &#125; return 0;&#125; （3）乘法 也是模拟手算乘法，区别是：逐位相乘时是乘以整个被乘数b。 12345678910111213141516171819202122232425262728293031323334#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;vector&lt;int&gt; mul(vector&lt;int&gt; &amp;A, int b)&#123; vector&lt;int&gt; C; int t = 0; for(int i = 0; i &lt; A.size() || t; i++) //注意这里将最高位的进位问题一起处理了 &#123; if(i &lt; A.size()) t += A[i] * b; C.push_back(t % 10); t /= 10; &#125; while(C.size() &gt; 1 &amp;&amp; C.back() == 0) C.pop_back(); //当被乘数是0时，要将结果的前导0去掉 return C;&#125;int main()&#123; string a; int b; cin &gt;&gt; a &gt;&gt; b; vector&lt;int&gt; A; for(int i = a.size() - 1; i &gt;= 0; i--) A.push_back(a[i] - '0'); auto C = mul(A, b); for(int i = C.size() - 1; i &gt;= 0; i--) printf(\"%d\", C[i]); return 0;&#125; （4）除法 同样的思路，模拟手算的过程，注意区别：除法是从最高位开始除的。 1234567891011121314151617181920212223242526272829303132333435363738394041#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;vector&lt;int&gt; div(vector&lt;int&gt; &amp;A, int b, int &amp;r) //余数r通过引用传入&#123; vector&lt;int&gt; C; r = 0; for(int i = A.size() - 1; i &gt;= 0; i--) //除法从数字的高位开始 &#123; r = r * 10 + A[i]; C.push_back(r / b); r %= b; &#125; //除法中C先存的是数字的高位，与定义的先存低位相反，要先翻转一下 reverse(C.begin(), C.end()); //去掉前导0 while(C.size() &gt; 1 &amp;&amp; C.back() == 0) C.pop_back(); return C;&#125;int main()&#123; string a; int b; cin &gt;&gt; a &gt;&gt; b; vector&lt;int&gt; A; for(int i = a.size() - 1; i &gt;= 0; i--) A.push_back(a[i] - '0'); //这里一定要记得减去'0' int r; auto C = div(A, b, r); for(int i = C.size() - 1; i &gt;= 0; i--) printf(\"%d\",C[i]); cout &lt;&lt; endl &lt;&lt; r &lt;&lt; endl; return 0;&#125; 前缀和，差分算法5：前缀和，差分 前缀和：设一个数组$a_1,a_2,a_3,\\dots,a_n$，（注意下标从1开始），定义其前缀和为$S_i=a_1+a_2+\\dots+a_i$，规定$S_0=0$。 如何求前缀和$S_i$：for遍历即可； 前缀和的作用：可以方便地求出序列中某一段的和，如求下标区间$[l,r]$内的元素的和，即可用$S_r-S_{l-1}$，时间复杂度为$O(1)$。 123456789101112131415161718192021222324#include&lt;iostream&gt;using namespace std;const int N = 100010;int n, m;int a[N], s[N];int main()&#123; scanf(\"%d%d\", &amp;n, &amp;m); for(int i = 1; i &lt;= n; i++) scanf(\"%d\", &amp;a[i]); for(int i = 1; i &lt;= n; i++) s[i] = s[i - 1] + a[i]; //计算前缀和 while(m--) &#123; int l, r; scanf(\"%d%d\", &amp;l, &amp;r); printf(\"%d\\n\", s[r] - s[l - 1]); //计算区间和 &#125; return 0;&#125; 前缀和也可以扩展到二维，求区间和$\\to$求子矩阵和。用$S_{ij}$表示左上角的子矩阵的和。如下图若要求以$(x_1,y_1)$为左上角，以$(x_2,y_2)$为右下角的子矩阵的和，那就可以转化成求： S_{x_2y_2}-S_{x_2y_1-1}-S_{x_1-1y_2}+S_{x_1-1y_1-1} 如何求子矩阵和$S_{ij}$：两层for遍历i,j， S_{ij}=S_{i-1j}+S_{ij-1}-S_{i-1j-1}+a_{ij} 主要的思想就是容斥原理。 12345678910111213141516171819202122232425#include&lt;iostream&gt;using namespace std;const int N = 1010;int n, m, q;int a[N][N], s[N][N];int main()&#123; scanf(\"%d%d%d\", &amp;n, &amp;m, &amp;q); for(int i = 1; i &lt;= n; i++) for(int j = 1; j &lt;= m; j++) &#123; scanf(\"%d\", &amp;a[i][j]); s[i][j] = s[i - 1][j] + s[i][j - 1] - s[i - 1][j - 1] + a[i][j]; &#125; while(q--) &#123; int x1, y1, x2, y2; scanf(\"%d%d%d%d\", &amp;x1, &amp;y1, &amp;x2, &amp;y2); printf(\"%d\\n\", s[x2][y2] - s[x1 - 1][y2] - s[x2][y1 - 1] + s[x1 - 1][y1 - 1]); &#125; return 0;&#125; 差分是前缀和的逆运算，设一个数组$a_1, a_2, \\dots, a_n$，现构造一个数组$b_1,b_2,\\dots, b_n$，使得$a_i=b_1+b_2+\\dots+b_n$，即$a$数组的元素是$b$数组的前缀和，$b$数组的元素是$a$数组的差分。则有： \\begin{align*} b_1 &= a_1 \\\\ b_2 &= a_2-a_1\\\\ b_3 &= a_3-a_2\\\\ &\\dots \\\\ b_n &= a_n-a_{n-1} \\end{align*}差分的作用： 若有$b$数组，就可以通过求前缀和的方法求得原数组$a$，时间复杂度$O(n)$。 若要对$a$数组下标为$[l,r]$区间的一段元素都加上$c$，则要$O(n)$的时间复杂度；若考虑改动$b$数组，那只要改变两个元素，即让$b_l+c$，让$b_{r+1}-c$，则只要$O(1)$的时间复杂度，这样由数组$b$得到的数组$a$的下标为$[l,r]$的一段就都加上了$c$。 那若有了数组$a$，如何得到数组$b$：可以假设数组$a$初始全部是0，依次在区间[1,1]加上$a_1$，在区间[2,2]加上$a_2$，……，在区间[n,n]加上$a_n$，即转换到对数组$b$的操作。 1234567891011121314151617181920212223242526272829303132#include&lt;iostream&gt;using namespace std;const int N = 100010;int n, m;int a[N], b[N];void insert(int l, int r, int c)&#123; b[l] += c; b[r + 1] -= c;&#125;int main()&#123; scanf(\"%d%d\", &amp;n, &amp;m); for(int i = 1; i &lt;= n; i++) &#123; scanf(\"%d\", &amp;a[i]); insert(i, i, a[i]); //求得数组b &#125; while(m--) &#123; int l, r, c; scanf(\"%d%d%d\", &amp;l, &amp;r, &amp;c); insert(l, r, c); &#125; for(int i = 1; i &lt;= n; i++) &#123; b[i] += b[i-1]; //求数组b的前缀和 printf(\"%d \", b[i]); &#125; return 0;&#125; 差分也有二维的形式，原矩阵元素$a_{ij}$，差分矩阵元素$b_{ij}$，使得$a_{ij}$是差分矩阵$b_{ij}$的前缀和。 其作用也可由上面的一维差分类比过来：给矩阵$a$的某一个子矩阵（左上角为$(x_1,y_1)$，右下角为$(x_2,y_2)$）中的元素全都加上一个数$c$，可以转化成： \\begin{align*} b_{x_1y_1} &+=c\\\\ b_{x_2+1y_1} &-=c\\\\ b_{x_1y_2+1} &-=c\\\\ b_{x_2+1y_2+1} &+=c \\end{align*} 123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;iostream&gt;using namespace std;const int N = 1010;int n, m, q;int a[N][N], b[N][N];void insert(int x1, int y1, int x2, int y2, int c)&#123; b[x1][y1] += c; b[x2 + 1][y1] -= c; b[x1][y2 + 1] -= c; b[x2 + 1][y2 + 1] += c;&#125;int main()&#123; scanf(\"%d%d%d\", &amp;n, &amp;m, &amp;q); for(int i = 1; i &lt;= n; i++) for(int j = 1; j &lt;= m; j++) &#123; scanf(\"%d\", &amp;a[i][j]); insert(i, j, i, j, a[i][j]); &#125; while(q--) &#123; int x1, y1, x2, y2, c; scanf(\"%d%d%d%d%d\", &amp;x1, &amp;y1, &amp;x2, &amp;y2, &amp;c); insert(x1, y1, x2, y2, c); &#125; for(int i = 1; i &lt;= n; i++) &#123; for(int j = 1; j &lt;= m; j++) &#123; b[i][j] += b[i-1][j] + b[i][j-1] - b[i-1][j-1]; printf(\"%d \", b[i][j]); &#125; printf(\"\\n\"); &#125; return 0;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"算法基础（1）","slug":"算法基础（1）","date":"2020-09-29T12:26:56.000Z","updated":"2020-10-03T15:27:32.584Z","comments":true,"path":"2020/09/29/算法基础（1）/","link":"","permalink":"http://nekomoon404.github.io/2020/09/29/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%EF%BC%881%EF%BC%89/","excerpt":"","text":"开个新坑( ･´ω`･ )，算法基础系列用来记录自己在Acwing上学习和刷题的过程。共勉。 第一章 基础算法 快速排序算法1：快速排序 快排用到了分治的思想，对一个下标左边界为$l$，下标右边界为$r$的数组，进行快速排序一般可以分为三个步骤： 其中最关键的是第二步-调整区间。暴力做法虽然时间复杂度是常数，但空间占用比较多（需要开额外的数组）。下面是优化后的方法： 在解题中为了避免在处理边界问题上浪费太多时间，可以记一些快排的模板。 快排模板题：给定你一个长度为$n$的整数数列。请你使用快速排序对这个数列按照从小到大进行排序。并将排好序的数列按顺序输出。 1234567891011121314151617181920212223242526272829303132333435#include&lt;iostream&gt;using namespace std;const int N = 1e6 + 10;int n;int q[N];void quick_sort(int q[], int l, int r)&#123; if(l &gt;= r) return; int x = q[l + r &gt;&gt; 1]; // x = q[l]，题目的数据加强过，写成x = q[l]会超时 int i = l - 1, j = r + 1; //先把i, j往外移一位。因为后面要先移位再判断 while(i &lt; j) &#123; do i++; while(q[i] &lt; x); do j--; while(q[j] &gt; x); if(i &lt; j) swap(q[i], q[j]); &#125; quick_sort(q, l, j); quick_sort(q, j+1, r);&#125;int main()&#123; scanf(\"%d\", &amp;n); for(int i = 0; i &lt; n; i++) scanf(\"%d\", &amp;q[i]); quick_sort(q, 0, n-1); for(int i = 0; i &lt; n; i++) printf(\"%d \", q[i]); return 0; &#125; 快速排序的时间复杂度和空间复杂度： 分析和证明过程可以参考：【算法】快速排序 时间复杂度 空间复杂度 最优 $O(n\\log n)$ $O(\\log n)$ 最坏 $O(n^2)$ $O(n)$ 平均 $O(n\\log n)$ $O(\\log n)$ 扩展题：快速选择。 第$k$个数：给定一个长度为$n$的整数数列，以及一个整数$k$，请用快速选择算法求出数列从小到大排序后的第$k$个数。 数据范围：$1 \\le n \\le 100000$，$1 \\le k \\le n$。 1234567891011121314151617181920212223242526272829303132#include&lt;iostream&gt;using namespace std;const int N = 100010;int n, k;int q[N];//快速选择，时间复杂度O(2n)int quick_sort(int l, int r, int k)&#123; if(l &gt;= r) return q[l]; int x = q[l + r &gt;&gt; 1], i = l - 1, j = r + 1; while(i &lt; j) &#123; while( q[ ++ i] &lt; x); while( q[ -- j] &gt; x); if( i &lt; j ) swap(q[i], q[j]); &#125; int sl = j - l + 1; if( k &lt;= sl ) return quick_sort(l, j, k); return quick_sort(j + 1, r, k - sl);&#125;int main()&#123; scanf(\"%d%d\", &amp;n, &amp;k); for(int i = 0; i &lt; n; i++) scanf(\"%d\", &amp;q[i]); cout &lt;&lt; quick_sort(0, n - 1, k); return 0;&#125; 归并排序算法2：归并排序 归并排序也用到了分治的思想，通常有三个步骤： 归并排序中最关键的是第三步—归并，可以使用双指针，使时间复杂度为$O(n)$。 归并排序模板题：给定你一个长度为$n$的整数数列。请你使用归并排序对这个数列按照从小到大进行排序。并将排好序的数列按顺序输出。 数据范围：$1 \\le n \\le 100000$. 1234567891011121314151617181920212223242526272829303132333435#include&lt;iostream&gt;using namespace std;const int N = 100010;int n;int q[N], tmp[N];void merge_sort(int q[], int l, int r)&#123; if(l &gt;= r) return; int mid = l + r &gt;&gt; 1; merge_sort(q, l, mid); merge_sort(q, mid+1, r); int k = 0, i = l, j = mid+1; while(i &lt;= mid &amp;&amp; j &lt;= r) if(q[i] &lt; q[j]) tmp[k++] = q[i++]; else tmp[k++] = q[j++]; while(i &lt;= mid) tmp[k++] = q[i++]; while(j &lt;= r) tmp[k++] = q[j++]; for(i = l, j = 0; i &lt;= r; i++, j++) q[i] = tmp[j];&#125;int main()&#123; scanf(\"%d\", &amp;n); for(int i = 0; i &lt; n; i++) scanf(\"%d\", &amp;q[i]); merge_sort(q, 0, n-1); for(int i = 0; i &lt; n; i++) printf(\"%d \", q[i]); return 0;&#125; 扩展题：逆序数对的数量 给定一个长度为$n$的整数数列，请你计算数列中的逆序对的数量。逆序对的定义如下：对于数列的第 $i$ 个和第 $j$ 个元素，如果满足 $i &lt; j $且 $a[i] &gt; a[j]$，则其为一个逆序对；否则不是。 数据范围：$1 \\le n \\le 100000$。 1234567891011121314151617181920212223242526272829303132333435363738394041#include&lt;iostream&gt;using namespace std;typedef long long LL;const int N = 100010;int n;int q[N], tmp[N];LL merge_sort(int q[], int l, int r)&#123; if(l &gt;= r) return; int mid = l + r &gt;&gt; 1; LL res = merge_sort(q, l, mid) + merge_sort(q, mid + 1, r); //归并的过程 int k = 0, i = l, j = mid + 1; while(i &lt;= mid &amp;&amp; j &lt;= r) &#123; if(q[i] &lt;= q[j]) tmp[K ++] = q[i ++]; else &#123; tmp[k ++] = q[j ++]; res += mid - i + 1; &#125; &#125; while(i &lt;= mid) tmp[k++] = q[i++]; while(j &lt;= r) tmp[k++] = q[j++]; for(i = l, j = 0; i &lt;= r; i++, j++) q[i] = tmp[j]; return res;&#125;int main()&#123; scanf(\"%d\", &amp;n); for(int i = 0; i &lt; n; i++) scanf(\"%d\", &amp;q[i]); cout &lt;&lt; merge_sort(q, 0, n - 1) &lt;&lt; endl; return 0;&#125; 二分查找算法3：二分查找 整数的二分查找 例题：给定一个按照升序排列的长度为n的整数数组，以及 q 个查询。对于每个查询，返回一个元素k的起始位置和终止位置（位置从0开始计数）。如果数组中不存在该元素，则返回“-1 -1”。 数据范围：$1 \\le n \\le 100000$，$1 \\le q \\le 10000$，$1 \\le k \\le 10000$。 1234567891011121314151617181920212223242526272829303132333435363738394041#include&lt;iostream&gt;using namespace std;const int N = 100010;int n, m;int q[N];int main()&#123; scanf(\"%d%d\", &amp;n, &amp;m); for(int i = 0; i &lt; n; i++) scanf(\"%d\", &amp;q[i]); while(m--) &#123; int x; scanf(\"%d\", &amp;x); //查找x的左边界，性质是左边界 右面的数都大于等于x int l = 0, r = n-1; while(l &lt; r) &#123; int mid = l + r &gt;&gt; 1; if(q[mid] &gt;= x) r = mid; else l = mid + 1; &#125; if(q[l] != x) cout &lt;&lt; \"-1 -1\" &lt;&lt; endl; //判断题目是不是无解 else &#123; cout &lt;&lt; l &lt;&lt; \" \"; //查找x的右边界，性质是右边界 左面的数都小于等于x int l = 0, r = n - 1; while(l &lt; r) &#123; int mid = l + r + 1 &gt;&gt; 1; if(q[mid] &lt;= x) l = mid; else r = mid - 1; &#125; cout &lt;&lt; l &lt;&lt; endl; &#125; &#125; return 0;&#125; 浮点数二分 例题：给定一个浮点数n，求它的三次方根。结果保留6位小数。 数据范围：$-10000 \\le n \\le 10000$ 12345678910111213141516171819#include&lt;cstdio&gt;int main()&#123; double x; scanf(\"%lf\", &amp;x); //根据数据范围确定l和r double l = -100, r = 100; while(r - l &gt; 1e-8) //通常要比题目要求的精度多2位 &#123; double mid = (l + r) / 2; if(mid * mid * mid &gt;= x) r = mid; else l = mid; &#125; printf(\"%.6lf\\n\", l); return 0;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"XGBoost原理简述","slug":"XGBoost原理简述","date":"2020-09-22T12:18:46.000Z","updated":"2020-09-22T13:09:43.377Z","comments":true,"path":"2020/09/22/XGBoost原理简述/","link":"","permalink":"http://nekomoon404.github.io/2020/09/22/XGBoost%E5%8E%9F%E7%90%86%E7%AE%80%E8%BF%B0/","excerpt":"","text":"笔记主要是参考了贪心学院在B站的公开课XGBoost的技术剖析 这篇博客也讲的十分详细：白话机器学习算法理论+实战番外篇之Xgboost，有一些上面的课程没有讲到的内容，如节点的最优切分点划分，要进行特征遍历，作者没有使用等宽或等频分桶，而是提出了等值percentiles划分算法（Weight Quantile Sketch）。 集成算法，弱分类器的概念等等就先略去了。 根据各个弱分类器之间有无依赖关系，集成算法可以分为Boosting和Bagging： Boosting流派：各分类器之间没有依赖关系，必须串行，比如Adaboost，GBDT，Xgboost； Bagging流派：各分类器之间没有依赖关系，可各自并行，比如随机森林。 为什么XGBoost这么火？ 算法可以并行，训练效率高； 比起其他算法，世界效果好； 由于可控参数（超参数）多，可以灵活调整； 学习路径： 如何构造目标函数？（XGBoost的目标函数不是连续型的） 目标函数直接优化难，如何近似？（泰勒级数，Taylor Expansion） 如何把树的结果引入到目标函数？ 仍然难优化，要不要使用贪心算法？ 1.如何构造目标函数回顾如何使用多棵树来预测： 假设已经训练了K颗树，则对于第$i$个样本的（最终）预测值为： \\hat{y_i} = \\sum^k_{k=1}f_k(x_i), \\, f_k \\in \\mathcal{F}目标函数为： Obj = \\sum^n_{i=1} l(y_i,\\hat{y_i}) + \\sum^k_{k=1} \\Omega(f_k)其中前一项为损失函数，$y_i$为真实值，$\\hat{y_i}$为预测值，$l(y_i,\\hat{y_i})$为针对当前问题的loss；后一项为Penalty，或者称Regulation，控制模型的复杂度，防止过拟合。 现在的问题是如何给每一个树加上Penalty / Regulation。 回顾在决策树中如何定义树的复杂度： $\\sum^n_{i=1}l(y_i,\\hat{y_i})$中计算了所有样本的loss，loss函数包含了不同树模型的loss，这时就可以使用叠加式的训练（Additive Training），当训练第$k$个模型（树）时，前面的第1到第$k-1$颗树是已知的。 假设现在我们要去构建第$k$颗树， 给定$x_i$； $\\hat{y_i}^{(0)} = 0 \\gets$ Default case ; $\\hat{y_i}^{(1)} = f_1(x_i) = \\hat{y_i}^{(0)} + f_1(x_i)$； $\\hat{y_i}^{(2)} = f_1(x_i) + f_2(x_i) = \\hat{y_i}^{(1)} + f_2(x_i)$； $\\dots$ $\\hat{y_i}^{(k)} = f_1(x_i) + f_2(x_i) + \\dots + f_k(x_i)= \\sum^{k-1}_{j=1}f_j(x_i)+f_k(x_i)=\\hat{y_i}^{(k-1)} + f_k(x_i)$； 其中$\\hat{y_i}^{(k-1)}$表示前$k-1$颗树的预测值之和，$f_k(x_i)$表示第$k$颗树的预测值，两者之和要和真实值$y_i$越接近越好。 因为前$k-1$颗树是训练好的，则目标函数可以写成： \\begin{align*} Obj &= \\sum^n_{i=1} l(y_i, \\hat{y_i}^{(k)}) + \\sum^k_{k=1}\\Omega(f_k)\\\\ &= \\sum^n_{i=1} l(y_i, \\hat{y_i}^{(k-1)} + f_k(x_i)) + \\sum^{k-1}_{j=1}\\Omega(f_j)+\\Omega(f_k) \\end{align*}其中$\\hat{y_i}^{(k-1)}$和$\\sum^{k-1}_{j=1}\\Omega(f_j)$可以看作是常数，则当训练第$k$颗树时，我们要最小化： minimize \\quad \\sum^n_{i=1} l(y_i, \\hat{y_i}^{(k-1)} + f_k(x_i)) + \\Omega(f_k) 2.使用泰勒级数优化目标函数由上一节我们可知，构建第$k$颗树时的目标函数是 ： \\begin{align*} Obj &= \\sum^n_{i=1} l(y_i, \\hat{y_i}^{(k-1)} + f_k(x_i)) + \\Omega(f_k) \\end{align*}回顾泰勒级数Taylor Expansion： f(x+\\Delta x) \\approx f(x) + f'(x) \\cdot \\Delta x + \\frac{1}{2} f^{''}(x)\\cdot \\Delta x^2令其中的$f(x) = l(y_i, \\hat{y_i}^{(k-1)} )$，$\\Delta x= f_k(x_i)$，则有： \\begin{align*} Obj &= \\sum^n_{i=1} l(y_i, \\hat{y_i}^{(k-1)} + f_k(x_i)) + \\Omega(f_k) \\\\ &= \\sum^n_{i=1} \\left[ l(y_i, \\hat{y_i}^{(k-1)} ) +\\partial_{\\hat{y_i}^{(k-1)}} l(y_i, \\hat{y_i}^{(k-1)} ) \\cdot f_k(x_i) + \\frac{1}{2}\\partial^2_{\\hat{y_i}^{(k-1)}} l(y_i, \\hat{y_i}^{(k-1)} ) \\cdot f^2_k(x_i) \\right]+ \\Omega(f_k) \\\\ &= \\sum^n_{i=1} \\left[ l(y_i, \\hat{y_i}^{(k-1)} ) +g_i \\cdot f_k(x_i) + h_i \\cdot f^2_k(x_i) \\right]+ \\Omega(f_k) \\end{align*}第一项$ l(y_i, \\hat{y_i}^{(k-1)} )$是已知的，那么最下化目标函数就等价于： minimize \\sum^n_{i=1} \\left[ g_i \\cdot f_k(x_i) + h_i \\cdot f^2_k(x_i) \\right]+ \\Omega(f_k)注：当训练第$k$颗树时，$\\{h_i, g_i\\}$是已知的。 3.在树的形状已知的情况下，求目标函数的最小值接下来我们要把$f_k(x_i)$和$\\Omega(f_k)$参数化。考虑现有如下图的一个树，那我们如何把这颗树用参数化表示出来： 定义一个权重变量，或者称leaf value，$w=(w_1, w_2, w_3) = (15, 12, 20)$； 定义一个函数$q(x)$，表示样本$x$的位置，$q(x_1) =1, q(x_2)=3, q(x_3)= 1, q(x_4) = 2, q(x_5)=3$； 则有$f_k(x_i) = w_{q(x_i)} $ ，这样就把$f_k(x_i)$参数化了，但有个问题是$w$的下标还是个函数，为此我们还需定义一个函数$I_j=\\{i | q(x_i)=j\\}$，表示那些样本$x_i$会落在第$j$个位置上，它按叶节点的位置把样本重新组织。$I_1=\\{1,3\\},I_2=\\{4\\}, I_3=\\{2, 5\\}$。 这样我们原先以样本为单位累加得到$\\sum^n_{i=1} g_i \\cdot f_k(x_i)=\\sum^n_{i=1} g_i \\cdot w_{q(x_i)}$这一项，就可以换种思路，以叶节点为单位累加，以上图为例： \\begin{align*} &g_1 \\cdot w_{q(x_1)}+g_2 \\cdot w_{q(x_2)}+g_3 \\cdot w_{q(x_3)}+g_4 \\cdot w_{q(x_4)}+g_5 \\cdot w_{q(x_5)}\\\\ =&g_1 \\cdot w_{q(x_1)}+g_3 \\cdot w_{q(x_3)}+ \\\\ &g_2 \\cdot w_{q(x_2)}+\\\\ &g_4 \\cdot w_{q(x_4)}+g_5 \\cdot w_{q(x_5)}\\\\ =& g_1 \\cdot w_1+g_3 \\cdot w_1+ \\\\ &g_2 \\cdot w_2+\\\\ &g_4 \\cdot w_3+g_5 \\cdot w_3\\\\ =&\\sum^T_{j=1}(\\sum_{i\\in I_j } g_i) \\cdot w_j \\end{align*}接着考虑如何定义一颗树的复杂度，可以是树的复杂度 = 叶节点个数 + leaf value，即： \\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda \\sum^T_{j=1} w_j^2其中$T$是叶节点的个数，$w_j$是第$j$个叶节点的leaf value；$\\gamma$和$\\lambda$控制两部分的权重，是超参数。 最后将两部分结合起来，得到新的目标函数（假设树的形状已知） \\begin{align*} & \\sum^n_{i=1} \\left[ g_i \\cdot f_k(x_i) + h_i \\cdot f^2_k(x_i) \\right]+ \\Omega(f_k)\\\\ =& \\sum^n_{i=1} \\left[ g_i \\cdot w_{q(x_i)} + h_i \\cdot w^2_{q(x_i)} \\right]+ \\gamma T + \\frac{1}{2} \\lambda \\sum^T_{j=1} w_j^2 \\\\ =& \\sum^T_{j=1} \\left[(\\sum_{i\\in I_j } g_i) \\cdot w_j + \\frac{1}{2}(\\sum_{i\\in I_j } h_i + \\lambda) \\cdot w^2_j\\right] + \\gamma T \\end{align*}令$G_j = \\sum_{i\\in I_j } g_i$，$H_j = \\sum_{i\\in I_j } h_i$，则使前一项最小的$w_j$值（回顾一元二次方程）为： w_j^* = -\\frac{G_j}{H_j+\\lambda}此时目标函数的最小值为： Obj* = \\frac{1}{2} \\cdot \\sum^T_{j=1} \\frac{G_j^2}{H_j+\\lambda} + \\gamma T 那么到目前我们解决了，在树的形状已知的情况下，可以求出第$k$树的最小的目标函数值。 那接下来我们要做的是在所有可能的形状的树中，寻找出$Obj^*$最小的那颗树。 4.寻找树的形状寻找树的形状可以用暴力算法（Brute Force Search），但这样做就效率太低了，复杂度也是节点个数的指数级的。 可行的方法是使用贪心算法去寻找。 回顾我们如何去构造一颗决策树。选择特征的依据是使不确定性变小，特征的score = 原（不确定性）- 分之后（不确定性），称为Information Gain（信息增益），每次分支的依据就是使信息增益最大化。那把这里的不确定性（Entropy）换成 $Obj$，就可以完成对有最小的$Obj^*$的树的寻找。 通过下面的例子来看一下如何寻找最好的树的形状，即寻找最好的Split。 xgboost贪心建树的思路：遍历所有特征以及所有分割点，每次算最好的那个。但这样做代价太大了，尤其是数据量很大，分割点很多的时候，计算起来非常复杂并且也无法读入内存进行计算。作者提出了一种近似分割的方式（可以理解为分割点分桶的思路），选出一些候选的分裂点，然后再遍历这些较少的分裂点来找到最佳分裂点。 进行分桶候选分裂点的一般思路是根据特征值的大小进行等宽分桶，或者进行等频分桶。这样做选择出的候选点确实少了很多，但这样划分是没什么依据的，缺乏可解释性。 作者采用了一种对loss的影响权重的等值percentiles（百分比分位数）划分算法（Weight Quantile Sketch）。考虑的是想让loss在左右子树上分布的均匀一些，而不是样本数量的均匀，因为每个样本对降低loss的贡献可能不一样，按样本均分会导致分开之后左子树和右子树loss分布不均匀， 其实这里这个损失函数还可以进一步化简的（和上面的化简不一样，上面的化简是把遍历样本转到了遍历叶子上得到基于决策树的目标函数，这里是从目标函数本身出发进行化简）： 后面的每一个分类器都是在拟合每个样本的一个残差 $\\frac{g_i}{h_i}$，$h_i$可以看做计算残差时某个样本的重要性，即每个样本对降低loss的贡献程度。第一个问题说的听清楚了吧。 Xgboost引入了二阶导之后，相当于在模型降低残差的时候给各个样本根据贡献度不同加入了一个权重，这样就能更好的加速拟合和收敛。GBDT只用到了一阶导数，这样只知道梯度大的样本降低残差效果好，梯度小的样本降低残差不好，但是好与不好的程度在GBDT中无法展现。而xgboost这里就通过二阶导可以展示出来，这样模型训的时候就有数了","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://nekomoon404.github.io/categories/Machine-Learning/"}],"tags":[{"name":"XGBoost","slug":"XGBoost","permalink":"http://nekomoon404.github.io/tags/XGBoost/"}]},{"title":"贷款违约预测（1）赛题理解","slug":"贷款违约预测（1）赛题理解","date":"2020-09-15T11:05:51.000Z","updated":"2020-09-15T13:02:38.461Z","comments":true,"path":"2020/09/15/贷款违约预测（1）赛题理解/","link":"","permalink":"http://nekomoon404.github.io/2020/09/15/%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8B%EF%BC%881%EF%BC%89%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/","excerpt":"","text":"1.赛题概况比赛地址：零基础入门金融风控-贷款违约预测 本次比赛以金融风控中的个人信贷为背景，根据贷款申请人的数据信息预测其是否有违约的可能，以此判断是否通过此项贷款，是一个二分类问题。 赛题的数据来自某信贷平台的贷款记录，包括47列变量信息，其中15列为匿名变量，比赛界面有对应的数据概况介绍，说明列的性质特征。总数据量120万条，其中，训练集80万条，测试集A 20万条，测试集B 20万条。 预测指标：采用AUC作为评价指标，AUC越接近1.0，模型的预测性能越好。 2.二分类问题中常见的评估指标1.混淆矩阵（Confuse Matrix） 二分类问题的预测结果可以根据情况分成以下四类： （1）真正 TP（True Positive）：预测值为1，真实值为1 （2）假正 FP（False Positive）：预测值为1，真实值为0 （3）真负 TN（True Negative）：预测值为1，真实值为0 （4）假负 FN（False Negative）：预测值为0，真实值为1 12345import numpy as npfrom sklearn.metrics import confusion_matrixy_true = [0, 1, 0, 1]y_pred = [1, 1, 1, 0]confusion_matrix(y_true, y_pred) 12array([[0, 2], [1, 1]], dtype=int64) 12tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()(tn, fp, fn, tp) 1(0, 2, 1, 1) 2.准确率（Accuracy） 分类正确的样本数占总样本数的比例数。准确率在样本不均衡的数据集上不适用。 Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}3.精确率（Precision） 又称查准率，正确预测为正样本（TP）占预测为正样本（TP+FP）的比例。 Percision=\\frac{TP}{TP+FP}4.召回率（Recall） 又称查全率，正确预测为正样本（TP）占正样本的（TP+FN）比例。 Recall=\\frac{TP}{TP+FN}5.F1 - score Precision和Recall指标有时是此消彼长的，即精准率高了，召回率就下降，在一些场景下要兼顾精准率和召回率，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即； \\frac{1}{F_{\\beta}}=\\frac{1}{1+\\beta^2} \\cdot \\left( \\frac{1}{P}+\\frac{\\beta^2}{R}\\right) \\\\ F_{\\beta} = \\frac{(1+\\beta^2)\\times P \\times R}{(\\beta^2 \\times P) + R}当$\\beta=1$时，也就是常见的F1-Score，是P和R的调和平均，当F1较高时，模型的性能越好。 F1-Socre = \\frac{2\\times P \\times R }{P+R}123456789from sklearn import metricsy_true = [0, 1, 0, 1]y_pred = [1, 1, 1, 0]print('accuracy:', metrics.accuracy_score(y_true, y_pred))print('precision:', metrics.precision_score(y_true, y_pred))print('recall:', metrics.recall_score(y_true, y_pred))print('f1-score:', metrics.f1_score(y_true, y_pred)) 1234accuracy: 0.25precision: 0.3333333333333333recall: 0.5f1-score: 0.4 6.P-R曲线（Precision-Recall Curve） 描述精确率/召回率变化的曲线。 若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。 7.ROC曲线（Receiver Operating Characteristic） ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类别不平衡（Class Imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化，ROC以及AUC可以很好的消除样本类别不平衡对指标结果产生的影响。 ROC曲线分别使用下面两个指标作为X轴和Y轴： （1）真正率（True Positive Rate , TPR），又称灵敏度（sensitivity）：（其实和召回率一样） TPR = \\frac{TP}{TP+FN}（2）假正率（False Positive Rate , FPR），又称特异度（specificity）： FPR = \\frac{FP}{TN+FP} 8.AUC（Area Under Curve） 曲线下面积，是处于ROC Curve下方的那部分面积的大小。对于ROC曲线下方面积越大表明模型性能越好，于是AUC就是由此产生的评价指标。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了模型较好的性能。 1234567891011121314import matplotlib.pyplot as plt import numpy as npfrom sklearn import metricsy_true = np.array([1, 1, 2, 2])y_score = np.array([0.1, 0.4, 0.35, 0.8])fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label=2) auc=metrics.auc(fpr, tpr)plt.title('ROC')plt.plot(fpr, tpr,'b',label='AUC = %0.4f'% auc)plt.legend(loc='lower right')plt.plot([0,1],[0,1],'r--')plt.ylabel('TPR')plt.xlabel('FPR') 参考：【机器学习】一文读懂分类算法常用评价指标","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://nekomoon404.github.io/categories/Machine-Learning/"}],"tags":[]},{"title":"DL笔记（16）Transfer Learning","slug":"ML笔记（16）Transfer-Learning","date":"2020-08-13T00:51:13.000Z","updated":"2020-08-13T02:56:46.358Z","comments":true,"path":"2020/08/13/ML笔记（16）Transfer-Learning/","link":"","permalink":"http://nekomoon404.github.io/2020/08/13/ML%E7%AC%94%E8%AE%B0%EF%BC%8816%EF%BC%89Transfer-Learning/","excerpt":"","text":"1. IntroductionTransfer learning（迁移学习），是属于机器学习的一种研究领域，它专注于存储已有问题的解决模型，并将其利用在其他不同但相关问题上，正如人类可以将一个领域学习到的知识和经验，应用到其他相似的领域中去一样，机器同样也能做到。 传统的机器学习/数据挖掘只有在训练集数据和测试集数据都来自同一个feature space（特征空间）和统一分布的时候才运行的比较好，这意味着每一次换了数据都要重新训练模型，太麻烦了。比如： （1）从数据类型/内容上看，对于新的数据集，获取新的训练数据很贵也很难。 （2）从时间维度上看，有些数据集很容易过期，即不同时期的数据分布也会不同。 Transfer learning的概念参考了这篇博客迁移学习 not directly related 以猫狗识别为例，解释“不直接相关”的含义： input domain（域）是类似的，但task是无关的，比如输入都是动物的图像，但这些data是属于另一组有关大象和老虎识别的task； input domain是不同的，但task是一样的，比如task同样是做猫狗识别，但输入的是卡通类型的图像。 domain：包括两部分：1.feature space（特征空间）；2.probability（概率）。所以当我们说domain不同的时候，就得分两种情况。可能是feature space不同，也可能是feature space一样但probability不同。这里指的是前者，即feature space不同。 overview 迁移学习是很多方法的集合，这里介绍一些概念： Target Data：和task直接相关的data； Source Data：和task没有直接关系的data。 source是用于训练模型的域/任务，target是要用前者的模型对自己的数据进行预测/分类/聚类等机器学习任务的域/任务。 按照labeled data和unlabeled data又可以划分为四种： 2. labelled source data &amp; labeled target data这里target data和source data都是带有标签的： target data：$(x^t,y^t)$，作为有效数据，通常量是很少的。如果target data量非常少，则被称为one-shot learning； source data：$(x^s, y^s)$，作为不直接相关数据，通常量是很多的。 2.1. Model Fine-tuningModel Fine-tuning（模型微调）的基本思想：用source data去训练一个model，再用target data对model进行fine-tune（微调）。“微调”类似于pre-training，就是把用source data训练出的model参数当做是参数的初始值，再用target data继续训练下去即可，但当直接相关的数据量非常少时，这种方法很容易会出问题。所以训练的时候要小心，有许多技巧值得注意。 Conservation Training 如果现在有大量的source data，比如在语音识别中有大量不同人的声音数据，可以拿它去训练一个语音识别的神经网络，而现在你拥有的target data，即特定某个人的语音数据，可能只有十几条左右，如果直接拿这些数据去再训练，肯定得不到好的结果 此时我们就需要在训练的时候加一些限制，让用target data训练前后的model不要相差太多： 可以让新旧两个model在看到同一笔data的时候，output越接近越好； 或者让新旧两个model的L2 norm越小越好，参数尽可能接近； 总之让两个model不要相差太多，防止由于target data的训练导致过拟合。 这里的限制就类似于做regularization。 Layer Transfer 现在我们已经有一个用source data训练好的model，此时把该model的某几个layer拿出来复制到同样大小的新model里，接下来用target data去训练余下的没有被复制到的layer。这样做的好处是target data只需要考虑model中非常少的参数，这样就可以避免过拟合。 这个对部分layer进行迁移的过程，就体现了迁移学习的思想，那么哪些layer需要被复制迁移，哪些不需要呢？ 值得注意的是，在不同的task上，需要被复制迁移的layer往往是不一样的： 在语音识别中，往往迁移的是最后几层layer，再重新训练与输入端相邻的那几层。 由于人口腔结构不同，同样的发音方式得到的发音是不一样的，NN的前几层会从声音信号里提取出发音方式，再用后几层判断对应的词汇，从这个角度看，NN的后几层是跟特定的人没有关系的，因此可做迁移。 在图像处理中，往往迁移的是前面几层layer，再重新训练后面的layer。 CNN在前几层通常是做最简单的识别，比如识别是否有直线斜线、是否有简单的几何图形等，这些layer的功能是可以被迁移到其它task上通用的。 主要还是具体问题具体分析。 2.2. Multitask LearningFine-tune仅考虑在target data上的表现，而Multitask Learning（多任务学习），则是同时考虑model在source data和target data上的表现。 如果两个task的输入特征类似，则可以用同一个神经网络的前几层layer做相同的工作，到后几层再分方向到不同的task上，这样做的好处是前几层得到的data比较多，可以被训练得更充分。有时候task A和task B的输入输出都不相同，但中间可能会做一些类似的处理，则可以让两个神经网络共享中间的几层layer，也可以达到类似的效果。 以上方法要求不同的task之间要有一定的“共性”，这样才有共用一部分layer的可能性。 Multilingual Speech Recognition 多任务学习可以应用在语音识别上，比如可以同时对法语、德语、西班牙语、意大利语训练一个model，它们在前几层layer上共享参数，而在后几层layer上拥有自己各自的参数。在机器翻译上也可以使用同样的思想，比如训练一个同时可以中翻英和中翻日的model。 注属于同一个语系的语言翻译，比如欧洲国家的语言，几乎都是可以做迁移学习的；而语音方面则可迁移的范围更广。下图展示了只用普通话的语音数据和加了欧洲语言后的语音数后得到的错误率对比，其中横轴为使用的普通话数据量，纵轴为错误率，可以看出使用了迁移学习后，只需要原先一半的普通话语音数据就可以得到几乎一样的准确率 2.3. Progressive Neural Network如果两个task完全不相关，硬是把它们拿来一起训练反而会起到负面效果。而在Progressive Neural Network（渐进式神经网络）中，每个task对应model的hidden layer的输出都会被接到后续model的hidden layer的输入上，这样做的好处是： task 2的data并不会影响到task 1的model，因此task 1一定不会比原来更差； task 2虽然可以借用task 1的参数，但可以将之直接设为0，最糟的情况下就等于没有这些参数，也不会对本身的表现产生影响； task 3也做一样的事情，同时从task 1和task 2的hidden layer中得到信息。 论文arxiv.org/pdf/1606.04671.pdf) 关于Progressive Neural Network可以参考：论文笔记之：Progressive Neural Network Google DeepMind；Progressive Neural Network 3. labelled source data &amp; unlabeled target data下面介绍target data不带标签，而source data带标签的情况： target data：$(x^t)$ source data：$(x^s, y^s)$ 3.1. Domain-adversarial Training如果source data是有label的，而target data是没有label的，该怎么处理呢？比如source data是labeled MNIST数字集，而target data则是加了颜色和背景的unlabeled数字集，虽然都是做数字识别，但两者的情况是非常不匹配的。 这个时候一般会把source data当做训练集，而target data当做测试集，如果不管训练集和测试集之间的差异，直接训练一个普通的model，得到的结果准确率会相当低。实际上，神经网络的前几层可以被看作是在抽取feature，后几层则是在做classification。如果把用MNIST训练好的model所提取出的feature做t-SNSE降维后的可视化，可以发现MNIST的数据特征明显分为紫色的十团，分别代表10个数字，而作为测试集的数据却是挤成一团的红色点，因此它们的特征提取方式根本不匹配。 所以我们希望前面的特征提取器(feature extractor)可以把domain的特性去除掉，不再使红点与蓝点分成两群，而是让它们都混在一起。这样我们就可以将用黑白MNIST训练好的model用在彩色MNIST数据上。 这里采取的做法是，在特征提取器(feature extractor)之后接一个域分类器(domain classifier)，以便分类出这些提取到的feature是来自于MNIST的数据集还是来自于MNIST-M的数据集，这个生成+辨别的架构与GAN非常类似。 只不过在这里，feature extractor可以通过把feature全部设为0，很轻易地骗过domain classifier，因此还需要给feature classifier增加任务的难度，它不只要骗过domain classifier，还要同时满足label predictor的需求。 此时通过特征提取器得到的feature不仅可以消除不同domain的特性，还要保留原先digit的特性，既可以区分不同类别的数字集，又可以正确地识别出不同的数字。 通常神经网络的参数都是朝着最小化loss的目标共同前进的，但在这个神经网络里，三个组成部分的参数“各怀鬼胎”： 对Label predictor，要把不同数字的分类准确率做的越高越好； 对Domain classifier，要正确地区分某张image是属于哪个domain； 对Feature extractor，要提高Label predictor的准确率，但要降低Domain classifier的准确率。 这里可以看出，Feature extractor和Domain classifier的目标是相反的，要做到这一点，只需要在两者之间加一层梯度反转的layer即可（给domain classifier的梯度乘一个$-\\lambda$），当NN做backward的时候，两者的参数更新往相反的方向走。 注意到，Domain classifier只能接受到Feature extractor给到的特征信息，而无法直接看到图像的样子，因此它最后一定会鉴别失败，所以如何提高Domain classifier的能力，让它经过一番“奋力挣扎”之后才牺牲是很重要的，如果它一直很弱，就无法把Feature extractor的潜能激发到极限。 3.2. Zero-shot Learning同样是source data有label，target data没有label的情况，但在Zero-shot Learning中的定义更严格一些，它假设source和target是两个完全不同的task，数据完全不相关。 在语音识别中，经常会遇到这个问题，毕竟词汇千千万万，总有一些词汇是训练时不曾遇到过的，它的处理方法是不要直接将识别的目标定成word，而是定成phoneme(音素)，再建立文字与phoneme之间的映射表即可。 在图像处理中，我们可以把每个类别都用其属性（attribute）表示，并且要具备独一无二的属性，在数据集中把每种动物按照特性划分，比如是否毛茸茸、有几只脚等，在训练的时候我们不直接去判断类别，而是去判断该图像的属性，再根据这些属性去找到最契合的类别即可。 有时候属性的维数也很大，以至于我们对属性要做embedding的降维映射，同样的，还要把训练集中的每张图像都通过某种转换投影到embedding space上的某个点，并且要保证属性投影的$g(y^i)$和对应图像投影的$f(x^i)$越接近越好，这里的$f()$和$g()$可以是两个神经网络。当遇到新的图像时，只需要将其投影到相同的embedding space，即可判断它与哪个属性对应的类别更接近。 但如果我们根本就无法找出每个动物的属性$y^i$是什么，那该怎么办？可以使用word vector，比如直接从维基百科上爬取图像对应的文字描述，再用word vector降维提取特征，映射到同样的空间即可。 以下这个loss function存在些问题，它会让model把所有不同的x和y都投影到同一个点上： f^*,g^*=\\arg \\min\\limits_{f,g} \\sum\\limits_n ||f(x^n)-g(y^n)||_2类似用t-SNE的思想，我们既要考虑同一对$x^n$和$y^n$距离要接近，又要考虑不属于同一对的$x^n$与$y^m$距离要拉大(这是前面的式子没有考虑到的)，于是有： f^*,g^*=\\arg \\min\\limits_{f,g} \\sum\\limits_n \\max(0, k-f(x^n)\\cdot g(y^n)+\\max\\limits_{m\\ne n} f(x^n)\\cdot g(y^m))其中$\\max()$项的最小值是0，当： k-f(x^n)\\cdot g(y^n)+\\max\\limits_{m\\ne n} f(x^n)\\cdot g(y^m)k就表明此时$f(x^n)$和$g(y^n)$的inner product很大，即两者很接近，而$f(x^n)$和其他的$g(y^m)$即差的很远，它们的inner product很小。 convex combination of semantic embedding 还有另外一个简单的Zero-Shot learning的方法叫做convex combination of semantic embedding。假设我们现在有一个语音辨识系统，有一个word vector，这两个是从网络上下载下来的，就可以做这件事情。 我把一张图丢到neural network里面去，它的output没有办法决定是哪一个class，但它觉得有0.5的几率是lion，有0.5的几率是tiger。接下来你在去找lion跟tiger的word vector，然后把lion跟tiger的word vector得到新的vector(用1:1的比例混合,0.5V(tiger)+0.5V(lion))，那你再看哪一个word的vector跟这个混合之后的结果最接近。假设是liger最接近，那这个东西就是liger(狮虎)。这样就省去了Training。 Zero-shot Learning in Machine Translation 下面是一个机器翻译的例子，Google Neural Machine Translation。在training的时候，machine看过如何把英文翻译成韩文，知道怎么把韩文翻译为英文，知道怎么把英文翻译为日文，知道怎么把日文翻译为英文。但是它从来没有看过日文翻译韩文的data，但是可以翻，但是它从来没有看过韩文翻译日文的data，但是可以翻。 为什么zero-shot在这个task上是可行的呢？如果你今天用同一个model做了不同语言之间的translation以后，machine可以学到的事情是：对不同语言的input 句子都可以project（投影）到同一个space上面。句子在这个space上的位置只跟句子的semantic有关。 比如现在根据learn好的translation，那个translation有一个encoder，它会把input的句子变成vector，decoder根据这个vector解回一个句子，就是翻译的结果。那把不同语言都丢到这个encoder里面让它变成vector的话，那这些不同语言的不同句子在这个space上面有什么不一样的关系呢？ 它发现有日文、英文、韩文这三个句子，这三个句子讲的是同一件事情，通过encoder embedding之后，它们在space上面是差不多的位置。machine做的是发现一个sequence language，每一种不同的语言都先要先转成它知道的sequence language，在用这个sequence language转为另外一种语言。所以对某一个翻译task ，你的input语言和output语言machine没有看过，它也可以透过这种自己学出来的sequence language来做translation。 —————— 最后简单介绍下Transfer Learning的另外两种情况： Target data有label，source data没有label: Self-taught learning。它的基本思想是： Learning to extract better representation from the source data(unsupervised approach) Extracting better representation for target data Self-taught learning和semi-supervised learning有些不一样的地方，semi-supervised learning在learning的时候会有一些labelled data和unlabeled data，可以说source data是unlabeled data，target data是label data，所以Self-taught learning也是一种semi-supervised learning。但它和一般的semi-supervised learning有些不一样，一般的semi-supervised learning会假设unlabeled data至少和labelled data是有关系的，但在Self-taught learning中，source data和target data的关系是比较远的。 Target label没有label，source data也没有label: Self-taught clustering。 论文：Self-taught learning；Self-taught clustering","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://nekomoon404.github.io/categories/Deep-Learning/"}],"tags":[{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"http://nekomoon404.github.io/tags/Transfer-Learning/"},{"name":"Model Fine-tuning","slug":"Model-Fine-tuning","permalink":"http://nekomoon404.github.io/tags/Model-Fine-tuning/"},{"name":"Multitask Learning","slug":"Multitask-Learning","permalink":"http://nekomoon404.github.io/tags/Multitask-Learning/"},{"name":"Progressive Neural Network","slug":"Progressive-Neural-Network","permalink":"http://nekomoon404.github.io/tags/Progressive-Neural-Network/"},{"name":"Domain-adversarial Training","slug":"Domain-adversarial-Training","permalink":"http://nekomoon404.github.io/tags/Domain-adversarial-Training/"}]},{"title":"DL笔记（15）Unsupervised Learning-Generative Model","slug":"ML笔记（15）Unsupervised-Learning-Generative-Model","date":"2020-07-29T07:59:14.000Z","updated":"2020-08-12T07:59:14.000Z","comments":true,"path":"2020/07/29/ML笔记（15）Unsupervised-Learning-Generative-Model/","link":"","permalink":"http://nekomoon404.github.io/2020/07/29/ML%E7%AC%94%E8%AE%B0%EF%BC%8815%EF%BC%89Unsupervised-Learning-Generative-Model/","excerpt":"","text":"关于Generative Model推荐一篇很好的文章，来自OpenAI的Generative Models。文章的开头引用了Richard Feynman的话，“What I cannot create, I do not understand”，我无法创造的东西，我也无法真正理解，机器可以做猫狗分类，但却不一定知道“猫”和“狗”的概念，但如果机器能自己画出“猫”来，它或许才真正理解了“猫”这个概念，这也是Generative Model想要让machine做的事。 下面将简要介绍：PixelRNN、VAE和GAN这三种Generative model的方法。 1. PixelRNNRNN可以处理长度可变的input，它的基本思想是可以根据过去发生的状态去推测下一个状态。PixelRNN的基本思想是每次只画一个pixel来生成一个image，这个pixel是由过去所有已产生的pixel共同决定的。例如一个$3\\times 3$的Image，第一次给一个橙色的pixel，输入到NN中，output得到一个蓝色的pixel；然后再将上一步得到的橙色和蓝色的pixel一起输入到NN中得到一个浅蓝色的pixel；再下一步将这三个pixel输入到NN中得到一个灰色的pixel，以此类推就可以得到一个$3\\times 3$的image。这种方法的精髓在于根据过去预测未来，画出来的图一般都是比较清晰的 （Reference[1]: Oord A, Kalchbrenner N, Kavukcuoglu K. Pixel recurrent neural networks[J]. arXiv preprint arXiv:1601.06759, 2016.） 这个方法也适用于语音生成，可以用前面一段的语音去预测接下来生成的语音信号。也可以用在影像上，用前面一段video来预测后面的video。 （Reference[2]: Oord A, Dieleman S, Zen H, et al. Wavenet: A generative model for raw audio[J]. arXiv preprint arXiv:1609.03499, 2016. Reference[3]: Kalchbrenner N, Oord A, Simonyan K, et al. Video pixel networks[C]//International Conference on Machine Learning. 2017: 1771-1779.） pokemon creation 下面这个小例子是给machine792个pekemon的image，想让machine学会去生成pekeon的Image。 在使用Generative model去生成宝可梦之前，有几个tips需要注意： 为了减少运算量，将40×40的图像截取成20×20 如果将每个pixel都以[R, G, B]的vector表示的话，生成的图像都是灰蒙蒙的，原因如下： 亮度比较高的图像，一般都是RGB值差距特别大而形成的，如果各个维度的值大小比较接近，则生成的图像偏向于灰色； 如果用sigmoid function，最终生成的RGB往往都是在0.5左右（归一化之后），导致色彩度不鲜艳； 解决方案：将所有色彩集合成一个1-of-N encoding，由于色彩种类比较多，因此这里先对相近的颜色做clustering聚类表示为一种颜色，最终获得了167种色彩组成的向量。我们用这样的向量去表示每个pixel，可以让生成的色彩比较鲜艳。 相关数据连接如下： 原始图像(40*40)数据的链接 裁剪后的图像(20*20)数据链接 数值与色彩(RGB)的映射关系链接 使用PixelRNN训练好模型之后，给它看没有被放在训练集中的3张图像的一部分，分别遮住原图的50%和75%，得到的原图如下： 训练好的pixel RNN预测到的图片如下： 做这种Generation的task有一个难点是，设计上的好坏较难去evaluate。接下来我们让训练好的model凭空去画，但如果什么都不给machine让它从头开始画的话，它得到的每一个image可能都是一样的，因此我们要故意加一些random，即machine在画下一个pixel的时候不一定是选几率最高的颜色，也有几率选几率比较低的颜色，这样它每次画出来的图才会有点不一样。 2. VAE上一篇笔记中介绍过Auto-encoder，如果我们拿出其中的Decoder，给它随机的code，就可以生成对应的图像。但普通的Decoder生成效果并不好，VAE（Variational Auto-encoder，可变自动编码器）可以得到更好的效果。 在VAE中，code不再直接等于Encoder的输出，这里假设目标降维空间为3维，那我们使Encoder分别输出$m_1,m_2,m_3$和$\\sigma_1,\\sigma_2,\\sigma_3$，此外我们从正态分布中随机取出三个点$e_1,e_2,e_3$，将下式作为最终的编码结果： c_i = \\exp(\\sigma_i)\\cdot e_i+m_i 再将$c_i$输入到Decoder里面得到output，我们希望MInimize reconstruction error。但此时，我们的训练目标不仅要最小化input和output之间的差距，还要同时最小化下式（比较“神妙”）： \\sum\\limits_{i=1}^3 (1+\\sigma_i-(m_i)^2-e^{\\sigma_i})2.1. Pokemon Creation与PixelRNN不同的是，VAE画出的图一般都是不太清晰的，但在某种程度上我们可以控制生成的image。假设我们现在把VAE用在Pokemon creation上面，在Trainig的时候我们input一个pokemon，然后reconstruct一个同样的pokemon，learn出来的code设为10维。Learn好这个VAE之后，我们把Decoder的部分拿出来。我们在给Decoder输入10维的vector时可以固定其中的8维，只选2维出来，我们可以在2维平面上sample一系列的点，加上我们固定的8维后Input到Decoder中看输出的image是什么样的。这样我们就可以解读code的每一个dimension是代表什么含义，然后去控制VAE去生成一些image。 下面是固定code中的8维，让剩下的2维变化得到的image，发现image的变化确实是有些规律的。 2.2. Writing PoertyVAE还可以用来写诗，将input和output都换成是sentence，我们只需要得到某两句话对应的code，然后在降维后的空间中得到这两个code所在点的连线，从中间等间隔地取一些点，把这些点输入给Decoder，得到还原后的句子，就可以得到类似下图中的效果。 2.3. Why VAE?VAE和传统的Auto-encoder相比，有什么优势呢？事实上，VAE就是加了噪声noise的Autoencoder，它的抗干扰能力更强，过渡生成能力也更强。 对原先的Autoencoder来说，假设我们得到了满月和弦月的code，从两者连线中随机获取一个点并映射回原来的空间，得到的图像很可能是完全不一样的东西，因为code和output得到的image是一一对应的。 而对VAE来说，它要保证在降维后的code空间中，加了noise的一段范围内的所有点都能够映射到目标图像，如下图所示，当某个点既被要求映射到满月、又被要求映射到弦月，则它最终映射出来的结果就很有可能是两者之间的过渡图像。 再回过来头看VAE的结构，其中： $m_i$其实就代表原来的code $c_i$则代表加了noise以后的code $\\sigma_i$代表了noise的variance，描述了noise的大小，这是由NN学习到的参数 （注：使用$\\exp(\\sigma_i)$的目的是保证variance是正的） $e_i$是正态分布中随机采样的点 注意到，损失函数仅仅让input和output差距最小是不够的，因为variance是由机器自己决定的，如果不加以约束，它自然会去让variance=0，这就跟普通的Autoencoder没有区别了。 额外加的限制函数解释如下： 下图中，蓝线表示$e^{\\sigma_i}$，红线表示$1+\\sigma_i$，两者相减得到绿线。绿线的最低点$\\sigma_i=0$，则variance $e^{\\sigma_i}=1$，此时loss最低，而$(m_i)^2$项则是对code的L2 regularization，让它比较sparse，不容易过拟合。 上面是比较直观的理由，以下是paper上比较常见的解释。我们回归到我们要做的事情是什么，假设要machine generate pokemon的image，那每一张pokemon的图都可以想成是高维空间中的一个点。假设它是20*20的image，在高维空间中也就是400维的点，在图上我们用一维描述它，但其实是一个高维的空间。那现在要做的就是estimate p(x)的分布，它表示一张图片像宝可梦的几率，然后就可以根据p(x)高的地方去sample出一张像宝可梦的图。 Estimate the probability distributon可以用Gaussion mixture model。Guassion mixture model：现在有一个distribution(黑色的线)，这个黑色的distribution其实是很多的Gaussion(青蓝色)用不同的weight叠合起来的结果。如果你的gaussion数目够多，你就可以产生很复杂的distribution，公式为 $p(x)=\\sum_{m}p(m)p(x|m)$ 。 这样每一个$x$并不属于某一个class或者cluster，而是有一个vector来描述它不同面向的disstribution，描述它不同面向的特性，VAE其实就是Gaussian Mixture Model的Distributed representation的版本。 假设我们要sample一个vector $z$，$z$是从normal distribution中sample出来的，$z$的每一个dimension都代表了某种attribute（特质，特性）。由$z$可以决定Gaussian的mean $\\mu$和variance $\\sigma$，由于$z$是continous的，所有它有无穷的可能，那mean和variance也有无穷多的可能。 $P(x)$的曲线是这样产生的：$z$上的每一个点都有可能被sample到，当sample出一个点后它就会对应到一个Gaussian，把它们mixture起来就得到了$P(x)$，即$P(x)=\\int \\limits_{z}P(z)P(x|z)dz $（注意因为$z$是continous的，所以这里要用积分，而不是sum）。 2.4. Maximizing Likelihood那给出一个$z$怎么找出mean和variance呢，假设mean和variance都来自一个function，这个function就可以是一个NN。当然$z$的分布不一定是Gaussian，可以自己设定。那训练这个NN的目标就是要Maximiza the likelihood of the observed $x$，即使下式最大： L=\\sum \\limits_{x}\\log P(x) 我们要做的就是调NN里的参数weight和bias，使得likelihood最大。然后我们需要引入另一个distribution $q(z|x)$，它是given $x$来决定在$z$的space上的mean和variance，还需要有另外一个NN’，input $x$之后会output对应的$z$的mean $\\mu’(x)$和variance $\\sigma’(x)$，即决定$z$要从什么样的mean和variance中被sample出来。 上图中上面的NN就相当于是VAE中的Decoder，下面的NN就相当于是VAE中的ENcoder。 下面是对$L=\\sum \\limits_{x}\\log P(x)$的表达式的具体的推导： 推导$\\log P(x)=L_b+KL(q(z|x)||P(z|x))$： 我们本来要做的是找使得$L$最大的$P(x|z)$，现在转换为求使$L_b$最大的$P(x|z)$和$q(z|x)$。 如果我们只找$p(x∣z)$ 这一项的话，然后去maximizing $L_b$ ，当增加$L_b$的时候，有可能会增加likehood，但不知道likehood跟lower bound之间到底有还差多少距离。你希望你做到的是：当lower bound上升的时候，likehood也跟着上升。但是有可能会遇到糟糕的事情是：lower bound上升的时候，likehood反而下降，因为不知道它们之间的差距是多少。 引入$q(z|x)$这一项就是为了解决这一问题。如上图中蓝色的是likehood， $\\log P(x)=L_b+KL$，如果你今天调 $q(z|x)$来maximizing $L_b$，会发现$q(z|x)$跟$\\log P(x)$是没有关系的，即ikelihood不变，那maximizing $L_b$的同时也在minimize KL divergence，即让lower bound（$L_b$）跟likehood越来越接近。如果固定住 $p(x|z)$这一项，去调 $q(z|x)$这一项的话，会让$L_b$ 一直上升，直到KL divergence会完全不见。由于likehood一定要比lower bound大，这时你再调$p(x|z)$和$q(z|x)$来maximizing $L_b$的话，就可以保证likehood会一定增大。 这样做也会得到一个副产物，当maximize $L_b$这一项的时候，会让KL divergence越来越小，意味着会让 $q(z|x)$ 跟 $p(z|x)$越来越接近。所以接下来做的事情就是找$p(x|z)$跟$q(z|x)$，可以让$L_b$越大越好，就等同于让likehood越来越大，最后顺便会得到$q(z|x)$可以去approximation $p(z|x)$。 而$q$是个neural network，要去minimize $KL(q(z|x)||P(z))$就是去调NN‘让它产生的distribution和normal distribution越接近越好，而loss function就是之前讲过的那个式子$\\sum \\limits^{3} \\limits_{i=1}(\\exp(\\sigma_i)-(1+\\sigma_i)+(m_i)^2)$（这部分的推导可以参考VAE的原始论文）。 另外一项是转化为$\\log P(x|z))$的期望值： P(x)=\\int \\limits_{z}P(z)P(x|z)dz=E_{q(z|x)}[\\log P(x|z)]可以理解是我们从$q(z|x)$中去sample data，要让$\\log P(x|z)$越大越好，这其实就是Auto-encoder在做的事情。你可以把$x$输入到NN’中得到mean $\\mu’(x)$和variance $\\sigma’(x)$，根据这个mean和variance可以sample出一个$z$；接下来把z输入到NN，得到mean $\\mu(x)$和variance $\\sigma(x)$，我们的目标是让这个mean和variance代表的distribution是$x$的几率越大越好，在实际使用中往往不考虑variance，那我们就是让最后输出的mean和$x$越接近越好，这不就是Auto-encoder在做的事情。 Conditional VAE 还有一种方法叫Conditional VAE，举个例子，如果你让VAE可以产生手写的数字，给它一个digit，然后它把这个digit的特性抽取出来(笔画的粗细等等)，然后丢进encoder的时候一方面给它有关这个数字特性的distribution，另外一方面告诉decoder它是什么数字。那你就可以根据这一个digit，generate跟它style相近的digit。Conditional VAE可以根据某一个digit画出跟它style相近的数字。 Problems of VAE VAE有一个缺点，它只是在努力做到让生成的图像与数据集里的图像尽可能相似，却从来没有想过怎么样真的产生一张新的图像，因此由VAE生成的图像大多是数据集中图像的线性变化，而很难自主生成全新的图像。假设Decoder output跟真正的image之间有一个pixel的差距，那有时不同的pixel落在不同的位置会得到非常不一样的结果，如下图中的两个数字“7”，人很容易发现其区别：左边的像是真的数字，而右边明显是fake。但对VAE来说，它们只是有一个pixel的差异，两张image都是一样好或者不好的。VAE做到的只是模仿，而不是创造，GAN的诞生，就是为了创造。 3. GANGenerative Adversarial Network（GAN，对抗生成网络），基本思想类似天敌之间相互竞争，相互进步。 GAN由生成器(Generator)和判别器(Discriminator)组成： 对判别器的训练：把生成器产生的图像标记为0，真实图像标记为1，丢给判别器训练分类，希望它能分辨real image和fake image； 对生成器的训练：调整生成器的参数，使产生的图像（fake image）能够“骗过”判别器，即判别器输出越接近1越好； 每次训练GAN时生成器和判别器要分开训练：先Fix住生成器，训练判别器的参数；再Fix 判别器，训练生成器的参数，如此反复。 （PS：李老师后面会有专门介绍GAN的课程，之后再做详细记录吧。）","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://nekomoon404.github.io/categories/Deep-Learning/"}],"tags":[{"name":"Pixel RNN","slug":"Pixel-RNN","permalink":"http://nekomoon404.github.io/tags/Pixel-RNN/"},{"name":"VAE","slug":"VAE","permalink":"http://nekomoon404.github.io/tags/VAE/"},{"name":"GAN","slug":"GAN","permalink":"http://nekomoon404.github.io/tags/GAN/"}]},{"title":"DL笔记（14）Unsupervised Learning-Deep Auto-encoder","slug":"ML笔记（14）Unsupervised-Learning-Deep-Auto-encoder","date":"2020-07-29T02:31:01.000Z","updated":"2020-07-29T04:31:01.000Z","comments":true,"path":"2020/07/29/ML笔记（14）Unsupervised-Learning-Deep-Auto-encoder/","link":"","permalink":"http://nekomoon404.github.io/2020/07/29/ML%E7%AC%94%E8%AE%B0%EF%BC%8814%EF%BC%89Unsupervised-Learning-Deep-Auto-encoder/","excerpt":"","text":"1. Auto-encoderAuto-encoder本质上就是一个自我压缩和解压的过程，比如在做图像处理时，我们想要获取压缩后的code，它代表了对原始数据的某种紧凑精简的有效表达，即降维结果，这个过程中我们需要： Encoder(编码器)，它可以把原先的图像压缩成更低维度的向量； Decoder(解码器)，它可以把压缩后的向量还原成图像； 注意到，Encoder和Decoder都是Unsupervised Learning，由于code是未知的，对Encoder来说，我们手中的数据只能提供图像作为NN的input，却不能提供code作为output；对Decoder来说，我们只能提供图像作为NN的output，却不能提供code作为input。 因此Encoder和Decoder单独拿出一个都无法进行训练，我们需要把它们连接起来，这样整个神经网络的输入和输出都是我们已有的图像数据，就可以同时对Encoder和Decoder进行训练，而降维后的编码结果就可以从最中间的那层hidden layer中获取。 1.1. Compare with PCA实际上PCA用到的思想与之非常类似，PCA的过程本质上就是按组件拆分，再按组件重构的过程。在PCA中，假设input一张image $x$（本应该是把$x-\\bar{x}$当做input，这边我们把$\\bar{x}$省略掉，通常在做NN的时候，你拿到的data其实会normalization的，即data的mean是0）。我们先把$x$乘上weight $W$，通过NN的一个layer得到得到component的weight $c$，然后再乘上$W^T$得到重组后的$\\hat x$，同样我们期望重构后的$\\hat x$与原始的$x$越接近越好，即Minimize $(x-\\hat{x})^2$。 如果把这个过程看作是神经网络，那么原始的$x$就是input layer，重构$\\hat x$就是output layer，中间组件分解权重$c$就是hidden layer，在PCA中它是linear的，我们通常又叫它瓶颈层(Bottleneck layer) 由于经过组件分解降维后的$c$，维数要远比输入输出层来得低，因此hidden layer实际上非常窄，因而有Bottleneck layer的叫法。对比于Auto-encoder，从input layer到hidden layer的按组件分解实际上就是编码(encode)过程，从hidden layer到output layer按组件重构实际上就是解码(decode)的过程。 这时候你可能会想，可不可以用更多层hidden layer呢？答案是肯定的 1.2. Deep Auto-encoderMulti Layer 对deep的自编码器来说，实际上就是通过多级编码降维，再经过多级解码还原的过程，此时： 从input layer到bottleneck layer的部分都属于Encoder​； 从bottleneck layer到output layer的部分都属于Decoder​； bottleneck layer的output就是自编码结果code​。 （paper: Hinton G E, Salakhutdinov R R. Reducing the dimensionality of data with neural networks[J]. science, 2006, 313(5786): 504-507.） Training这个Deep Auto-encoder的方法就是用之前讲过的Backpropagation，在多层layer的中间会有一层特别“窄”的layer，即为Bottleneck layer，它的output就代表了一组Code。从整个NN的input到Bottleneck layer就是Encoder，从Bottleneck layer的output到整个NN的output就是Decoder。 注意到，如果按照PCA的思路，则Encoder的参数$W_i$需要和Decoder的参数$W_i^T$保持一致的对应关系，这可以通过给两者相同的初始值并设置同样的更新过程得到，这样做的好处是，可以节省一半的参数，降低overfitting的概率。但这件事情并不是必要的，实际操作的时候，你完全可以对神经网络进行直接训练而不用保持编码器和解码器的参数一致 Visualize 下图给出了Hinton分别采用PCA和Deep Auto-encoder对手写数字进行编码解码后的结果，从784维降到30维，然后再从30维reconstruct到784维，可以看出，Deep的自编码器还原效果比PCA要更好。 如果将其降到二维平面做可视化，不同颜色代表不同的数字，可以看到： 通过PCA降维得到的编码结果中，不同颜色代表的数字被混杂在一起； 通过Deep Auto-encoder降维得到的编码结果中，不同颜色代表的数字被分散成一群一群的。 2. More Application2.1. Text RetrievalAuto-encoder也可以被用在文字处理上，用Auto-encoder把一篇文章压成code。比如我们要做文字检索，很简单的一个做法是Vector Space Model，把每一篇文章都表示成空间中的一个vector。 假设查询者输入了某个词汇，那我们就把该查询词汇也变成空间中的一个点，并计算query和每一篇document之间的内积 inner product 或余弦相似度 cos-similarity（余弦相似度有均一化的效果，可能会得到更好的结果）。下图中跟query向量最接近的几个向量的cosine-similarity是最大的，于是可以从这几篇文章中去检索。实际上这个模型的好坏，就取决于从document转化而来的vector的好坏，它是否能够充分表达文章信息。 Bag-of-word 最简单的vector表示方法是Bag-of-word，维数等于所有词汇的总数，某一维等于1则表示该词汇在这篇文章中出现，此外还可以根据词汇的重要性在对应的维上乘weight。但这个模型是非常脆弱的，对它来说每个词汇都是相互独立的，无法体现出词汇之间的语义(semantic)。 Auto-encoder 虽然Bag-of-word不能直接用于表示文章，但我们可以把它作为Auto-encoder的input，通过降维来抽取有效信息，以获取所需的vector。同样为了可视化，这里将Bag-of-word降维到二维平面上，下图中每个点都代表一篇文章，不同颜色则代表不同的文章类型，发现同一类文章都有较好地聚集在一起。 如果用户做查询，就把查询的语句（query）用相同的方式映射到该二维平面上，并找出属于同一类别的所有文章即可。在矩阵分解(Matrix Factorization)中，我们介绍了LSA算法，它可以用来寻找每个词汇和每篇文章背后的隐藏关系(vector)，如果在这里我们采用LSA，并使用二维latent vector来表示每篇文章，得到的可视化结果如上图右下角所示，可见效果并没有Auto-encoder好。 2.2. Similar Image SearchAuto-encoder同样可以被用在图像检索（Image Search）上。以图找图最简单的做法就是直接对输入的图片与数据库中的图片计算pixel的相似度，并挑出最像的图片，但这种方法的效果是不好的，因为单纯的pixel所能够表达的信息太少了。 我们需要使用Deep Auto-encoder对图像进行降维和特征提取，并在编码得到的code所在空间做检索。下图展示了Encoder的过程，并给出了原图与Decoder后的图像对比。因为Auto-encoder是unsupervised的方法，所有通常我们不必担心数据量的问题。 这么做的好处如下： Auto-encoder可以通过降维提取出一张图像中最有用的特征信息，包括pixel与pixel之间的关系； 降维之后数据的size变小了，这意味着模型所需的参数也变少了，同样的数据量对参数更少的模型来说，可以训练出更精确的结果，一定程度上避免了过拟合的发生； Auto-encoder是一个无监督学习的方法，数据不需要人工打上标签，这意味着我们只需简单处理就可以获得大量的可用数据； 下图给出了分别以原图的pixel计算相似度和以auto-encoder后的code计算相似度的两种方法在图像检索上的结果，可以看到，通过pixel检索到的图像会出现很多奇怪的物品，而通过code检索到的图像则都是人脸 可能有些人脸在原图的pixel上看起来并不像，但把它们投影到256维的空间中却是相像的，可能在投影空间中某一维就代表了人脸的特征，因此能够被检索出来。 2.3. Pre-training DNN在训练神经网络的时候，我们一般都会对如何初始化参数比较困扰，预训练(pre-training)是一种寻找比较好的参数初始化值的方法，而我们可以用Auto-encoder来做pre-training。 以MNIST数据集为例，我们对每层hidden layer都做一次auto-encoder，使每一层都能够提取到上一层最佳的特征向量 为了方便表述，这里用$x-z-\\widetilde{x}$来表示一个自编码器，其中$x$表述输入输出层的维数，$z$表示隐藏层的维数。 首先使input通过一个$784-1000-784$的自编码器，当该自编码器训练稳定后，就把参数$W^1$固定住，然后将数据集中所有784维的图像都转化为1000维的vector 注意：这里encoder做的不是降维而是升维，当编码后的维数比输入维数要高时，需要注意可能会出现编码前后原封不动的情况，$W^1$的一部分就是个identity matrix。为此需要额外加一个很强的正则项regularization，比如L1 regularization，强迫使code的分布是分散的。 接下来把训练好的Auto-encoder中的$W^1$保留下来，再让这些1000维的vector通过一个$1000-1000-1000$的编码器，其$a^1$与$\\widetilde{a}^1$越接近越好，当其训练稳定后，再把参数$W^2$保留下来fix住，对数据集再做一次转换。 接下来再用转换后的数据集去训练第三个$1000-500-1000$的自编码器，训练稳定后固定$W^3$，数据集再次更新转化为500维。 此时三个隐藏层的参数$W^1$、$W^2$、$W^3$就是训练整个神经网络时的参数初始值； 然后随机初始化最后一个隐藏层到输出层之间的参数$W^4$； 再用反向传播去调整一遍参数，因为$W^1$、$W^2$、$W^3$都已经是很好的参数值了，这里只是做微调，这个步骤也因此得名为Fine-tune。 由于现在训练机器的条件比以前更好，因此pre-training并不是必要的，但它也有自己的优势。如果你只有大量的unlabeled data和少量的labeled data，那你可以先用这些unlabeled data把$W^1$、$W^2$、$W^3$先初始化好，最后再用labeled data去微调$W^1$~$W^4$即可。因此pre-training在有大量unlabeled data的场景下(如半监督学习)是比较有用的。 2.4. CNNCNN as Encoder 处理图像通常都会用卷积神经网络CNN，它的基本思想是交替使用卷积层和池化层，让图像越来越小，最终展平，这个过程跟Encoder编码的过程其实是类似的 理论上要实现自编码器，Decoder只需要做跟Encoder相反的事即可，那对CNN来说，解码的过程也就变成了交替使用去卷积层Deconvolution和去池化层Unpooling即可。 那什么是去卷积层(Deconvolution)和去池化层(Unpooling)呢？ Unpooling 做pooling的时候，假如得到一个4×4的matrix，就把每4个pixel分为一组，从每组中挑一个最大的留下，此时图像就变成了原来的四分之一大小。如果还要做Unpooling，就需要提前记录pooling所挑选的pixel在原图中的位置，下图中用灰色方框标注。 然后做Unpooling，就要把当前的matrix放大到原来的四倍，也就是把2×2 matrix里的pixel按照原先记录的位置插入放大后的4×4 matrix中，其余项补0即可。当然这不是唯一的做法，在Keras中，pooling并没有记录原先的位置，做Unpooling的时候就是直接把pixel的值复制四份填充到扩大后的matrix里即可。 Deconvolution 实际上，Deconvolution就是convolution。这里以一维的卷积为例，假设输入是5维，过滤器(filter)的大小是3。 卷积的过程就是每三个相邻的点通过过滤器生成一个新的点，如下图左侧所示。在你的想象中，去卷积的过程应该是每个点都生成三个点，不同的点对生成同一个点的贡献值相加；但实际上，这个过程就相当于在周围补0之后再次做卷积，如下图右侧所示，两个过程是等价的。 卷积和去卷积的过程中，不同点在于，去卷积需要补零且过滤器的weight与卷积是相反的： 在卷积过程中，依次是橙线、蓝线、绿线； 在去卷积过程中，依次是绿线、蓝线、橙线。 因此在实践中，做去卷积的时候直接对模型加卷积层即可。 2.5. Generate在用自编码器的时候，通常是获取Encoder之后的code作为降维结果，但实际上Decoder也是有作用的，我们可以拿它来生成新的image。以MNIST为例，训练好编码器之后，取出其中的Decoder，输入一个随机的code，就可以生成一张图像 假设将28×28维的图像通过一层2维的hidden layer投影到二维平面上，得到的结果如下所示，不同颜色的点代表不同的数字，然后在红色方框中，等间隔的挑选二维向量丢进Decoder中，就会生成许多数字的图像。 这往往需要我们先观察一下二维的code的分布，确定哪些region是有值的，然后sample出来。此外，我们还可以对code加L2 regularization，以限制code分布的范围集中在0附近，此时就可以直接以0为中心去随机采取样本点，再通过Decoder生成图像。观察生成的数字图像，可以发现横轴的维度可以理解是表示是否含有圆圈，纵轴的维度表示是否倾斜。 3. Other Auto-encoder3.1. De-noising Auto-encoder有一个方法可以让Auto-encoder做的更好，叫作De-noising Auto-encoder（去噪自编码器）。它的基本思想是，把输入的$x$加上一些噪声(noise)变成$x’$，再对$x’$依次做编码(encode)和解码(decode)，得到还原后的$y$。 值得注意的是，一般的自编码器都是让输入输出尽可能接近，但在去噪自编码器中，我们的目标是让解码后的$y$与加噪声之前的$x$越接近越好。这种方法可以增加系统的鲁棒性，因为此时的编码器Encoder不仅仅是在学习如何做编码，它还学习到了如何过滤掉噪声这件事情。 （paper: Vincent, Pascal, et al. “Extracting and composing robust features with denoising autoencoders.” ICML, 2008.） 3.2. Contractive Auto-encoderContractive Auto-encoder（收缩自动编码器）的基本思想是，在做encode编码的时候，要加上一个约束，它可以使得：input的变化对编码后得到的code的影响最小化。 这个描述跟去噪自编码器很像，只不过去噪自编码器的重点在于加了噪声之后依旧可以还原回原先的输入，而收缩自动编码器的重点在于加了噪声之后能够保持编码结果不变。 （paper: Rifai, Salah, et al. “Contractive auto-encoders: Explicit invariance during feature extraction.“ Proceedings of the 28th International Conference on Machine Learning (ICML-11). 2011.） 其实还有很多nonlinear的dimension reduction的方法，比如Restricted Boltzmann Machine（受限玻尔兹曼机），它并不是neural network的方法，只是看起来有点像；Deep Belief Network（深度信念网络），它也和Deep neural network不是一回事。 3.3. Seq2Seq Auto-encoder在之前介绍的自编码器中，输入都是一个固定长度的vector，但类似文章、语音等信息实际上不应该单纯被表示为vector，那会丢失很多前后联系的信息。Seq2Seq就是为了解决这个问题提出的，具体内容在RNN那次的笔记中有介绍。","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://nekomoon404.github.io/categories/Deep-Learning/"}],"tags":[{"name":"Auto-encoder","slug":"Auto-encoder","permalink":"http://nekomoon404.github.io/tags/Auto-encoder/"},{"name":"Fine-tune","slug":"Fine-tune","permalink":"http://nekomoon404.github.io/tags/Fine-tune/"}]},{"title":"DL笔记（13）Unsupervised Learning-Neighbor Embedding","slug":"ML笔记（13）Unsupervised-Learning-Neighbor-Embedding","date":"2020-07-29T00:35:26.000Z","updated":"2020-07-29T02:35:26.000Z","comments":true,"path":"2020/07/29/ML笔记（13）Unsupervised-Learning-Neighbor-Embedding/","link":"","permalink":"http://nekomoon404.github.io/2020/07/29/ML%E7%AC%94%E8%AE%B0%EF%BC%8813%EF%BC%89Unsupervised-Learning-Neighbor-Embedding/","excerpt":"","text":"1.Manifold Learning之前的两篇笔记中介绍了PCA和Word Embedding，它们都是线性降维的方法，本文要介绍的Neighbor Embedding是非线性降维的方法，它用的是降维之前每一个data point与它的“邻居”之间的关系来做降维，这种方法也叫作Manifold Learning（流行学习），简单地可以理解为是高维空间中的低维空间。 样本点的分布可能是在高维空间里的一个流行(Manifold)，也就是说，样本点其实是分布在低维空间里面，只是被“扭曲”地塞到了一个高维空间里。比如地球的表面就是一个流行(Manifold)，它是一个二维的平面，但是被塞到了一个三维空间里。 在Manifold中，只有对距离很近用欧式距离判断其相似程度才会成立，如而在下图的S型曲面中，当点的距离比较远时，用欧氏距离是无法判断两个样本点的相似程度的。而Manifold Learning要做的就是把这个S型曲面降维展开，把塞在高维空间里的低维空间摊平，此时使用欧氏距离就可以描述样本点之间的相似程度，这会对接下来要做的cluster或者supervised learning都会有帮助的。类似的方法有很多，接下来简单介绍几种方法，最后介绍一下t-SNE。 2. Locally Linear EmbeddingLLE（locally linear embedding，局部线性嵌入）的基本思想是：假设在原来的空间中，样本点的分布如下所示，我们关注$x^i$和它的邻居$x^j$，用$w_{ij}$来描述$x_i$和$x_j$的关系。 假设每一个样本点$x^i$都是可以用它的neighbor做linear combination组合而成，那$w_{ij}$就是拿$x^j$去组合$x^i$时的权重weight，因此找点与点的关系$w_{ij}$这个问题就转换成，找一组使得所有样本点与周围点线性组合的差距能够最小的参数$w_{ij}$： \\sum\\limits_i||x^i-\\sum\\limits_j w_{ij}x^j ||_2接下来就要做Dimension Reduction，把$x^i$和$x^j$降维到$z^i$和$z^j$，并且保持降维前后两个点之间的关系$w_{ij}$是不变的。这就像是白居易的诗《长恨歌》中写到的“在天愿做比翼鸟，在地愿做连理枝”。 LLE的具体做法如下： 在原先的高维空间中找到$x^i$和$x^j$之间的关系$w_{ij}$以后就把它固定住 使$x^i$和$x^j$降维到新的低维空间上的$z^i$和$z^j$ $z^i$和$z^j$需要minimize下面的式子： \\sum\\limits_i||z^i-\\sum\\limits_j w_{ij}z^j ||_2 即在原本的空间里，$x^i$可以由周围点通过参数$w_{ij}$进行线性组合得到，则要求在降维后的空间里，$z^i$也可以用同样的线性组合得到 实际上，LLE并没有给出明确的降维函数，它没有明确地告诉我们怎么从$x^i$降维到$z^i$，只是给出了降维前后的约束条。在实际应用LLE的时候，对$x^i$来说，需要选择合适的邻居点数目K才会得到好的结果。用LLE或者其他类似的方法有一个好处就是，比如原来你不知道$x^i$、$x^j$，只知道$w_{i,j}$，那你也可用LLE来降维。 下图给出了原始paper中的实验结果，K太小或太大得到的结果都不太好，注意到在原先的空间里，只有距离很近的点之间的关系需要被保持住，如果K选的很大，就会选中一些由于空间扭曲才导致距离接近的点，而这些点的关系我们并不希望在降维后还能被保留。 3. Laplacian Eigenmaps另一种方法叫Laplacian Eigenmaps（拉普拉斯特征映射）。之前在讲semi-supervised learning有提到smoothness assumption，即我们仅知道两点之间的欧氏距离是不够的，还需要观察两个点在high density区域下的距离。如果两个点在high density的区域里比较近，那才算是真正的接近。我们可以依据某些规则把样本点建立graph，那么smoothness的距离就可以使用graph中连接两个点路径上的edges数来近似。 简单回顾一下在semi-supervised：如果两个点$x^1$和$x^2$在高密度区域上是相近的，那它们的label $y^1$和$y^2$很有可能是一样的 L=\\sum\\limits_{x^r} C(y^r,\\hat y^r) + \\lambda S\\\\ S=\\frac{1}{2}\\sum\\limits_{i,j} w_{i,j}(y^i-y^j)^2=y^TLy其中$C(y^r,\\hat y^r)$表示labeled data项，$\\lambda S$表示unlabeled data项，它就像是一个regularization term，用于判断我们当前得到的label是否是smooth的。 其中如果点$x^i$与$x^j$是相连的，则$w_{i,j}$等于相似度，否则为0，$S$的表达式希望在$x^i$与$x^j$很接近的情况下，相似度$w_{i,j}$很大，而label差距$|y^i-y^j|$越小越好，同时也是对label平滑度的一个衡量 降维的基本原则：如果$x^i$和$x^j$在high density区域上是相近的，即相似度$w_{i,j}$很大，则降维后的$z^i$和$z^j$也需要很接近，总体来说就是让下面的式子尽可能小： S=\\frac{1}{2}\\sum\\limits_{i,j} w_{i,j}(y^i-y^j)^2注意，与LLE不同的是，这里的$w_{i,j}$表示$x^i$与$x^j$这两点的相似度，上式也可以写成$S=\\sum\\limits_{i,j} w_{i,j} ||z^i-z^j||_2$ （paper：Belkin, M., Niyogi, P. Laplacian eigenmaps and spectral techniques for embedding and clustering. Advances in neural information processing systems . 2002） 但光有上面这个式子是不够的，假如令所有的z相等，比如令$z^i=z^j=0$，那上式就会直接停止更新。在semi-supervised中，如果所有label $z^i$都设成一样，会使得supervised部分的$\\sum\\limits_{x^r} C(y^r,\\hat y^r)$变得很大，因此lost就会很大，但在这里少了supervised的约束，因此我们需要给$z$一些额外的约束： 假设降维后$z$所处的空间为$M$维，则$Span \\{z^1,z^2,…,z^N\\}=R^M$，我们希望降维后的$z$占据整个$M$维的空间，而不希望它展开后在一个比$M$更低维的空间里 最终解出来的$z$其实就是Graph Laplacian $L$比较小的特征值所对应的特征向量。 这也是Laplacian Eigenmaps名称的由来，我们找的$z$就是Laplacian matrix的特征向量。如果通过拉普拉斯特征映射找到$z$之后再对其利用K-means做聚类，就叫做谱聚类(spectral clustering)。 4. t-SNEt-SNE（T-distributed Stochastic Neighbor Embedding，t分布随机邻居嵌入） 前面的方法有一个缺点就是，只假设了相邻的点要接近，却没有假设不相近的点要分开，所以在MNIST使用LLE会遇到下图的情形，它确实会把同一个class的点都聚集在一起，却没有办法避免不同class的点重叠在一个区域，这就会导致依旧无法区分不同class的现象。COIL-20数据集包含了同一张图片进行旋转之后的不同形态，对其使用LLE降维后得到的结果是，同一个圆圈代表同张图像旋转的不同姿态，但许多圆圈之间存在重叠 做t-SNE同样要降维，在原来$x$的分布空间上，我们需要计算所有$x^i$与$x^j$之间的相似度$S(x^i,x^j)$，然后需要将其做归一化：$P(x^j|x^i)=\\frac{S(x^i,x^j)}{\\sum_{k\\ne i}S(x^i,x^k)}$，即$x^j$与$x^i$的相似度（similarity）占 除了$x^j$之外所有的点与$x^i$的simiarity之和的比例。将$x$降维到$z$，同样可以计算相似度$S’(z^i,z^j)$，并做归一化：$Q(z^j|z^i)=\\frac{S’(z^i,z^j)}{\\sum_{k\\ne i}S’(z^i,z^k)}$。 注意，这里的归一化是有必要的，因为我们无法判断在$x$和$z$所在的空间里，$S(x^i,x^j)$与$S’(z^i,z^j)$的scale是否是一致的，需要将其映射到一个统一的概率区间，即(0,1)。 我们希望找到的投影空间$z$，可以让$P(x^j|x^i)$和$Q(z^j|z^i)$的分布越接近越好。用于衡量两个分布之间相似度的方法就是**KL散度(KL divergence)**，我们的目标就是让$L$越小越好，解法可以用Gradient Descent： L=\\sum\\limits_i KL(P(*|x^i)||Q(*|z^i))\\\\ =\\sum\\limits_i \\sum\\limits_jP(x^j|x^i)log \\frac{P(x^j|x^i)}{Q(z^j|z^i)}（KL Divergence 这里简单补充一下KL散度的基本知识。KL 散度，最早是从信息论里演化而来的，所以在介绍 KL 散度之前，我们要先介绍一下信息熵，信息熵的定义如下： H=-\\sum\\limits_{i=1}^N p(x_i)\\cdot log\\ p(x_i)其中$p(x_i)$表示事件$x_i$发生的概率，信息熵其实反映的就是要表示一个概率分布所需要的平均信息量 在信息熵的基础上，我们定义KL散度为： D_{KL}(p||q)=\\sum\\limits_{i=1}^N p(x_i)\\cdot (log\\ p(x_i)-log\\ q(x_i))\\\\ =\\sum\\limits_{i=1}^N p(x_i)\\cdot log\\frac{p(x_i)}{q(x_i)}$D_{KL}(p||q)$表示的就是概率$q$与概率$p$之间的差异，很显然，KL散度越小，说明概率$q$与概率$p$之间越接近，那么预测的概率分布与真实的概率分布也就越接近。） t-SNE会计算所有样本点之间的相似度，运算量会比较大，当数据量大的时候跑起来效率会比较低。常见的做法是对原先的空间用类似PCA的方法先做一次降维，然后用t-SNE对这个简单降维空间再做一次更深层次的降维，以期减少运算量。 值得注意的是，t-SNE的式子无法对新的样本点进行处理，一旦出现新的$x^i$，就需要重新跑一遍该算法，所以t-SNE通常不是用来训练模型的，它更适合用于做基于固定数据的可视化。t-SNE常用于将固定的高维数据可视化到二维平面上。 t-SNE Similarity Measure t-SNE中对如何计算similarity的选择是非常的“神妙的“，如果根据欧氏距离计算降维前的相似度，往往采用RBF function $S(x^i,x^j)=\\exp (-||x^i-x^j||_2)$，这个表达式的好处是，只要两个样本点的欧氏距离稍微大一些，相似度就会下降得很快 在t-SNE之前有一种叫做SNE的方法，它在降维后的新空间采用与上述相同的相似度算法$S’(z^i,z^j)=e^{-||z^i-z^j||_2}$。而t-SNE的“神妙”之处在于，它在降维后的新空间所采取的相似度算法是与之前不同的，它选取了t-distribution中的一种，即$S’(z^i,z^j)=\\frac{1}{1+||z^i-z^j||_2}$。 以下图为例，假设横轴代表了在原先$x$空间上的欧氏距离或者做降维之后在$z$空间上的欧氏距离，红线代表RBF function，是降维前的分布；蓝线代表了t-distribution，是降维后的分布 你会发现，降维前后相似度从RBF function到t-distribution： 如果原先在高维空间中两个点距离($\\Delta x$)比较近，则降维转换之后，它们的相似度($\\Delta y$)依旧是比较接近的 如果原先在高维空间中两个点距离($\\Delta x$)比较远，则降维转换之后，它们的相似度($\\Delta y$)会被拉得更远 也就是说t-SNE可以聚集相似的样本点，同时还会放大不同类别之间的距离，从而使得不同类别之间的分界线非常明显，特别适用于可视化，下图则是对MNIST和COIL-20先做PCA降维，再做t-SNE降维可视化的结果： Conclusion 小结一下，本文主要介绍了三种非线性降维的算法： LLE(Locally Linear Embedding)，局部线性嵌入算法，主要思想是降维前后，每个点与周围邻居的线性组合关系不变，$x^i=\\sum\\limits_j w_{ij}x^j$、$z^i=\\sum\\limits_j w_{ij}z^j$； Laplacian Eigenmaps，拉普拉斯特征映射，主要思想是在high density的区域，如果$x^i$、$x^j$这两个点相似度$w_{i,j}$高，则投影后的距离$||z^i-z^j||_2$要小； t-SNE(t-distribution Stochastic Neighbor Embedding)，t分布随机邻居嵌入，主要思想是，通过降维前后计算相似度由RBF function转换为t-distribution，在聚集相似点的同时，拉开不相似点的距离，比较适合用在数据固定的可视化领域。","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://nekomoon404.github.io/categories/Deep-Learning/"}],"tags":[{"name":"Manifold Learning","slug":"Manifold-Learning","permalink":"http://nekomoon404.github.io/tags/Manifold-Learning/"},{"name":"LLE(Locally Linear Embedding)","slug":"LLE-Locally-Linear-Embedding","permalink":"http://nekomoon404.github.io/tags/LLE-Locally-Linear-Embedding/"},{"name":"Laplacian Eigenmaps","slug":"Laplacian-Eigenmaps","permalink":"http://nekomoon404.github.io/tags/Laplacian-Eigenmaps/"},{"name":"t-SNE","slug":"t-SNE","permalink":"http://nekomoon404.github.io/tags/t-SNE/"},{"name":"KL Divergence","slug":"KL-Divergence","permalink":"http://nekomoon404.github.io/tags/KL-Divergence/"}]},{"title":"DL笔记（12）Unsupervised Learning-Word Embedding","slug":"ML笔记（12）Unsupervised-Learning-Word-Embedding","date":"2020-07-26T02:46:43.000Z","updated":"2020-07-26T03:46:43.000Z","comments":true,"path":"2020/07/26/ML笔记（12）Unsupervised-Learning-Word-Embedding/","link":"","permalink":"http://nekomoon404.github.io/2020/07/26/ML%E7%AC%94%E8%AE%B0%EF%BC%8812%EF%BC%89Unsupervised-Learning-Word-Embedding/","excerpt":"","text":"Word Embedding（词嵌入）就是specific用在word上的Dimension Reductionn，在word embedding里面我们希望machine做的是，machine在看了大量的文章以后，它可以自动地把每一个词汇用不同的vector来表示，而vector的dimension能代表某种含义，至少能够做到比如一些词汇有相近的语义或特殊的关系，可以在vector上呈现出来，而这些我们用来描述词汇的vector就称之为Word Embedding。 1-of-N Encoding 一个词汇用vector来描述它，最简单的方法就是1-of-N Encoding，假设这个vector的维数就等于世界上所有单词的数目，那么对每一个单词来说，只需要某一维为1，其余都是0即可；这样做的坏处是词汇与词汇之间的关系无法借用这种vector来传递出来，每一个词汇对应的vector都是独立的，无法建立起同类word之间的联系。 Word Class 还可以把有同样性质的word进行聚类(clustering)，划分成多个class，然后用word所属的class来表示这个word，但光做clustering是不够的，太粗略了，不同class之间关联依旧无法被有效地表达出来。 1. Word Embedding在Word Embedding中，一个词汇不是硬归类在某一个cluster里面，每一个词汇都用一个continuous的vector来描述词，vector的每个dimension可能都代表着某种含义。Word Embedding把每一个word都投影到高维空间上，当然这个空间的维度要远比1-of-N Encoding的维度低，假如1-of-N Encoding有10w维，那Word Embedding只需要50~100维就够了，这实际上也是Dimension Reduction的过程。 类似语义(semantic)的词汇，在这个word embedding的投影空间上是比较接近的，而且该空间里的每一维都可能有特殊的含义。比如词嵌入的投影空间如下图所示，则横轴可以理解是代表了生物与其它东西之间的区别，而纵轴则代表了会动的东西与静止的东西之间的差别。Word embedding是一个无监督的方法(unsupervised approach)，只要让机器阅读大量的文章，它就可以知道每一个词汇embedding之后的特征向量应该长什么样子。 我们的任务就是训练一个neural network，input是词汇，output则是它所对应的word embedding vector，实际训练的时候我们只有data的input，该如何解这类问题呢？之前提到过一种基于神经网络的降维方法，Auto-encoder，就是训练一个model，让它的输入等于输出，取出中间的某个隐藏层就是降维的结果，自编码的本质就是通过自我压缩和解压的过程来寻找各个维度之间的相关信息；但word embedding这个问题是不能用Auto-encoder来解的，因为输入的向量通常是1-of-N编码，各维无关，很难通过自编码的过程提取出什么有用信息。 那找Word Embedding的基本思想就是：每一个词汇的含义都可以根据它的上下文来得到。A word can be understood by its context，you shall know a word by the cmpany it keeps. 比如机器在两个不同的地方阅读到了“马英九520宣誓就职”、“蔡英文520宣誓就职”，它就会发现“马英九”和“蔡英文”前后都有类似的文字内容，于是机器就可以推测“马英九”和“蔡英文”这两个词汇代表了可能有同样地位的东西，即使它并不知道这两个词汇是人名。 那如何用这个思想来找出word embedding的vector呢？有两种常用的做法： Count based（基于计数的词嵌入） Prediction based（基于预测的词嵌入） 1.1. Count based假如$w_i$和$w_j$这两个词汇常常在同一篇文章中出现(co-occur)，它们的word vector分别用$V(w_i)$和$V(w_j)$来表示，则$V(w_i)$和$V(w_j)$会比较接近。假设$N_{i,j}$是$w_i$和$w_j$这两个词汇在相同文章里同时出现的次数，我们希望它与$V(w_i)\\cdot V(w_j)$的内积越接近越好，这个思想和之前的文章中提到的矩阵分解(matrix factorization)的思想其实是一样的。这种方法有一个很代表性的例子是Glove Vector 1.2. Prediction based1.2.1. Language modelPrediction based的方法可以用在Language Modeling上，即predict一个句子出现的几率，比如你想让machine去估测“wreck a nice beach”这个句子出现的几率。但实际上你没有办法去估测一个句子出现的几率，因为word数目就已经很多了，那由word组成的句子就更数不胜数，即使我们能搜到很多句子的database要预测的句子在database出现过的几率极小，且很可能是0。所以在预测句子出现的几率时，通常会将其拆分成小的片段，然后分别去计算每一片段出现的几率，比如我们要计算“wreck a nice beach”出现的几率，那就分别去计算”Start”后面接”wreck”的几率P(wreck|START)，”wreck”后面接”a”的几率P(a|wreck)……其中$P(b|a)$可以用统计的方法去计算，也可以用NN（Neural Network）来做，去计算input 一个词汇a，output词汇b的几率。 Language Modeling其实很有用的，可以用在机器翻译或者语音识别当中，它们需要这样的语言模型，比如在做语言辨识使只考虑声学的特性是不够的，不同的sentence可能有相同的发音，所以需要一个语言模型来告诉你哪个句子出现的几率是最高的。下面是一篇最早用神经网络来解决Language Model的paper，Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.，为后来深度学习在解决语言模型问题甚至很多别的nlp问题时奠定了坚实的基础。 关于这篇论文的解读：A Neural Probabilistic Language Model 论文阅读及实战，解析NNLM-A Neural Probabilistic Language Model，A Neural Probabilistic Language Model 1.2.2. how to do perdition给定一个sentence，我们要训练一个神经网络，它要做的就是根据当前的word $w_{i-1}$，来预测下一个可能出现的word $w_i$是什么 。假设我们使用1-of-N encoding把$w_{i-1}$表示成feature vector，它作为neural network的input，output的维数和input相等，只不过每一维都是小数，代表在1-of-N Encoding中该维为1其余维为0所对应的word会是下一个word $w_i$的概率。 如果我们把第一个hidden layer的input $z_1,z_2,…$拿出来，即NN的input乘一个matrix transform（做dimension reduction）得到的feature，用来做代表这个word的vector，它们所组成的$Z$就是word的另一种表示方式，当我们input不同的词汇，向量$Z$就会发生变化。也就是说，第一层hidden layer的维数可以由我们决定，而它的input又唯一确定了一个word，因此提取出第一层hidden layer的input，实际上就得到了一组可以自定义维数的Word Embedding的向量。如果把这些vector都画到平面上，就有可能得到“相似的word有相近的vector”这种图。 1.2.3. Why prediction worksprediction-based方法是如何体现根据词汇的上下文来了解该词汇的含义这件事呢？ 假设在两篇文章中，“蔡英文”和“马英九”代表$w_{i-1}$，“宣誓就职”代表$w_i$，我们希望对神经网络输入“蔡英文”或“马英九”这两个词汇，输出都是”宣誓就职”，即vector中对应“宣誓就职”词汇的那个维度的概率值是高的。为了使这两个不同的input通过NN能得到相同的output，就必须在进入hidden layer之前，就通过weight的转换将这两个input vector投影到位置相近的低维空间上。 也就是说，尽管两个input vector作为1-of-N编码看起来完全不同，但经过linear transform之后，将两者都降维到某一个空间中，在这个空间里，经过转换后的new vector 1和vector 2是非常接近的，因此它们同时进入一系列的hidden layer，最终输出时得到的output是相同的。因此，词汇上下文的联系就自动被考虑在这个prediction model里面。 总结一下，对1-of-N编码进行Word Embedding降维的结果就是神经网络模型第一层hidden layer的输入向量$\\left [ \\begin{matrix} z_1\\ z_2\\ … \\end{matrix} \\right ]^T$，该向量同时也考虑了上下文词汇的关联，我们可以通过控制第一层hidden layer的大小从而控制目标降维空间的维数。 有一个Tips是，在用prediction based的word embedding时，我们用的network的hidden layer通常只有一层，而不会是deep，并且activation function会用linear的，这样就有很像PCA。（提出这个方法的作者曾表示想要找word embedding其实不必用deep network，用shallow的network就够了，而且会train得非常快；另一个理由是word embedding有点像feature extraction，word embedding这个model抽出的vector是要拿来当接下来其他NLP task的Input，其他task用的是deep的model，那或许特征提取的部分就不需用deep的model。） 1.2.4. Sharing Parameters你可能会觉得通过当前词汇预测下一个词汇这个约束太弱了（即只看一个词汇去predict下一个词汇），由于不同词汇的搭配千千万万，即便是人也无法准确地给出下一个词汇具体是什么。所有你可以扩展这个问题，使用10个及以上的词汇去预测下一个词汇，可以帮助得到较好的结果。这里假设machine看的是前2个词汇，在实际使用中你可以extent到让machine看前10个词汇或前20个词汇，道理是一样的。 如果是一般的神经网络，我们直接把$w_{i-2}$和$w_{i-1}$这两个vector拼接成一个更长的vector作为input即可。但实际上，我们会用一个trick，希望和$w_{i-2}$相连的weight与和$w_{i-1}$相连的weight是tight在一起的，简单来说就是$w_{i-2}$与$w_{i-1}$的相同dimension对应到第一层hidden layer相同neuron之间的连线拥有相同的weight，即share parameters，在上图中，用同样的颜色标注相同的weight。（回想在CNN中，不同的fliter也会share同样的参数）。 如果我们不这么做，那把同一个word放在$w_{i-2}$的位置和放在$w_{i-1}$的位置，得到的Embedding结果是会不一样的，把两组weight设置成相同，可以使$w_{i-2}$与$w_{i-1}$的相对位置不会对结果产生影响。除此之外，这么做还可以通过共享参数的方式有效地减少参数量，不会由于input的word数量增加而导致参数量剧增。 1.2.5. Formulation假设$w_{i-2}$的1-of-N编码为$x_{i-2}$，$w_{i-1}$的1-of-N编码为$x_{i-1}$，维数均为$|V|$，表示数据中的words总数。Hidden layer的input为向量$z$，长度为$|Z|$，表示降维后的维数。则有： z=W_1 x_{i-2}+W_2 x_{i-1}其中$W_1$和$W_2$都是$|Z|×|V|$维的weight matrix，它由$|Z|$组$|V|$维的向量构成。我们强迫让$W_1=W_2=W$，此时$z=W(x_{i-2}+x_{i-1})$。因此，只要我们得到了这组参数$W$，就可以与1-of-N编码$x$相乘得到word embedding的结果$z$ 1.2.6. In Practice那在实际操作上，我们如何保证$W_1$和$W_2$一样呢？以下图中的$w_i$和$w_j$为例，我们希望它们的weight是一样的： 首先在训练的时候就要给它们一样的初始值 然后分别计算loss function $C$对$w_i$和$w_j$的偏微分，并对其进行更新 w_i=w_i-\\eta \\frac{\\partial C}{\\partial w_i}\\\\ w_j=w_j-\\eta \\frac{\\partial C}{\\partial w_j}这个时候你就会发现，$C$对$w_i$和$w_j$的偏微分是不一样的，这意味着即使给了$w_i$和$w_j$相同的初始值，更新过一次之后它们的值也会变得不一样，因此我们必须保证两者的更新过程是一致的，即： w_i=w_i-\\eta \\frac{\\partial C}{\\partial w_i}-\\eta \\frac{\\partial C}{\\partial w_j}\\\\ w_j=w_j-\\eta \\frac{\\partial C}{\\partial w_j}-\\eta \\frac{\\partial C}{\\partial w_i} 这个时候，我们就保证了$w_i$和$w_j$始终相等： $w_i$和$w_j$的初始值相同 $w_i$和$w_j$的更新过程相同 如何去训练这个神经网络呢？注意到这个NN完全是unsupervised，你只需要上网爬一下文章数据直接“喂”给它即可。比如喂给NN的input是“潮水”和“退了”，希望它的output是“就”，之前提到这个NN的输出是一个由概率组成的vector，而targret“就“是只有某一维为1的1-of-N编码，我们希望minimize它们之间的cross entropy，也就是使得输出的那个vector在“就”所对应的那一维上概率最高。 1.2.7. Various Architectures除了上面的基本形态，Prediction-based方法还可以有多种变形： CBOW(Continuous bag of word model) 用前后的词汇去预测中间的词汇 Skip-gram 用中间的词汇去预测前后的词汇 尽管word2vec是deep learning的一个应用，但这个neural network其实并不是deep的，它就只有一个linear的hidden layer。我们把1-of-N编码输入给神经网络，经过weight的转换得到Word Embedding，再通过第一层hidden layer就可以直接得到输出。其实过去有很多人使用过deep model，但这个task不用deep就可以实现，这样做既可以减少运算量，跑大量的data，又可以节省下训练的时间(deep model很可能需要长达好几天的训练时间)。 Word2Vec会有一些有趣的特性，当把同样类型的东西word vector摆在一起(Italy跟Rome摆在一起，Japen跟Tokyo摆在一起，每一个国家和它的首都之间是有类似的关系的)，或者把动词的三种时态摆在一起，动词的三态中间有某种类似的关系，如下右图中的三角形。 关于word2vec，可以参考：[NLP] 秒懂词向量Word2vec的本质；word2vec详解(CBOW，skip-gram，负采样，分层Softmax) 2. Application2.1. Subtraction机器问答 从得到的Word2vec里，我们可以发现一些原本并不知道的word与word之间的关系。把word vector两两相减，再投影到下图中的二维平面上，如果某些量量配对的word之间有相同关系，比如中下图每对word中的两个word是前者包含后者的关系，那它们的vector做Subtraction（相减）就会被投影到同一块区域。 利用这个概念，我们可以做一些简单的推论： 在word vector的特征上，$V(Rome)-V(Italy)≈V(Berlin)-V(Germany)$ 此时如果有人问“罗马之于意大利等于柏林之于？”，那机器就可以回答这个问题 因为德国的vector会很接近于“柏林的vector-罗马的vector+意大利的vector”，因此机器只需要计算$V(Berlin)-V(Rome)+V(Italy)$，然后选取与这个结果最接近的vector即可 2.2. Multi-lingual Embedding机器翻译 此外，Word2vec还可以建立起不同语言之间的联系。如果你要用上述方法分别训练一个英文的语料库(corpus)和中文的语料库，你会发现两者的word vector之间是没有任何关系的，因为Word Embedding只体现了上下文的关系，如果你的文章没有把中英文混合在一起使用，机器就没有办法判断中英文词汇之间的关系。 但是，如果你知道某些中文词汇和英文词汇的对应关系，你可以先分别获取它们的word vector，然后再去训练一个模型，把具有相同含义的中英文词汇投影到新空间上的同一个点。接下来遇到未知的新词汇，无论是中文还是英文，你都可以采用同样的方式将其投影到新空间，就可以自动做到类似翻译的效果。 参考文献：Bilingual Word Embeddings for Phrase-Based Machine Translation, Will Zou, Richard Socher, Daniel Cer and Christopher Manning, EMNLP, 2013 2.3. Multi-domain Embedding图像分类 这个Embedding不只局限于文字的应用，你也可以对文字+图像做Embedding。假设你已经得到horse、cat和dog这些词汇的vector在空间上的分布情况，你就可以去训练一个模型，把一些已知的horse、cat和dog图片去投影到和对应词汇相同的空间区域上。 比如对模型输入一张图像，使之输出一个跟word vector具有相同维数的vector，使dog图像的映射向量就散布在dog词汇向量的周围，horse图像的映射向量就散布在horse词汇向量的周围。训练好这个模型之后，输入新的未知图像，根据投影之后的位置所对应的word vector，就可以判断它所属的类别。 Paper: zero-shot learning through cross-modal transfer 我们知道在做图像分类的时候，很多情况下都是事先定好要分为哪几个具体的类别，再用这几个类别的图像去训练模型，由于我们无法在训练的时候穷尽所有类别的图像，因此在实际应用的时候一旦遇到属于未知类别的图像，这个模型就无能为力了。而使用image+word Embedding的方法，就算输入的图像类别在训练时没有被遇到过，比如上图中的cat，但如果这张图像能够投影到cat的word vector的附近，根据词汇向量与图像向量的对应关系，你自然就可以知道这张图像叫做cat。 2.4. Document Embedding文档嵌入 除了Word Embedding，我们还可以对Document做Embedding。最简单的方法是把document变成bag-of-word，然后用Auto-encoder就可以得到该文档的语义嵌入(Semantic Embedding)，但光这么做是不够的。 因为词汇的顺序代表了很重要的含义，两句词汇相同但语序不同的话可能会有完全不同的含义，比如 白血球消灭了传染病——正面语义 传染病消灭了白血球——负面语义 想要解决这个问题，具体可以参考下面的几种处理方法（都是unsupervised的做法）： Paragraph Vector: Le, Quoc, and Tomas Mikolov. “Distributed Representations of Sentences and Documents.“ ICML, 2014 Seq2seq Auto-encoder: Li, Jiwei, Minh-Thang Luong, and Dan Jurafsky. “A hierarchical neural autoencoder for paragraphs and documents.” arXiv preprint, 2015 Skip Thought: Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler, “Skip-Thought Vectors” arXiv preprint, 2015.","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://nekomoon404.github.io/categories/Deep-Learning/"}],"tags":[{"name":"Word embedding","slug":"Word-embedding","permalink":"http://nekomoon404.github.io/tags/Word-embedding/"},{"name":"1-of-N Encoding","slug":"1-of-N-Encoding","permalink":"http://nekomoon404.github.io/tags/1-of-N-Encoding/"},{"name":"Count based","slug":"Count-based","permalink":"http://nekomoon404.github.io/tags/Count-based/"},{"name":"Prediction based","slug":"Prediction-based","permalink":"http://nekomoon404.github.io/tags/Prediction-based/"},{"name":"Language model","slug":"Language-model","permalink":"http://nekomoon404.github.io/tags/Language-model/"},{"name":"Word2Vec","slug":"Word2Vec","permalink":"http://nekomoon404.github.io/tags/Word2Vec/"}]},{"title":"DL笔记（11）Unsupervised Learning-PCA","slug":"ML笔记（11）Unsupervised-Learning-PCA","date":"2020-07-25T13:24:52.000Z","updated":"2020-07-26T13:24:52.000Z","comments":true,"path":"2020/07/25/ML笔记（11）Unsupervised-Learning-PCA/","link":"","permalink":"http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/","excerpt":"","text":"1.Unsupervised Learning无监督学习(Unsupervised Learning)可以做的事大致分为两种： “化繁为简” 聚类(Clustering) 降维(Dimension Reduction) “无中生有”：Generation 对于无监督学习(Unsupervised Learning)来说，我们通常只会拥有$(x,\\hat y)$中的$x$或$\\hat y$，其中： 化繁为简就是把复杂的input变成比较简单的output，比如把一大堆没有打上label的树图片转变为一棵抽象的树，此时training data只有input $x$，而没有output $\\hat y$； 无中生有就是随机给function一个数字，它就会生成不同的图像，此时training data没有input $x$，而只有output $\\hat y$。 下面我们先简单介绍下Clustering，然后focus在dimension reduction上，而且只focus在linear dimension reduction上。 2. ClusteringClustering（聚类），顾名思义，就是把相近的样本划分为同一类，比如对下面这些没有标签的image进行分类，手动打上cluster 1、cluster 2、cluster 3的标签，这个分类过程就是化繁为简的过程。那有一个很critical的问题：我们到底要分几个cluster？ 2.1. K-means最常用的聚类方法是K-means： 我们有一大堆的unlabeled data $\\{x^1,…,x^n,…,x^N\\}$，我们要把它划分为K个cluster； 对每个cluster都要找一个center $c^i,i\\in \\{1,2,…,K\\}$，initial的时候可以从training data里随机挑K个object $x^n$出来作为K个center $c^i$的初始值； 遍历所有的object $x^n$，并判断它属于哪一个cluster，如果$x^n$与第i个cluster的center $c^i$最接近，那它就属于该cluster，我们用$b_i^n=1$来表示第n个object属于第i个cluster，$b_i^n=0$表示不属于； 更新center：把每个cluster里的所有object取平均值作为新的center值，即$c^i=\\sum\\limits_{x^n}b_i^n x^n/\\sum\\limits_{x^n} b_i^n$； 重复进行以上的操作； 注：如果不是从原先的data set里取center的初始值，可能会导致部分cluster没有样本点。 2.2. HACHAC（Hierarchical Agglomerative Clustering，层次聚类）。假设现在我们有5个样本点，想要做clustering： build a tree: （整个过程类似建立Huffman Tree，只不过Huffman是依据词频，而HAC是依据相似度建树） 对5个样本点两两计算相似度，挑出最相似的一对，比如样本点1和2； 将样本点1和2进行merge (可以对两个vector取平均)，生成代表这两个样本点的新结点； 此时只剩下4个结点，再重复上述步骤进行样本点的合并，直到只剩下一个root结点。 pick a threshold： 选取阈值，形象来说就是在构造好的tree上横着切一刀，相连的叶结点属于同一个cluster； 下图中，不同颜色的横线和叶结点上不同颜色的方框对应着切法与cluster的分法，比如按红色的线切就会分成2个cluster，按蓝色的线切就会分成3个cluster。 HAC和K-means最大的区别在于如何决定cluster的数量，在K-means里，K的值是要你直接决定的；而在HAC里，你并不需要直接决定分多少cluster，而是去决定“这一刀切在树的哪里” 3. Dimension Reductionclustering的缺点是以偏概全，它强迫每个object都要属于某个cluster。但实际上某个object可能拥有多种属性，或者多个cluster的特征，如果把它强制归为某个cluster，就会失去很多信息；或许我们应该用一个vector来描述该object，这个vector的每一维都代表object的某种属性（这个vector的dimension肯定要比object原来的feature数目少），这种做法就叫做Distributed Representation。如果原先的object是high dimension的，比如image，那现在用计算出来的它的某些attribute（属性，特征）来描述它，就可以使之从高维空间转变为低维空间，这就是所谓的降维(Dimension Reduction)。Distribution Representation和Dimension Reduction其实是一样的事情，只是叫法不同。 比如哟下图中动漫“全职猎人”中小杰的念能力分布，从表中可以看出我们不能仅仅把他归为强化系，而应该要用一个vector来表示他的attribute。 3.1 Why Dimension Reduction Help?接下来我们从另一个角度来看为什么Dimension Reduction可能是有用的。假设data为下图左侧中的3D螺旋式分布，你会发现用3D的空间来描述这些data其实是很浪费的，因为我们完全可以把这个卷摊平，此时只需要用2D的空间就可以描述这个3D的信息。 如果以MNIST(手写数字集)为例，每一张image都有28*28的dimension，但我们反过来想，大多数28*28 dimension的vector转成image，看起来都不会像是一个数字，所以描述数字所需要的dimension可能远比28*28要来得少。 举一个极端的例子，下面这几张表示“3”的image，我们完全可以用中间这张image旋转$\\theta$角度来描述，也就是说，我们只需要用$\\theta$这一个dimension就可以描述原先28*28 dimension的图像。你只要抓住角度的变化就可以知道28维空间中的变化，这里的28*28维pixel就是之前提到的樊一翁的胡子，而1维的角度则是他的头，也就是“去芜存菁，化繁为简”的思想 3.2. How to do Dimension Reduction？那怎么去做Dimension Reduction呢，在Dimension Reduction里，我们要找一个function，这个function的input是原始的$x$，output是经过降维之后的$z$。最简单的方法是Feature Selection，即直接从原有的dimension里删掉一些直观上就对结果没有影响的dimension，就做到了降维，比如下图中从$x_1,x_2$两个维度中直接拿掉$x_1$；但这个方法不总是有用，因为很多情况下任何一个dimension其实都不能被拿掉，就像下图中的螺旋卷。 4. PCA另一个常见的方法叫做PCA（Principe Component Analysis，主成分分析）。PCA认为降维就是一个很简单的linear function，它的input x和output z之间是linear transform，即$z=Wx$，PCA要做的，就是根据training data的$x$把W给找出来。 4.1 PCA for 1-Dimension我们先考虑一个简单的case，假设$z$是1维的vector，也就是把$x$投影到一维空间，此时$W$是一个row vector。$z_1=w^1\\cdot x$，其中$w^1$表示$w$的第一个row vector，我们假设$w^1$的长度为1，即$||w^1||_2=1$，此时$z_1$就是$x$在$w^1$方向上的投影。 那我们到底要找什么样的$w^1$呢？假设我们现在已有的宝可梦样本点分布如下，横坐标代表宝可梦的攻击力，纵坐标代表防御力，我们的任务是把这个二维分布投影到一维空间上。我们希望选这样一个$w^1$，它使得$x$经过投影之后得到的$z_1$分布越大越好，也就是说经过这个投影后，不同样本点之间的区别，应该仍然是可以被看得出来的。即： 我们希望找一个projection（投影）的方向，它可以让$x$经过projection后的variance越大越好； 我们不希望projection使这些data point通通挤在一起，导致点与点之间的奇异度消失； 要去maximize的对象是$z_1$的variance，其中variance的计算公式：$Var(z_1)=\\frac{1}{N}\\sum\\limits_{z_1}(z_1-\\bar{z_1})^2, ||w^1||_2=1$，$\\bar {z_1}$是$z_1$的平均值。 如下图给出了所有样本点在两个不同的方向上投影之后的variance比较情况，从这个图上，你可以看出$w^1$或许是代表宝可梦的强度，宝可梦可能有一个隐藏的factor代表它的强度，这个隐藏的factor同时影响了它的防御力跟攻击力，所以防御力跟攻击力是会同时上升的。 4.2 PCA for n-D当然我们不可能只投影到一维空间，我们还可以投影到更高维的空间 对$z=Wx$来说： $z_1=w^1\\cdot x$，表示$x$在$w^1$方向上的投影 $z_2=w^2\\cdot x$，表示$x$在$w^2$方向上的投影 … $z_1,z_2,…$串起来就得到$z$，而$w^1,w^2,…$分别是$W$的第1,2,…个row，需要注意的是，这里的$w^i$必须相互正交，此时$W$是正交矩阵(orthogonal matrix)，如果不加以约束，则算出来的的$w^1,w^2,…$实际上是相同的值 。 4.3. Lagrange multiplier求解PCA，实际上已经有现成的函数可以调用，此外你也可以把PCA描述成neural network，然后用gradient descent的方法来求解，这里主要介绍用Lagrange multiplier（拉格朗日乘数法）求解PCA的数学推导过程。 （注：$w^i$和$x$均为列向量，下文中类似$w^i\\cdot x$表示的是矢量内积，而$(w^i)^T\\cdot x$表示的是矩阵相乘。） Step1: calculate $w^1$ 目标：maximize $(w^1)^TSw^1 $，条件：$(w^1)^Tw^1=1$ 首先计算出$\\bar{z_1}$： \\begin{split} &z_1=w^1\\cdot x\\\\ &\\bar{z_1}=\\frac{1}{N}\\sum z_1=\\frac{1}{N}\\sum w^1\\cdot x=w^1\\cdot \\frac{1}{N}\\sum x=w^1\\cdot \\bar x \\end{split} 然后计算maximize的对象$Var(z_1)$： （其中$Cov(x)=\\frac{1}{N}\\sum(x-\\bar x)(x-\\bar x)^T$） \\begin{split} Var(z_1)&=\\frac{1}{N}\\sum\\limits_{z_1} (z_1-\\bar{z_1})^2\\\\ &=\\frac{1}{N}\\sum\\limits_{x} (w^1\\cdot x-w^1\\cdot \\bar x)^2\\\\ &=\\frac{1}{N}\\sum (w^1\\cdot (x-\\bar x))^2\\\\ &=\\frac{1}{N}\\sum(w^1)^T(x-\\bar x)(x-\\bar x)^T w^1\\\\ &=(w^1)^T\\frac{1}{N}\\sum(x-\\bar x)(x-\\bar x)^T w^1\\\\ &=(w^1)^T Cov(x)w^1 \\end{split} 当然这里想要求$Var(z_1)=(w^1)^TCov(x)w^1$的最大值，还要加上$||w^1||_2=(w^1)^Tw^1=1$的约束条件，否则$w^1$可以取无穷大。 令$S=Cov(x)$，它是： 对称的(symmetric) 半正定的(positive-semidefine)，即所有特征值(eigenvalues)是非负的(non-negative) （看来是要复习一下研一上学的矩阵理论了(￣ω￣;)） 使用拉格朗日乘数法，利用目标和约束条件构造函数： g(w^1)=(w^1)^TSw^1-\\alpha((w^1)^Tw^1-1) 对$w^1$这个vector里的每一个element做偏微分： \\partial g(w^1)/\\partial w_1^1=0\\\\ \\partial g(w^1)/\\partial w_2^1=0\\\\ \\partial g(w^1)/\\partial w_3^1=0\\\\ ... 整理上述推导式，可以得到： 其中，$w^1$是$S$的特征向量(eigenvector) Sw^1=\\alpha w^1 注意到满足$(w^1)^Tw^1=1$的特征向量$w^1$有很多，我们要找的是可以maximize $(w^1)^TSw^1$的那一个，于是利用上一个式子： (w^1)^TSw^1=(w^1)^T \\alpha w^1=\\alpha (w^1)^T w^1=\\alpha 此时maximize $(w^1)^TSw^1$就变成了maximize $\\alpha$，也就是$S$的最大的特征值$\\alpha$对应的那个特征向量，就是我们要找的$w^1$ 结论：$w^1$是$S=Cov(x)$这个matrix中的最大的特征值$\\lambda_1$对应的特征向量 Step2: calculate $w^2$ 在推导$w^2$时，相较于$w^1$，多了一个限制条件：$w^2$必须与$w^1$正交(orthogonal)。 目标：maximize $(w^2)^TSw^2$，条件：$(w^2)^Tw^2=1,(w^2)^Tw^1=0$ 结论：$w^2$也是$S=Cov(x)$这个matrix第二大的特征值$\\lambda_2$对应的特征向量 同样是用拉格朗日乘数法求解，先写一个关于$w^2$的function，包含要maximize的对象，以及两个约束条件 g(w^2)=(w^2)^TSw^2-\\alpha((w^2)^Tw^2-1)-\\beta((w^2)^Tw^1-0) 对$w^2$的每个element做偏微分： \\partial g(w^2)/\\partial w_1^2=0\\\\ \\partial g(w^2)/\\partial w_2^2=0\\\\ \\partial g(w^2)/\\partial w_3^2=0\\\\ ... 整理后得到： Sw^2-\\alpha w^2-\\beta w^1=0 上式两侧同乘$(w^1)^T$，得到： (w^1)^TSw^2-\\alpha (w^1)^Tw^2-\\beta (w^1)^Tw^1=0 其中$\\alpha (w^1)^Tw^2=0,\\beta (w^1)^Tw^1=\\beta$， 而由于$(w^1)^TSw^2$是vector×matrix×vector=scalar，因此在外面套一个transpose（转置）不会改变其值，因此该部分可以转化为： （注：$S$是symmetric matrix（对称矩阵）的，既有$S^T=S$。） \\begin{split} (w^1)^TSw^2&=((w^1)^TSw^2)^T\\\\ &=(w^2)^TS^Tw^1\\\\ &=(w^2)^TSw^1 \\end{split}我们已经知道$w^1$满足$Sw^1=\\lambda_1 w^1$，代入上式： \\begin{split} (w^1)^TSw^2&=(w^2)^TSw^1\\\\ &=\\lambda_1(w^2)^Tw^1\\\\ &=0 \\end{split} 因此有$(w^1)^TSw^2=0$，$\\alpha (w^1)^Tw^2=0$，$\\beta (w^1)^Tw^1=\\beta$，又根据 (w^1)^TSw^2-\\alpha (w^1)^Tw^2-\\beta (w^1)^Tw^1=0可以推得$\\beta=0$ 此时$Sw^2-\\alpha w^2-\\beta w^1=0$就转变成了$Sw^2-\\alpha w^2=0$，即 Sw^2=\\alpha w^2 由于$S$是symmetric的，因此在不与$w_1$冲突的情况下，这里$\\alpha$选取第二大的特征值$\\lambda_2$时，可以使$(w^2)^TSw^2$最大 结论：$w^2$也是$S=Cov(x)$这个matrix中的特征向量，对应第二大的特征值$\\lambda_2$ （实对称矩阵的不同特征值对应的特征向量是正交的，这个在线性代数或者矩阵理论课程中都有讲过，或者可以参考實對稱矩陣可正交對角化的證明，这是我上学期末在复习矩阵理论的时候发现了一个台湾线代老师的博客，里面有非常多讲解线代知识的文章，思路和内容都要比SJTU用的教材好太多，简直救我期末于水火ヾ(ｏ･ω･)ﾉ） 4.4. PCA-decorrelation$z=W\\cdot x$的神奇之处在于$Cov(z)=D$，即$z$的covariance是一个diagonal matrix，推导过程如下图所示。PCA可以让不同dimension之间的covariance变为0，即不同new feature之间是没有correlation的，这样做的好处是，减少feature之间的联系从而减少model所需的参数量。 如果你把原来的input data通过PCA之后再给其他model使用，那这些model就可以使用简单的形式，而无需考虑不同dimension之间类似$x_1\\cdot x_2,x_3\\cdot x_5^3,…$这些交叉项，此时model得到简化，参数量大大降低，相同的data量可以得到更好的训练结果，从而可以避免overfitting的发生。 5. PCA - Another Point of View上面我们进行了PCA的数学推导，下面从另一个角度更直观地介绍PCA做了什么。 5.1. Reconstruction Component假设我们现在考虑的是手写数字识别，这些数字是由一些类似于笔画的basic component组成的，本质上就是一个vector，记做$u_1,u_2,u_3,…$，以MNIST为例，不同的笔画都是一个28×28的vector，把某几个vector加起来，就组成了一个28×28的digit，写成表达式就是：$x≈c_1u^1+c_2u^2+…+c_ku^k+\\bar x$，其中$x$代表某张digit image中的pixel，它等于k个component的加权和$\\sum c_iu^i$，加上所有image的平均值$\\bar x$。 比如7就是$x=u^1+u^3+u^5$，我们可以用$\\left [\\begin{matrix}c_1\\ c_2\\ c_3…c_k \\end{matrix} \\right]^T$来表示一张digit image，如果component的数目k远比pixel的数目要小，那这个描述就是比较有效的。 实际上目前我们并不知道$u^1$~$u^k$具体的值，因此我们要找这样k个vector，使得$x-\\bar x$与$\\hat x$越接近越好： x-\\bar x≈c_1u^1+c_2u^2+...+c_ku^k=\\hat x而用未知component来描述的这部分内容，叫做Reconstruction error，即$||(x-\\bar x)-\\hat x||$，接下来我们就要去找k个vector $u^i$去minimize这个error： L=\\min\\limits_{u^1,...,u^k}\\sum||(x-\\bar x)-(\\sum\\limits_{i=1}^k c_i u^i) ||_2回顾PCA，$z=W\\cdot x$，实际上我们通过PCA最终解得的$\\{w^1,w^2,…,w^k\\}$就是使reconstruction error最小化的$\\{u^1,u^2,…,u^k\\}$，简单证明如下： 将所有的$x^i-\\bar x≈c_1^i u^1+c_2^i u^2+…$写在一起，就可以用下图中的矩阵相乘来表示，我们的目标是使等号两侧矩阵之间的差距越小越好； 可以使用SVD（Singular Value Decomposition，奇异值分解）将每个matrix $X_{m×n}$都拆成matrix $U_{m×k}$、$\\Sigma_{k×k}$、$V_{k×n}$的乘积，其中k为component的数目； （SVD可以参考奇異值分解專題)，我之前也有纸质笔记，有时间的话可以整理一下，咕咕咕。） 值得注意的是，使用SVD拆解后的三个矩阵相乘，是跟等号左边的矩阵$X$最接近的，此时$U$就对应着$u^i$那部分的矩阵，$\\Sigma\\cdot V$就对应着$c_k^i$那部分的矩阵 根据SVD的结论，组成矩阵$U$的k个列向量(标准正交向量, orthonormal vector)就是$XX^T$最大的k个特征值(eignvalue)所对应的特征向量(eigenvector)，而$XX^T$实际上就是$x$的covariance matrix，因此$U$就是PCA的k个解组成的matrix； 因此我们可以发现，通过PCA找出来的Dimension Reduction的transform，实际上就是把$X$拆解成能够最小化Reconstruction error的component的过程，通过PCA所得到的$w^i$就是component $u^i$，而Dimension Reduction的结果就是参数$c_i$ 简单来说就是，用PCA对$x$进行降维的过程中，我们要找的投影方式$w^i$就相当于恰当的组件$u^i$，投影结果$z^i$就相当于这些组件各自所占的比例$c_i$ 下面的式子简单演示了将一个样本点$x$划分为k个组件的过程，其中$\\left [\\begin{matrix}c_1 \\ c_2\\ … c_k \\end{matrix} \\right ]^T$是每个组件的比例；把$x$划分为k个组件即从n维投影到k维空间，$\\left [\\begin{matrix}c_1 \\ c_2\\ … c_k \\end{matrix} \\right ]^T$也是投影结果，其中$x$和$u_i$均为n维列向量。 \\begin{split} &x= \\left [ \\begin{matrix} u_1\\ u_2\\ ...\\ u_k \\end{matrix} \\right ]\\cdot \\left [ \\begin{matrix} c_1\\\\ c_2\\\\ ...\\\\ c_k \\end{matrix} \\right ]\\\\ \\\\ &\\left [ \\begin{matrix} x_1\\\\ x_2\\\\ ...\\\\ x_n \\end{matrix} \\right ]=\\left [ \\begin{matrix} u_1^1\\ u_2^1\\ ... u_k^1 \\\\ u_1^2\\ u_2^2\\ ... u_k^2 \\\\ ...\\\\ u_1^n\\ u_2^n\\ ... u_k^n \\end{matrix} \\right ]\\cdot \\left [ \\begin{matrix} c_1\\\\ c_2\\\\ ...\\\\ c_k \\end{matrix} \\right ]\\\\ \\end{split} 5.2. NN for PCA现在我们已经知道，用PCA找出来的$\\{w^1,w^2,…,w^k\\}$就是k个component $\\{u^1,u^2,…,u^k\\}$，而$\\hat x=\\sum\\limits_{k=1}^K c_k w^k$，我们要使$\\hat x$与$x-\\bar x$之间的差距越小越好，我们已经根据SVD找到了$w^k$的值，而对每个不同的样本点，都会有一组不同的$c_k$值，在PCA中我们已经证得，$\\{w^1,w^2,…,w^k\\}$这k个vector是标准正交化的(orthonormal)，因此： c_k=(x-\\bar x)\\cdot w^k这个时候我们就可以使用神经网络来表示整个过程，假设$x$是3维向量，要投影到k=2维的component上： 对$x-\\bar x$与$w^k$做inner product的过程类似于neural network，$x-\\bar x$在3维空间上的坐标就相当于是neuron的input，而$w^1_1$，$w^1_2$，$w^1_3$则是neuron的weight，表示在$w^1$这个维度上投影的参数，而$c_1$则是这个neuron的output，表示在$w^1$这个维度上投影的坐标值；对$c_2$也同理 得到$c_1$之后，再让它乘上$w^1$，得到$\\hat x$的一部分 对$c_2$进行同样的操作，乘上$w^2$，贡献$\\hat x$的剩余部分，此时我们已经完整计算出$\\hat x$三个分量的值 此时，PCA就被表示成了只含一层hidden layer的神经网络，且这个hidden layer是线性的激活函数，训练目标是让这个NN的input $x-\\bar x$与output $\\hat x$越接近越好，这件事就叫做Autoencoder。 注意，通过PCA求解出的$w^i$与直接对上述的神经网络做梯度下降所解得的$w^i$是会不一样的，因为PCA解出的$w^i$是正交的(orgonormal)，而用NN的方式得到的解无法保证$w^i$相互垂直，NN无法做到Reconstruction error比PCA小，因此： 在linear的情况下，直接用PCA找$W$远比用神经网络的方式更快速方便 用NN的好处是，它可以使用不止一层hidden layer，它可以做deep autoencoder 5.3. Weakness of PCAPCA也有很明显的弱点： 它是unsupervised的，如果我们要将下图绿色的点投影到一维空间上，PCA给出的从左上到右下的划分很有可能使原本属于蓝色和橙色的两个class的点被merge在一起 这时要解决问题可能就需要引入label data，LDA（Linear Discriminant Analysis，线性判别分析）则是考虑了labeled data之后进行降维的一种方式，但属于supervised，不在本文讨论。 LDA不同于PCA方差最大化理论，LDA算法的思想是将数据投影到低维空间之后，使得同一类数据尽可能的紧凑，不同类的数据尽可能分散。因此，LDA算法是一种有监督的机器学习算法。同时，LDA有如下两个假设:(1) 原始数据根据样本均值进行分类。(2) 不同类的数据拥有相同的协方差矩阵。 （摘自文章机器学习-LDA(线性判别降维算法)） 它是linear的，对于下图中的彩色曲面，我们期望把它平铺拉直进行降维，但这是一个non-linear的投影转换，PCA无法做到这件事情，PCA只能做到把这个曲面打扁压在平面上，类似下图，而无法把它拉开。对类似曲面空间的降维投影，需要用到non-linear transformation 6. More Examples6.1. PCA for Pokemon这里举一个实际应用的例子，用PCA来分析宝可梦的数据，假设总共有800只宝可梦，每只都用一个六维的vector来表示，即vector={HP, Atk, Def, Sp Atk, Sp Def, Speed}，然后我们用PCA来分析，首先要面对的问题是，要将6维的vector投影到多少维的空间上？ 如果做可视化分析的话，投影到二维或三维平面可以方便人眼观察，实际上，宝可梦的$cov(x)$是6维，最多可以投影到6维空间。一个常用的方法是：我们可以先找出6个特征向量和对应的特征值$\\lambda_i$，其中$\\lambda_i$表示第$i$个投影维度的variance有多大(即在第i个维度的投影上点的散步程度有多大，variance越大，点的分布就越散，这也是我们所希望的)，然后我们就可以计算出每个$\\lambda_i$的比例，ratio=$\\frac{\\lambda_i}{\\sum\\limits_{i=1}^6 \\lambda_i}$ 从上图的ratio可以看出$\\lambda_5$、$\\lambda_6$所占比例不高，即第5和第6个principle component(可以理解为维度)所发挥的作用是比较小的，用这两个dimension做投影所得到的variance很小，投影在这两个方向上的点比较集中，意味着这两个维度表示的是宝可梦的共性，无法对区分宝可梦的特性做出太大的贡献，所以我们只需要利用前4个principle component即可。 注意到新的维度本质上就是旧的维度的加权矢量和，下图给出了前4个维度的加权情况，从PC1到PC4这4个principle component都是6维度加权的vector，它们都可以被认为是某种组件，每一个宝可梦都可以由这4个principle component加权之和。 我们来仔细分析一下这四个component代表的“含义”： 对第一个vector PC1来说，每个值都是正的，因此这个组件在某种程度上代表了宝可梦的强度 对第二个vector PC2来说，防御力Def很大而速度Speed很小，这个组件可以增加宝可梦的防御力但同时会牺牲一部分的速度 如果将宝可梦仅仅投影到PC1和PC2这两个维度上，则降维后的二维可视化图像如下图所示： 从该图中也可以得到一些信息： 在PC2维度上特别大的那个样本点刚好对应着普普(海龟)，确实是防御力且速度慢的宝可梦 在PC1维度上特别大的那三个样本点则对应着盖欧卡、超梦等综合实力很强的宝可梦 对第三个vector PC3来说，sp Def很大而HP和Atk很小，这个组件是用生命力和攻击力来换取特殊防御力 对第四个vector PC4来说，HP很大而Atk和Def很小，这个组件是用攻击力和防御力来换取生命力 同样将宝可梦只投影到PC3和PC4这两个维度上，则降维后得到的可视化图像如下图所示： 该图同样可以告诉我们一些信息： 在PC3维度上特别大的样本点依旧是普普，第二名是冰柱机器人，它们的特殊防御力都比较高 在PC4维度上特别大的样本点则是吉利蛋和幸福蛋，它们的生命力比较强 6.2. PCA for MNIST再次回到手写数字识别的问题上来，这个时候我们就可以熟练地把一张数字图像用多个组件(维度)表示出来了： digit\\ image=a_1 w^1+a_2 w^2+...这里的$w^i$就表示降维后的其中一个维度，即一个principle component，它是由原先28×28维进行加权求和的结果，因此$w^i$也是一张28×28的图像，下图列出了通过PCA得到的前30个组件的形状： 注：PCA就是求$Cov(x)=\\frac{1}{N}\\sum (x-\\bar x)(x-\\bar x)^T$的前30个最大的特征值对应的特征向量 6.3. PCA for Face Recognition同理，在人脸识别的例子中，通过PCA找出人脸的前30个component(维度)，如下图所示，用这些脸的组件做线性组合就可以得到所有的脸。 在对MNIST和Face的PCA结果展示的时候，你可能会注意到我们找到的组件好像并不算是组件，比如MNIST找到的几乎是完整的数字雏形，而Face找到的也几乎是完整的人脸雏形，但我们预期的组件不应该是类似于横折撇捺，眼睛鼻子眉毛这些吗？如果你仔细思考了PCA的特性，就会发现得到这个结果是可能的： digit\\ image=a_1 w^1+a_2 w^2+...注意到linear combination的weight $a_i$可以是正的也可以是负的，因此我们可以通过把组件进行相加或相减来获得目标图像，这会导致你找出来的component不是基础的组件，但是通过这些组件的加加减减肯定可以获得基础的组件元素。 6.4. NMF如果你要一开始就得到类似笔画这样的基础组件，就要使用NMF(non-negative matrix factorization)，非负矩阵分解的方法。PCA可以看成对原始矩阵$X$做SVD进行矩阵分解，但并不保证分解后矩阵的正负，实际上当进行图像处理时，如果部分组件的matrix包含一些负值的话，如何处理负的像素值也会成为一个问题(可以做归一化处理，但比较麻烦)。而NMF的基本思想是，强迫使所有component的每一个dimension都是正的，且它的加权值$a_1$都必须是正的，也就是说所有图像都必须由组件叠加得到： Forcing $a_1$, $a_2$…… be non-negative additive combination Forcing $w_1$, $w_2$…… be non-negative More like “parts of digits” (注：关于NMF的具体算法内容可参考paper Daniel D. Lee and H. Sebastian Seung. “Algorithms for non-negative matrix factorization.”Advances in neural information processing systems. 2001. ） NMF for MNIST 在MNIST数据集上，通过NMF找到的前30个组件如下图所示，可以发现这些组件都是由基础的笔画构成： NMF for Face 在Face数据集上，通过NMF找到的前30个组价如下图所示，相比于PCA这里更像是脸的一部分 More Related Approaches降维的方法有很多，这里再列举一些与PCA有关的方法： Multidimensional Scaling (MDS) [Alpaydin, Chapter 6.7] MDS不需要把每个data都表示成feature vector，只需要知道特征向量之间的distance，就可以做降维，PCA保留了原来在高维空间中的距离，在某种情况下MDS就是特殊的PCA Probabilistic PCA [Bishop, Chapter 12.2] PCA概率版本 Kernel PCA [Bishop, Chapter 12.3] PCA非线性版本 Canonical Correlation Analysis (CCA) [Alpaydin, Chapter 6.9] CCA常用于两种不同的data source的情况，比如同时对声音信号和唇形的图像进行降维 Independent Component Analysis (ICA) ICA常用于source separation，PCA找的是正交的组件，而ICA则只需要找“独立”的组件即可 Linear Discriminant Analysis (LDA) [Alpaydin, Chapter 6.8] LDA是supervised的方式 7. Matrxi Factorization接下来用一个简单的推荐系统的例子来介绍下矩阵分解的思想。有时候存在两种object，它们之间会受到某种共同潜在因素(latent factor)的操控，如果我们找出这些潜在因素，就可以对用户的行为进行预测，这也是推荐系统常用的方法之一。 假设我们现在去调查每个人购买的手办数目，ABCDE代表5个人，每个人或者每个手办的动漫人物实际上都是有着傲娇的属性或天然呆的属性（老师是老二次元实锤）。我们可以用vector去描述人和公仔的属性，如果某个人的属性和某个公仔的属性是match的，即他们背后的vector很像(内积值很大)，这个人就会偏向于拥有更多这种类型的公仔。 matrix expression 但是，我们没有办法直接观察某个人背后这些潜在的属性，也不会有人在意一个肥宅心里想的是什么（肥宅大哭），我们同样也没有办法直接得到动漫人物背后的属性；我们目前有的，只是动漫人物和人之间的关系，即每个人已购买的公仔数目，我们要通过这个关系去推测出动漫人物与人背后的潜在因素(latent factor)。 我们可以把每个人的属性用vector $r^A$、$r^B$、$r^C$、$r^D$、$r^E$来表示，而动漫人物的属性则用vector $r^1$、$r^2$、$r^3$、$r^4$来表示，购买的公仔数目可以被看成是matrix $X$，对$X$来说，行数为人数，列数为动漫角色的数目。做一个假设：matrix $X$里的每个element，都是属于人的vector和属于动漫角色的vector的内积，比如，$r^A\\cdot r^1≈5$，表示$r^A$和$r^1$的属性比较贴近，那A这个人就会买比较多凉宫春日的手办。 接下来就用下图所示的矩阵相乘的方式来表示这样的关系，其中$K$为latent factor的数量，这是未知的，需要你自己去调整选择，我们要找一组$r^A$~$r^E$和$r^1$~$r^4$，使得右侧两个矩阵相乘的结果与左侧的matrix $X$越接近越好，可以使用SVD的方法求解。 prediction 但有时候，部分的information可能是会missing的，这时候就难以用SVD精确描述，但我们可以使用梯度下降的方法求解，loss function如下： L=\\sum\\limits_{(i,j)}(r^i\\cdot r^j-n_{ij})^2其中$r^i$值的是人背后的latent factor，$r^j$指的是动漫角色背后的latent factor，我们要让这两个vector的内积与实际购买该公仔的数量$n_{ij}$越接近越好，这个方法的关键之处在于，计算上式时，可以跳过missing的数据，最终通过gradient descent求得$r^i$和$r^j$的值。 假设latent factor的数目等于2，则人的属性$r^i$和动漫角色的属性$r^j$都是2维的vector，这里实际进行计算后，把属性中较大值标注出来，可以发现： 人：A、B属于同一组属性，C、D、E属于同一组属性 动漫角色：1、2属于同一组属性，3、4属于同一组属性 结合动漫角色，可以分析出动漫角色的第一个维度是天然呆属性，第二个维度是傲娇属性 接下来就可以预测未知的值，只需要将人和动漫角色的vector做内积即可 这样就可以针对阿宅做一个简单的推荐系统了。 more about matrix factorization 实际上除了人和动漫角色的属性之外，可能还存在其他因素操控购买数量这一数值，因此我们可以将式子更精确地改写为： r^A\\cdot r^1+b_A+b_1≈5其中$b_A$表示A这个人本身有多喜欢买公仔，$b_1$则表示这个动漫角色本身有多受欢迎，这些内容是跟属性vector无关的，此时loss function被改写为： L=\\sum\\limits_{(i,j)}(r^i\\cdot r^j+b_i+b_j-n_{ij})^2当然你也可以加上一些regularization去对结果做约束。（有关Matrix Factorization和推荐系统更多内容的介绍，可以参考paper Matrix Factorization Techniques For Recommender Systems；有关Matrix Factorization的理论知识，可以参考：線代啟示錄） for Topic Analysis 如果把matrix factorization的方法用在topic analysis上，就叫做LSA(Latent semantic analysis)，潜在语义分析。我们只需要把动漫人物换成文章，人换成词汇，表中的值从购买数量换成词频即可，table里面的值就是term frequency，把这个term frequency乘上一个weight代表说这个term本身有多重要。 Evaluation一个term的重要性的常用的方式是：inverse document frequency(计算每一个词汇在整个paper有多少比率的document涵盖这个词汇，假如说，每个词汇，每个document都有，那它的inverse document frequency就很小，代表着这个词汇的重要性是低的，假设某个词汇只有某一篇document有，那它的inverse document frequency就很大，代表这个词汇对于这篇文章的重要性是高的，这个词汇的含义与文章的语义关联性就比较高。) 在这个task里，做matrix factorization，就会找到每一个document背后的latent factor，它可能指的是topic(主题)，这个topic有多少是跟财经有关的，有多少是跟政治有关的。document1跟document2有比较多的“投资，股票”这样的词汇，那document1跟document2就有比较高的可能背后的latent factor是比较偏向“财经”的。我们可以用词汇的重要性给词频加权，在各种文章中出现次数越多的词汇越不重要，出现次数越少则越重要。这个场景下找出的latent factor可能会是主题(topic)，比如某个词汇或某个文档有多少比例是偏向于财经主题、政治主题… Topic analysis的方法多如牛毛，但基本的思想是差不多的，常见的方法有pLSA（probability latent semantic analysis，概率潜在语义分析）和LDA（latent Dirchlet allocation，隐含狄利克雷分布）。注意这里的LDA和之前在5.3节提到的LDA是不一样的。 pLSA 可以参考：pLSA原理及其代码实现； LDA 可以参考：一文详解LDA主题模型","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://nekomoon404.github.io/categories/Deep-Learning/"}],"tags":[{"name":"K-means","slug":"K-means","permalink":"http://nekomoon404.github.io/tags/K-means/"},{"name":"HAC","slug":"HAC","permalink":"http://nekomoon404.github.io/tags/HAC/"},{"name":"PCA","slug":"PCA","permalink":"http://nekomoon404.github.io/tags/PCA/"},{"name":"NMF","slug":"NMF","permalink":"http://nekomoon404.github.io/tags/NMF/"},{"name":"Matrix Factorization","slug":"Matrix-Factorization","permalink":"http://nekomoon404.github.io/tags/Matrix-Factorization/"}]},{"title":"ML笔记（10）Semi-supervised Learning","slug":"ML笔记（10）Semi-supervised-Learning","date":"2020-07-25T07:31:35.000Z","updated":"2020-07-25T08:31:35.000Z","comments":true,"path":"2020/07/25/ML笔记（10）Semi-supervised-Learning/","link":"","permalink":"http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8810%EF%BC%89Semi-supervised-Learning/","excerpt":"","text":"我们之间讲的Learning方法基本都是Supervised Learning（监督学习），这篇文章主要介绍Semi-supervised Learning（半监督学习）。 Supervised Learning：$(x^r,\\hat y^r)$$_{r=1}^R$ training data中，每一组data都有input $x^r$和对应的output $y^r$ Semi-supervised Learning：$\\{(x^r,\\hat y^r)\\}_{r=1}^R$} + $\\{x^u\\}_{u=R}^{R+U}$ training data中，部分data没有标签，只有input $x^u$ 通常遇到的场景是，无标签的数据量远大于有标签的数据量，即U&gt;&gt;R semi-supervised learning分为以下两种情况： Transductive Learning（直推学习）: unlabeled data is the testing data 即把testing data当做无标签的training data使用，适用于事先已经知道testing data的情况(一些比赛的时候) 值得注意的是，这种方法使用的仅仅是testing data的feature，而不是label，因此不会出现“直接对testing data做训练而产生cheating的效果” Inductive Learning（归纳学习）: unlabeled data is not the testing data 即不把testing data的feature拿去给机器训练，适用于事先并不知道testing data的情况(更普遍的情况) 半监督学习的情况，训练集为$\\mathcal{D}=\\{\\pmb{X_{tr}},\\pmb{y_{tr}}\\}$，测试集为$\\pmb{X_{tr}}$，此时，$\\pmb{X_{un}}$与 $\\pmb{X_{te}}$都是未标记的，但我们测试的$\\pmb{X_{te}}$在训练时没有见过，这种情况是 inductive semi-supervised learning。 如果我们不管$\\pmb{X_{te}}$的效果怎么样时，由于此时在训练的时候我们已经见过$\\pmb{X_{un}}$（利用了$\\pmb{X_{un}}$的特征信息)，这时就叫transductive semi-supervised learning。 简单来说，transductive和inductive的区别在于我们想要预测的样本，是不是我们在训练的时候已经见（用）过的。 通常transductive比inductive的效果要好，因为inductive需要从训练generalize到测试。 （摘自知乎回答：如何理解 inductive learning 与 transductive learning?） 为什么要做semi-supervised learning？ 用机器学习解决实际问题时，很多情况我们是不缺data的 ，只是缺有label的data，就像你可以拍很多照片，但它们一开始都是没有标签的，而给data加label是要花费很大成本的。 1. Why semi-supervised learning help？那为什么semi-supervised learning会有帮助呢？The distribution of the unlabeled data tell us something. unlabeled data虽然只有input，但它的分布，却可以告诉我们一些事情。 假设我们现在要做分类的task，建一个猫跟狗的classifier，我们同时有一大堆猫跟狗的图片。这些图片是没有label的，并不知道哪些是猫哪些是狗。 在只有labeled data的情况下，红线是二元分类的分界线。 但当我们加入unlabeled data的时候，由于特征分布发生了变化，分界线也随之改变。 semi-supervised learning的使用往往伴随着假设，而该假设的合理与否，决定了结果的好坏程度；比如上图中的unlabeled data，它显然是一只狗，而特征分布却与猫被划分在了一起，很可能是由于这两张图片的背景都是绿色导致的。 2. Semi-supervised Learning for Generative Model在监督学习中，我们已经讨论过概率生成模型了，假设class1和class2的分布分别为$mean_1=u^1,covariance_1=\\Sigma$、$mean_2=u^2,covariance_2=\\Sigma$的高斯分布，计算出Prior Probability后，再根据贝叶斯公式可以推得新生成的$x$所属的类别。 如果在原先的数据基础上加了unlabeled data(下图中绿色的点)，它就会影响最终的决定，你会发现原先的$u,\\Sigma$显然是不合理的，新的$u,\\Sigma$需要使得样本点的分布更接近下图虚线圆所标出的范围，除此之外，右侧的Prior Probability会给人一种比左侧大的感觉(右侧样本点”变多”了)。此时，unlabeled data对$P(C_1),P(C_2),u^1,u^2,\\Sigma$都产生了一定程度的影响，划分两个class的decision boundary也会随之发生变化 上面是比较直观的解释，接下来进行具体推导(假设做二元分类)： 先随机初始化一组参数：$\\theta=\\{P(C_1),P(C_2),u^1,u^2,\\Sigma\\}$ step1：利用初始model计算每一笔unlabeled data $x^u$属于class 1的概率$P_{\\theta}(C_1|x^u)$ step2：update model 如果不考虑unlabeled data，则先验概率显然为属于class1的样本点数$N_1$/总的样本点数$N$，即$P(C_1)=\\frac{N_1}{N}$ 而考虑unlabeled data时，分子还要加上所有unlabeled data属于class 1的概率和，此时它们被看作小数，可以理解为按照概率一部分属于$C_1$，一部分属于$C_2$ P(C_1)=\\frac{N_1+\\sum_{x^u}P(C_1|x^u)}{N}同理，对于均值，原先的mean $u_1=\\frac{1}{N_1}\\sum\\limits_{x^r\\in C_1} x^r$加上根据概率对$x^u$求和再归一化的结果即可 u_1=\\frac{1}{N_1}\\sum\\limits_{x^r\\in C_1} x^r+\\frac{1}{\\sum_{x^u}P(C_1|x^u)}\\sum\\limits_{x^u}P(C_1|x^u)x^u剩余的参数同理，接下来就有了一组新的参数$\\theta’$，于是回到step1-&gt;step2-&gt;step1循环 理论上该方法保证是可以收敛的，而一开始给$\\theta$的初始值会影响收敛的结果，类似gradient descent。 以上的推导基于的基本思想是，把unlabeled data $x^u$看成是可以划分的，一部分属于$C_1$，一部分属于$C_2$，此时它的概率$P_{\\theta}(x^u)=P_{\\theta}(x^u|C_1)P(C_1)+P_{\\theta}(x^u|C_2)P(C_2)$，也就是$C_1$的先验概率乘上$C_1$这个class产生$x^u$的概率+$C_2$的先验概率乘上$C_2$这个class产生$x^u$的概率。 实际上我们在利用极大似然函数更新参数的时候，就利用了该拆分的结果： logL(\\theta)=\\sum\\limits_{x^r} logP_{\\theta}(x^r)+\\sum\\limits_{x^u}logP_{\\theta}(x^u)3. Low-density Separation Assumption接下来介绍一种新的方法，它基于的假设是Low-density separation。通俗来讲，Low-density separation把这个世界看作是“非黑即白的”，在两个class的交界处data的密度(density)是很低的，它们之间会有一道明显的鸿沟，此时unlabeled data(下图绿色的点)就是帮助你在原本正确的基础上挑一条更好的boundary。 3.1. Self-traininglow-density separation最具代表性也最简单的方法是self training，大致的流程是： 先从labeled data去训练一个model $f^*$，训练方式没有限制； 然后用该$f^$去对unlabeled data打上label，$y^u=f^(x^u)$，也叫作pseudo label（伪标签）； 从unlabeled data中拿出一些data加到labeled data里，至于data的选取需要你自己设计算法来挑选； 回头再去训练$f^*$，循环即可。 注：low-density separation对Regression是不适用的。Regression是output一个数值，那用$f^$给unlabeled data打上label后，再取一部分加到原来的data里，对model $f^$是不会产生影响的，这部分新的data的Loss是0 。 实际上，该方法与之前提到的generative model还是挺像的，区别在于： Self Training使用的是hard label：假设一笔data就是属于某一个class； Generative Model使用的是soft label：假设一笔data可以按照概率划分，有多少几率属于class 1，有多少几率属于class 2等等。 如果我们使用的是neural network的做法，$\\theta^$是从labeled data中得到的一组参数，此时丢进来一个unlabeled data $x^u$，通过$f^_{\\theta^*}()$后得到$\\left [\\begin{matrix} 0.7\\\\ 0.3 \\end{matrix}\\right ]$，即它有0.7的概率属于class 1，0.3的概率属于class 2 如果此时使用hard label，则$x^u$的label被转化成$\\left [\\begin{matrix}1\\\\ 0 \\end{matrix}\\right ]$ 如果此时使用soft label，则$x^u$的label依旧是$\\left [\\begin{matrix} 0.7\\\\ 0.3 \\end{matrix}\\right ]$ 可以看到，在neural network里使用soft label是没有用的，如上面的例子中，unlabeled data $x^u$输入model得到的output是$\\left [\\begin{matrix} 0.7\\\\ 0.3 \\end{matrix}\\right ]$，那我们给$x^u$设的target（或者说给$x^u$新加的label）就是$\\left [\\begin{matrix} 0.7\\\\ 0.3 \\end{matrix}\\right ]$，那下次update model的时候，$x^u$的Loss就是0了，对参数$\\theta^*$的更新毫无贡献；而如果我们用hard label，unlabeled data $x^u$输入model得到的output是$\\left [\\begin{matrix} 0.7\\\\ 0.3 \\end{matrix}\\right ]$，那我们给$x^u$设的target就是$\\left [\\begin{matrix} 1\\\\ 0 \\end{matrix}\\right ]$，那下次update model的时候，$x^u$的Loss即不为0，对参数更新是有贡献的。实际上low density separation就是通过强制分类来提升分类效果的方法。 3.2. Entropy-based RegularizationEntropy-based Regularization是low-density separation的进阶版，你可能会觉得hard label这种直接强制性打标签的方式有些太武断了，你如果用neural network，得到的output是一个distribution，我们用hard label去限制这个output一定是class 1或者class 2，但是我们之前做了Low-density separation的假设，即希望得到的distribution是很集中的，比如distribution集中在class 1，而在其他class上很小。 由于我们不知道unlabeled data $x^u$的label到底是什么，但如果通过entropy-based regularization得到的分布集中在某个class上的话，那这个model就是好的，而如果分布是比较分散的，那这个model就是不好的，那如何去评价我们得到的distribution是好还是坏呢，就可以用到Entropy-based regularization。 接下来的问题是，如何用数值的方法来evaluate distribution的集中(好坏)与否，要用到的方法叫entropy，一个distribution的entropy可以告诉你它的集中程度。输入unlabeled data $x^u$得到的output $y^u=f^_{\\theta^}(x^u)$，其中$y^u$是一个概率分布(distribution)，那Entropy of $y^u$为： E(y^u)=-\\sum\\limits_{m=1}^5 y_m^u ln(y_m^u)对上图中的第1、2种情况，算出的$E(y^u)=0$，而第3种情况，算出的$E(y^u)=-ln(\\frac{1}{5})=ln(5)$，可见entropy越大，distribution就越分散，entropy越小，distribution就越集中。 我们的目标是在labeled data上分类要正确，在unlabeled data上，output的entropy要越小越好，此时就要修改loss function。 对labeled data来说，它的output要跟正确的label越接近越好，用cross entropy表示如下： L=\\sum\\limits_{x^r} C(y^r,\\hat y^r) 对unlabeled data来说，要使得该distribution(也就是output)的entropy越小越好： L=\\sum\\limits_{x^u} E(y^u) 两项综合起来，可以用weight来加权，以决定哪个部分更为重要一些 L=\\sum\\limits_{x^r} C(y^r,\\hat y^r) + \\lambda \\sum\\limits_{x^u} E(y^u) 可以发现该式长得很像之前讲过的L1/L2 Regularization，这也就是entropy-based regularization的名称由来。 3.3. Semi-supervised SVM另一个很著名的半监督学习的方法是Semi-supervised SVM。（SVM在之后的课程才会讲到，这里先做一下简单的介绍）SVM要做的是，给你两个class的data，去找一个boundary： 要有最大的margin，让这两个class分的越开越好 要有最小的分类错误 对unlabeled data穷举所有可能的label，如下图中列举了三种可能的情况；然后对每一种可能的结果都去算SVM，再找出可以让margin最大，同时又minimize error的那种情况，如下图中用黑色方框标注的情况。 SVM的基本想法是极大化划分超平面与两类样例之间的最小距离，即所谓的“Margin”，以增强算法的鲁棒性。 （摘自机器学习中的“Margin”） SVM paper：Thorsten Joachims, ”Transductive Inference for Text Classification using Support Vector Machines”, ICML, 1999 当然这么做会存在一个问题，对于n笔unlabeled data，意味着即使在二元分类里也有$2^n$种可能的情况，数据量大的时候，几乎难以穷举完毕，上面给出的paper提出了一种approximate的方法，基本精神是：一开始你先得到一些label，然后每次改一笔unlabeled data的label，看看可不可以让你的objective function变大，如果变大就去改变该label。 3. Smoothness Assumptionsmoothness assumption的思想可以理解是：“近朱者赤，近墨者黑”，即使相似的$x$具有相同的$\\hat y$，精确的定义是： x的分布是不平均的； 如果$x^1$和$x^2$在一个high density region上很接近的话，那么$\\hat y^1$和$\\hat y^2$就是相同的，也就是这两个点可以在样本点高密度集中分布的区域块中有一条可连接的路径，即 connected by a high density path。 假设下图是data的分布，$x^1,x^2,x^3$是其中的三笔data，如果单纯地看$x$的相似度，显然$x^2$和$x^3$更接近一些，但对于smoothness assumption来说，$x^1$和$x^2$是处于同一块区域的，它们之间可以有一条相连的路径；而$x^2$与$x^3$之间则是“断开”的，没有high density path，因此$x^1$与$x^2$更“像”。 以手写数字识别为例，对于最右侧的2和3以及最左侧的2，显然最右侧的2和3在pixel上相似度更高一些；但如果把所有连续变化的2都放进来，就会产生一种“不直接相连的相似”，根据Smoothness Assumption的理论，由于2之间有连续过渡的形态，因此第一个2和最后一个2是比较像的，而最右侧2和3之间由于没有过渡的data，因此它们是比较不像的；人脸识别的两个侧面的过渡也同理。 Smoothness Assumption在file classification（文件分类）上是非常有用的，假设对天文学(astronomy)和旅行(travel)的文章进行分类，它们各自有专属的词汇，此时如果unlabeled data与label data的词汇是相同或重合(overlap)的，那么就很容易分类；但在真实的情况下，unlabeled data和labeled data之间可能没有任何重复的words，因为世界上的词汇太多了，sparse的分布很难会使overlap发生。但如果收集到的unlabeled data足够多，就会以一种相似传递的形式，比如下图中$d_1$和$d_5$像，$d_5$和$d_6$像，$d_6$和$d_7$像，会propagation起来，建立起文档之间相似的桥梁。 cluster and then label 在具体实现上，有一种简单的方法是cluster and then label，也就是先把data分成几个cluster，划分class之后再拿去训练，但这种方法不一定会得到好的结果，因为它的假设是你可以把同一个class的样本点cluster在一起，而这其实是没那么容易的。 对图像分类来说，如果单纯用pixel的相似度来划分cluster，得到的结果一般都会很差，你需要设计一个很好的方法来描述image(类似Deep Autoencoder的方式来提取feature)，这样cluster才会有效果。 Graph-based Approach 之前讲的是比较直觉的做法，接下来引入Graph Structure来表达connected by a high density path这件事。 我们把所有的data points都建成一个graph，有时候建立vertex（顶点）之间的关系是比较容易的，比如网页之间的链接关系、论文之间的引用关系；但有时候需要你自己去寻找vertex之间的关系。Graph的好坏，对结果起着至关重要的影响，而如何build graph却是一件heuristic（启发式的，探索的）的事情，需要凭着经验和直觉来做。 首先定义两个object $x^i,x^j$之间的相似度 $s(x^i, x^j)$ 如果是基于pixel的相似度，performance可能会不太好；建议使用autoencoder提取出来的feature来计算相似度，得到的performance会好一些 算完相似度后，就可以建graph了，方式有很多种： k nearest neighbor：假设k=3，则每个point与相似度最接近的3个点相连 e-neighborhood：每个point与相似度超过某个特定threshold $e$的点相连 除此之外，还可以给Edge特定的weight，让它与要连接起来的两个data point的相似度$s(x^i,x^j)$成正比 建议用RBM function来确定相似度：$s(x^i,x^j)=\\exp(-\\gamma||x^i-x^j||^2 )$ 这里$x^i,x^j$均为vector，计算它们的Euclidean Distance(欧几里得距离)，加上参数后再取exponential 至于加exponential，经验上来说通常是可以帮助提升performance的，在这里只有当$x^i,x^j$非常接近的时候，singularity才会大；只要距离稍微远一点，singularity就会下降得很快，变得很小 使用exponential的RBM function可以做到只有非常近的两个点才能相连，稍微远一点就无法相连的效果，避免了下图中跨区域相连的情况 graph-based approach的基本思想是，在graph上已经有一些labeled data，那么跟它们相连的point，属于同一类的概率就会上升，每一笔data都会去影响它的邻居，而graph带来的最重要的好处是，这个影响是会随着edges传递出去的，即使有些点并没有真的跟labeled data相连，也可以被传递到相应的属性。 比如下图中，如果graph建的足够好，那么从两个被分别label为蓝色和红色的点就可以传递出两张完整的图；从中我们也可以看出，如果想要让这种方法生效，收集到的data一定要足够多，否则可能传递到一半，graph就断掉了，information的传递就失效了。 介绍完了如何定性使用graph，接下来介绍一下如何定量使用graph。定量的使用方式是定义label的smoothness，我们期望smooth的值越小越好。下图中，edge上的数字是weight，$x^i$表达data，$y^i$表示data的label，计算smoothness的方式为： S=\\frac{1}{2}\\sum\\limits_{i,j} w_{i,j}(y^i-y^j)^2比如计算下面两个graph的smoothness： \\begin{split} S_1 &=\\frac{1}{2}\\left[w_{1,2}(y^1-y^2)^2 +w_{1,3}(y^1-y^3)^2+ w_{2,3}(y^2-y^3)^2+w_{3,4}(y^3-y^4)^2\\right]\\\\ &=\\frac{1}{2}\\left[2*(1-1)^2 +3*(1-1)^2+ 1*(1-1)^2+1*(1-0)^2\\right]=0.5\\\\ S_2&=\\frac{1}{2}\\left[2*(0-1)^2 +3*(0-1)^2+ 1*(1-1)^2+1*(1-0)^2\\right]=3\\\\ \\end{split}则有$S_1&lt;S_2$，那么左边的graph比右边的graph要更smooth。 当然上面的式子还可以化简，如果把labeled data和unlabeled data的y组成一个(R+U)-dim vector，即 y=\\left [\\begin{matrix} ...y^i...y^j \\end{matrix} \\right ]^T于是smooth可以改写为： S=\\frac{1}{2}\\sum\\limits_{i,j} w_{i,j}(y^i-y^j)^2=y^TLy其中L为(R+U)×(R+U) matrix，成为Graph Laplacian， 定义为$L=D-W$ $W$：把data point两两之间weight的关系建成matrix，代表了$x^i$与$x^j$之间的weight值 $D$：把$W$的每一个row上的值加起来放在该行对应的diagonal上即可，比如5=2+3,3=2+1,… 对$S=y^TLy$来说，$y$是label，是neural network的output，取决于neural network的parameters，因此要在原来仅针对labeled data的loss function中加上这一项，得到： L=\\sum\\limits_{x^r}C(y^r,\\hat y^r) + \\lambda S其中的$\\lambda S$实际上也是一个regularization term。那现在我们的训练目标是： labeled data的cross entropy越小越好(neural network的output跟真正的label越接近越好) smooth S越小越好(neural network的output，不管是labeled还是unlabeled，都要符合Smoothness Assumption的假设) 具体训练的时候，不一定只局限于neural network的output要smooth，可以对中间任意一个hidden layer加上smooth的限制，也可以使每一个hidden layer的output都是smooth。 4. Better RepresentationBetter Representation的思想可以理解是“去芜存菁，化繁为简”。（我们观察到的世界是比较复杂的，而在它的背后其实是有一些比较简单的东西，在操控着这个复杂的世界，所以只要你能够看透这个世界的假象，直指它的核心的话，就可以让training变得比较容易）举一个例子，在神雕侠侣中，杨过要在三招之内剪掉樊一翁的胡子，虽然胡子的变化是比较复杂的，但头的变化是有限的，杨过看透了这一件事情就可以把胡子剪掉。在这个例子中，樊一翁的胡子就是original representation，而他的头就是你要找的better representation。（李老师真是太博学了Σ(っ°Д°;)っ） 算法具体思路和内容将在下一次的笔记Unsupervised Learning-Principal Component Analysis (PCA)中介绍。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://nekomoon404.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Semi-supervised learning","slug":"Semi-supervised-learning","permalink":"http://nekomoon404.github.io/tags/Semi-supervised-learning/"},{"name":"Low-density separation assumption","slug":"Low-density-separation-assumption","permalink":"http://nekomoon404.github.io/tags/Low-density-separation-assumption/"},{"name":"Self-training","slug":"Self-training","permalink":"http://nekomoon404.github.io/tags/Self-training/"},{"name":"Entropy-based regularization","slug":"Entropy-based-regularization","permalink":"http://nekomoon404.github.io/tags/Entropy-based-regularization/"},{"name":"Smoothness assumption","slug":"Smoothness-assumption","permalink":"http://nekomoon404.github.io/tags/Smoothness-assumption/"},{"name":"Graph-based approach","slug":"Graph-based-approach","permalink":"http://nekomoon404.github.io/tags/Graph-based-approach/"}]},{"title":"DL笔记（9）Recurrent Neural Network(RNN)","slug":"ML笔记（9）Recurrent-Neural-Network-RNN","date":"2020-07-20T13:38:02.000Z","updated":"2020-07-23T13:38:02.000Z","comments":true,"path":"2020/07/20/ML笔记（9）Recurrent-Neural-Network-RNN/","link":"","permalink":"http://nekomoon404.github.io/2020/07/20/ML%E7%AC%94%E8%AE%B0%EF%BC%889%EF%BC%89Recurrent-Neural-Network-RNN/","excerpt":"","text":"1. Example Application先来看一个例子：Slot Filling（槽填充）。这是个NLP领域的知识点，这里作一下简单介绍，参考知乎文章槽填充（Slot Filling）的定义、用途、意义及其他。 One way of making sense of a piece of text is to tag the words or tokens which carry meaning to the sentences. In the field of Natural Language Processing, this problem is known as Semantic Slot Filling.——mc.ai 槽填充是指从大规模的语料库中抽取给定实体（query）的被明确定义的属性（slot types）的值（slot fillers）。 填槽指的是为了让用户意图转化为用户明确的指令而补全信息的过程。 Slot Filling的用途或意义可以是：用于任务型对话；作为意图识别的关键字；作为下一步对话的提示信息。 设想一个订票系统，它听到用户说：“ i would like to arrive Taipei on November 2nd”，你的系统有一些slot(如有一个slot是Destination，一个slot是time of arrival)，系统要自动知道这边的每一个词汇是属于哪一个slot，比如Taipei属于Destination这个slot，November 2nd属于time of arrival这个slot。 这个问题当然可以使用一个feedforward neural network来解，叠一个feedforward neural network，input是一个词汇(把Taipei变成一个vector)输入到这个neural network。 以下是把词汇用向量来表示的方法： 1-of-N encoding Beyond 1-of-N encoding 如果只是用1-of-N encoding来描述一个词汇的话会遇到一些问题，因为有很多词汇你可能都没有见过，所以需要在1-of-N encoding里面多加dimension，图中左边绿色条最后一个dimension代表other。不是在我们词言有的词汇就归类到other里面去(如Gandalf,Sauron归类到other里面去)。或者可以用每一个词汇的字母来表示它的vector，比如apple，apple里面有出现app、ppl、ple，那在这个vector里面对应到app,ple,ppl的dimension就是1,而其他都为0。 假设我们已经解决了把词汇表示为vector，然后把这个vector输入到feedforward neural network里，在这个task里面，我们希望output是一个probability distribution。这个probability distribution代表着我们现在input词汇属于每一个slot的几率，比如Taipei属于destination的几率和Taipei属于time of departure的几率。 但这会遇到一些feedforward neural network没办法解决的问题。假设现在有一个使用者说：“arrive Taipei on November 2nd”(arrive-other,Taipei-dest, on-other,November-time,2nd-time)。那现在有人说:”leave Taipei on November 2nd”，这时候Taipei就变成了“place of departure”，它应该是出发地而不是目的地。但是对于feedforward neural network来说，input一样的东西output就应该是一样的东西(input “Taipei”，output要么是destination几率最高，要么就是place of departure几率最高)，没有办法一会让出发地的几率最高，一会让它目的地几率最高。 那如果今天我们的neural network是有记忆力的，它记得看红色的Taipei之前它就已经看过arrive这个词汇；它记得看绿色的Taipei之前，它就已经看过leave这个词汇，那么它就可以根据上下文产生不同的output。如果让我们的neural network是有记忆力的话，它就可以解决在两句不同的话中，input相同的词汇，得到的output不同，这样一个问题。 2. RNN这种有记忆的neural network就叫做Recurrent Neural network，RNN（循环神经网络）。在RNN里面，每一次hidden layer的neuron产生output的时候，这些output会被存到memory里去(下图中用蓝色方块表示memory)。那下一次当有input时，这些neuron不只是考虑input$x_1,x_2$，还会考虑存到memory里的值$a_1,a_2$。 通过下面的例子来理解RNN的原理，设下图中的RNN中的neuron的weight都是1，bias都是0，若input sequence为$[1,1]^T,[1,1]^T,[2,2]^T$，给定memory中的初始值$a_1=a_2=0$，那么output sequence为： input $[1,1]^T$，$x_1=x_2=1$，$a_1=a_2=0$；hidden layer得到 $a_1=a_2=x_1+x_2+a_1+a_2=2$；output $y_1=y_2=a_1+a_2=4$； input $[1,1]^T$，$x_1=x_2=1$，$a_1=a_2=2$；hidden layer得到 $a_1=a_2=6$；output $y_1=y_2=12$； input $[2,2]^T$，$x_1=x_2=2$，$a_1=a_2=6$；hidden layer得到 $a_1=a_2=16$；output $y_1=y_2=32$； 需要注意的是：对于RNN，input sequence的顺序并不是independent的， 是会对输出的结果造成影响的，所以在RNN中我们要考虑input sequence的order。 那现在用RNN来处理Slot Filling这个问题的流程大致是这样的：顾客说”arrive Taipei on November 2nd”，arrive转换为vector输入到network；然后经过hidden layer得到$a^1$，它是hidden layer的输出，也是个vector；由$a^1$在output layer得到输出$y^1$，$y^1$是arrive属于每一个slot的几率。$a^1$会被存储到memory中，接下来Taipei转换为输入的vector $x^2$，hidden layer会同时考虑$a^1$和$x^2$，得到$a^2$，然后得到$y^2$，$y^2$是Taipei属于每个slot的几率。之后就以此类推。注意图中并不是三个network，而是同一个network在三个不同的时间点被使用，同样的weight用同样的颜色表示。 这样就可以解决之前那个问题了，如果两次分别输入的是arrive Taipei和leave Taipei，由于arrive和leave的不同，导致hidden layer得到$a^1$不同，由于在输入Taipei时，hidden layer会同时考虑$a^1$和$x^2$，那两次得到的$y^2$就很会不同，从而把语义区分开。 当然RNN的架构是可以根据具体的需要来设计的，上图中的hidden layer只有一层，当然也可以做成Deep Recurrent Neural Network。 Elman network &amp;Jordan network RNN有很多种变形，上面讲到的叫Elman Network，它是把hidden layer得到的值存起来，在下一个时间点再读出来；有另外一种RNN叫作Jordan Network，它每次存的是network的output值，在下一个时间点再读出来。（据说Jordan Network的效果可能好一下，理解是因为output y是有target的，我们清楚是把什么东西放进了memory；而hidden layer是没有target的，很难控制说它能学到什么hidden layer information）。 Bidirectional neural network RNN还可以是双向的，上面讲的RNN，input一个句子，是从句首一直读到句尾。假设句子里的每一个词汇由$x_t$表示，RNN就是先读$x_t$，再读$x_{t+1}$，再读$x_{t+2}$。那它的读取方向也可以是反过来的，可以先读$x_{t+2}$，再读$x_{t+1}$，再读$x_t$。我们可以同时train一个正向的RNN和一个逆向的RNN，然后把这两个RNN的hidden layer得到的值拿出来，都接给一个output layer得到最后的$y_t$，比如正向的network在input$x_t$的时候hidden layer得到的值，跟逆向的network在input $x_t$时hidden layer得到值，都输入到output layer中得到$y_t$，然后以此类推得到$y_{t+1},y_{t+2}$。 用Bidirectional neural network的好处是，neural在产生output的时候，它看的范围是比较广的。如果你只有正向的network，再产生$y_t,y_{t+1}$的时候，你的network只看过$x_1$到$x_{t+1}$的input。但Bidirectional neural network，在产生$y_t,y_{t+1}$的时候，你的network不只看过$x_1$到$x_{t+1}$的input，它也看了从句尾$x_N$到$x_{t+1}$的input，就等于整个input的sequence。那对于第一节中讲到的slot filling的例子，你的network就等于看了整个sentence后，才决定每一个词汇的slot应该是什么。这样会比看sentence的一半还要得到更好的performance。 3. LSTM3.1 What is LSTM上面讲到的memory方式是最简单的，我们可以随时把值存到memory去，也可以把值读出来。但目前最常用的memory方式是Long Short-term Memory(LSTM，长时间的短期记忆)。 Long Short-term Memory有三个gate，当某个neural的output想要被写到memory cell里面的时候，必须通过一个input Gate，input Gate被打开的时候，你才能把值写到memory cell里面，而input Gate是打开还是关起来是neural network自己学的。这个cell输出的地方有一个output Gate，output Gate决定其他的neural可不可以从这个memory里面把值读出来，output Gate关闭时不能读出，打开时才可以把值读出来。跟input Gate一样，output Gate何时打开或关闭，是由network自己学到的。第三个gate叫做forget Gate，forget Gate决定什么时候memory cell要把过去记得的东西“忘掉”，即将memory存的值清0再去存新的值，当然forget Gate何时把存在memory的值忘掉，也是由network自己学到的。 那整个LSTM你可以看成，它有四个input 1个output，这四个input中，一个是想要被存在memory cell的值(但它不一定存的进去)还有操控input Gate的讯号，操控output Gate的讯号，操控forget Gate的讯号，最终只会得到一个output。 （冷知识：这个“-”应该在short-term中间，是长时间的短期记忆。想想我们之前看的Recurrent Neural Network，它的memory在每一个时间点都会被洗掉，只要有新的input进来，每一个时间点都会把memory 洗掉，所以的short-term是非常short的，但如果是Long Short-term Memory，它记得会比较久一点(只要forget Gate不要决定要忘记，它的值就会被存起来。) 下面来跟详细地看一下LSTM的memory cell的formulation。 假设要被存到cell的input是z，input gate的输入叫做$z_i$（一个scalar），forget gate的输入叫做$z_f$，output gate的输入叫做$z_o$，得到的output 记为$a$。假设cell里面有这四个输入之前，它里面已经存了值$c$。 假设要输入的部分为$z$，那三个gate分别是由$z_i,z_f,z_0$所操控的，每个gate里是activation function，通常会选择sigmoid function，选择sigmoid function的意义是它的值是介在0到1之间的。这个0到1之间的值代表了这个gate被打开的程度(如果这个f的output是1，表示为被打开的状态，反之代表这个gate是关起来的)。 z通过activation function得到$g(z)$，$z_i$通过input Gate的activation function得到$f(z_i)$，把$g(z)$乘以$f(z_i)$得到$g(z)f(z_i)$，$z_f$通过forget Gate的sigmoid function得到$f(z_f)$。接下来把存到memory里面的值$c$乘以$f(z_f)$得到$cf(z_f)$，然后加起来$c’=g(z)f(z_i)+cf(z_f)$，那么$c’$就是重新存到memory里面的值。 所以根据目前的运算，$f(z_i)$会cortrol$g(z)$可不可以make snese，假设$f(z_i)$为0，那$g(z)f(z_i)$就等于0，那$g(z)$就没什么用了，如果$f(z_i)$f等于1，就相当于是把$g(z)$当做输入) 。而$f(z_f)$则会决定要不要把存在memory的值洗掉，假设$f(z_f)$为1(forget gate 开启的时候)，这时$c$还会被记得，如果$f(z_f)$等于0(forget gate关闭的时候)，$cf(z_f)$等于0，原来的$c$相当于被洗掉了。然后把这个两个值加起来$c’=g(z)f(z_i)+cf(z_f)$。（forget gate的开关跟我们的直觉有些相反，那这个forget gate打开的时候代表的是记得，关闭的时候代表的是遗忘）。 $c’$通过另一个avtivation function得到$h(c’)$，将$h(c’)$乘以$f(z_o)$得到$a = h(c’)f(z_o)$，output gate受$f(z_o)$control，$f(z_o)$等于1的话，就说明$h(c’)$能通过，$f(z_o)$等于0的话，说明memory里面存在的值没有办法通过output gate被读取出来。 3.2 LSTM - Example下面通过一个例子来更直观地理解LSTM。假设我们的network里面只有一个LSTM的cell，input都是三维的vector，output都是一维的。三维的vector跟output还有memory的关系是： 当第二个dimension $x_2$的值是1时，$x_1$的值就会被add到memory里； 当$x_2$的值是-1时，就会reset the memory; 当$x_3$的值为1时，才会把memory中的值output出来。 假设我们原来存到memory里面的值是0，当第二个input的$x_2$值是1，3会被存到memory里面去。第四个输入的$x_2$等于1，所以4会被add到memory里面去，memory中存的值变为7。第六个输入的$x_3$等于1，这时memory中的7会被输出。第七个dimension的$x_2$的值为-1，memory里面的值会被洗掉变为0。第八个dimension的$x_2$的值为1，所以把6存进去，下一个input的$x_3$的值为1，所以把6输出。 那接下来我们做一下实际的运算。下图是一个memory cell。cell的四个input是由：input的三维vector和bias（$x_1,x_2,x_3,1$）乘以不同的四组权重得到$z,z_i,z_f,z_o$， 这些权重和bias用train data通过Gradient Descent的方式训练得到的。 这里假设我们已经知道这些值是多少了，并且假设$g(z)=z$和$h(c’)=c’$，三个Gate都是sigmoid function，memory中的$c$的初始值为0。 我们先来大致分析一下这个LSTM cell在运算时会发生什么。对于$z$，$x_1$乘以1，其他的元素乘以0，那就是直接把$x_1$当做输入。在input gate时，$x_2$乘以100，bias乘以-10，$x_2$值很小时，通常input gate是关闭的，若$x_2$的值较大，如大于1，$z_i$会是一个正值，代表input gate会被打开 。forget gate通常是会被打开的，既它平常会一直记得东西，只有当$x_2$的值为一个很大的负值时，$f(z_f)$接近0，才会把forget gate关起来。output gate平常是被关闭的，因为bias项乘-1得到一个很大的负值，那当$x_3$是一个很大的正值时，output Gate会打开。 那给定一组input sequence会得到什么样的输出呢，我们一步一步来看。 （1） $\\begin{bmatrix} x_1&amp;x_2&amp;x_3\\end{bmatrix}^T=\\begin{bmatrix} 3&amp; 1&amp; 0\\end{bmatrix}^T$ $z=\\begin{bmatrix}1 &amp;0&amp;0&amp;0\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}1 &amp;0&amp;0&amp;0\\end{bmatrix}\\cdot\\begin{bmatrix} 3\\\\1\\\\0\\\\1\\end{bmatrix} =3$；$g(z)=z=3$； $z_i=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} 3\\\\1\\\\0\\\\1\\end{bmatrix} =90$；$f(z_i)=\\sigma(90)\\approx1$； $z_f=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; 10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; 10\\end{bmatrix}\\cdot\\begin{bmatrix} 3\\\\1\\\\0\\\\1\\end{bmatrix} =110$；$f(z_f)=\\sigma(110)\\approx1$； Memory：$c’=g(z)f(z_i)+cf(z_f)=3\\cdot1+0\\cdot1=3$；$h(c’)=c’=3$； $z_o=\\begin{bmatrix}0 &amp; 0 &amp;100 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 0 &amp; 100 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} 3\\\\1\\\\0\\\\1\\end{bmatrix} =-10$；$f(z_f)=\\sigma(-10)\\approx 0$； Output：$a = h(c’)f(z_o)=3\\cdot 0=0$ （2）$\\begin{bmatrix} x_1&amp;x_2&amp;x_3\\end{bmatrix}^T=\\begin{bmatrix} 4&amp; 1&amp; 0\\end{bmatrix}^T$ $z=\\begin{bmatrix}1 &amp;0&amp;0&amp;0\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}1 &amp;0&amp;0&amp;0\\end{bmatrix}\\cdot\\begin{bmatrix} 4\\\\1\\\\0\\\\1\\end{bmatrix} =4$；$g(z)=z=4$； $z_i=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} 4\\\\1\\\\0\\\\1\\end{bmatrix} =90$；$f(z_i)=\\sigma(90)\\approx1$； $z_f=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; 10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; 10\\end{bmatrix}\\cdot\\begin{bmatrix} 4\\\\1\\\\0\\\\1\\end{bmatrix} =110$；$f(z_f)=\\sigma(110)\\approx1$； Memory：$c’=g(z)f(z_i)+cf(z_f)=4\\cdot1+3\\cdot1=7$；$h(c’)=c’=7$； $z_o=\\begin{bmatrix}0 &amp; 0 &amp;100 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 0 &amp; 100 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} 4\\\\1\\\\0\\\\1\\end{bmatrix} =-10$；$f(z_f)=\\sigma(-10)\\approx 0$； Output：$a = h(c’)f(z_o)=7\\cdot 0=0$ （3）$\\begin{bmatrix} x_1&amp;x_2&amp;x_3\\end{bmatrix}^T=\\begin{bmatrix} 2&amp; 0&amp; 0\\end{bmatrix}^T$ $z=\\begin{bmatrix}1 &amp;0&amp;0&amp;0\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}1 &amp;0&amp;0&amp;0\\end{bmatrix}\\cdot\\begin{bmatrix} 2\\\\0\\\\0\\\\1\\end{bmatrix} =2$；$g(z)=z=2$； $z_i=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} 2\\\\0\\\\0\\\\1\\end{bmatrix} =-10$；$f(z_i)=\\sigma(-10)\\approx 0$； $z_f=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; 10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; 10\\end{bmatrix}\\cdot\\begin{bmatrix} 2\\\\0\\\\0\\\\1\\end{bmatrix} =10$；$f(z_f)=\\sigma(10)\\approx1$； Memory：$c’=g(z)f(z_i)+cf(z_f)=2 \\cdot 0 + 7 \\cdot 1=7$；$h(c’)=c’=7$； $z_o=\\begin{bmatrix}0 &amp; 0 &amp;100 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 0 &amp; 100 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} 2 \\\\ 0 \\\\0 \\\\1\\end{bmatrix} =-10$；$f(z_f)=\\sigma(-10)\\approx 0$； Output：$a = h(c’)f(z_o)=7\\cdot 0=0$ （4）$\\begin{bmatrix} x_1&amp;x_2&amp;x_3\\end{bmatrix}^T=\\begin{bmatrix} 1&amp; 0&amp; 1\\end{bmatrix}^T$ $z=\\begin{bmatrix}1 &amp;0&amp;0&amp;0\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}1 &amp;0&amp;0&amp;0\\end{bmatrix}\\cdot\\begin{bmatrix} 1 \\\\0 \\\\1\\\\1\\end{bmatrix} =1$；$g(z)=z=1$； $z_i=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} 1 \\\\0 \\\\1 \\\\1\\end{bmatrix} =-10$；$f(z_i)=\\sigma(-10)\\approx 0$； $z_f=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; 10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; 10\\end{bmatrix}\\cdot\\begin{bmatrix} 1\\\\0\\\\1\\\\1\\end{bmatrix} =10$；$f(z_f)=\\sigma(10)\\approx1$； Memory：$c’=g(z)f(z_i)+cf(z_f)=1 \\cdot 0 + 7 \\cdot 1=7$；$h(c’)=c’=7$； $z_o=\\begin{bmatrix}0 &amp; 0 &amp;100 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 0 &amp; 100 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} 1 \\\\ 0 \\\\1 \\\\1\\end{bmatrix} =90$；$f(z_f)=\\sigma(90)\\approx 1$； Output：$a = h(c’)f(z_o)=7\\cdot 1= 7$ （5）$\\begin{bmatrix} x_1&amp;x_2&amp;x_3\\end{bmatrix}^T=\\begin{bmatrix} 3&amp; -1&amp; 0\\end{bmatrix}^T$ $z=\\begin{bmatrix}1 &amp;0&amp;0&amp;0\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}1 &amp;0&amp;0&amp;0\\end{bmatrix}\\cdot\\begin{bmatrix} 3 \\-1 \\\\0\\\\1\\end{bmatrix} =3$；$g(z)=z=3$； $z_i=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} 3 \\-1 \\\\0 \\\\1\\end{bmatrix} =-110$；$f(z_i)=\\sigma(-110)\\approx 0$； $z_f=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; 10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 100 &amp;0 &amp; 10\\end{bmatrix}\\cdot\\begin{bmatrix} 3 \\-1 \\\\0\\\\1\\end{bmatrix} =-90$；$f(z_f)=\\sigma(-90)\\approx 0$； Memory：$c’=g(z)f(z_i)+cf(z_f)=3 \\cdot 0 + 7 \\cdot 0=0$；$h(c’)=c’=0$； $z_o=\\begin{bmatrix}0 &amp; 0 &amp;100 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}=\\begin{bmatrix}0 &amp; 0 &amp; 100 &amp; -10\\end{bmatrix}\\cdot\\begin{bmatrix} 3 \\-1 \\\\0 \\\\1\\end{bmatrix} =-10$；$f(z_f)=\\sigma(-10)\\approx 0$； Output：$a = h(c’)f(z_o)=0\\cdot 0= 0$ 所以最终得到的output sequence为：$0,0,0,7,0$ 3.3 Replace the neurons with LSTM到这里你可能会想这个LSTM跟我们的neural network有什么样的关系呢[・ヘ・?]。可以这样来理解，在原来的neural network里会有很多的neuron，我们把input乘以不同的weight当做不同neuron的输入，每一个neuron都是一个function，输入一个值然后输出一个值。但是如果是LSTM的话，其实你只要把这一整个memory cell当成是一个neuron就好了。 你做的事情其实就是原来简单的neuron换成LSTM，现在的input($x_1,x_2$)会乘以不同的weight当做LSTM的输入(假设我们这个hidden layer只有两个neuron)。input($x_1,x_2$)会乘以不同的weight当做底下的input $z$，乘以不同的weight操控input gate，乘以不同的weigh去操控forget gate，乘以不同的weight会去操控output gate。 在原来的neural network里每个neuron是有一个input一个output。而LSTM有四个input和一个output，对于LSTM来说，这四个input是不一样的。所以LSTM需要的参数量(假设你现在用的neural的数目跟LSTM是一样的)是一般neural network的四倍。 那这个LSTM跟Recurrent Neural Network的关系是什么呢，借助下面的图来理解一下。 假设我们现在有一整排的LSTM cell，这些LSTM里面的memory都存了一个值，把所有的值接起来就变成了vector，写为 $c^{t-1}$。在时刻t，input一个vector $x^t$，这个vector首先会乘上一matrix，即经过一个linear transform变成一个vector $z$，$z$的dimension等于LSTM cell的数目，其中每一个元素分别是一个个cell的“底下”的input $z$。 同样的道理，$x^t$会乘上另外的一个transform得到$z^i$，然后这个$z^i$zi的dimension也跟cell的数目一样，$z^i$的每一个dimension会分别去操控这些cell的input gate，forget gate 跟output gate也都是一样的道理。所以我们把$x^t$乘以四个不同的transform得到四个不同的vector $z^f,z^i,z,z^o$，四个vector的dimension跟cell的数目一样，这四个vector合起来就会去操控这些memory cell运算（上小节中讲到的运算过程中的$z,z_i,z_f,z_o$是scalar，现在换成vector即可完成下图中的运算）。 经过这一轮运算后memory中存放的是$c^t$，输出$y^t$，将这个process反复的进行，在下一个时间点input $x^{t+1}$，把$f(z)$跟input gate相乘，把forget gate跟存在memory里面的值相乘，然后将前面两个值再相加起来，在乘上output gate的值，然后得到下一个时间点的输出$y^{t+1}$。 但是这远远不是LSTM的最终形态 ∑(ﾟДﾟノ)ノ，LSTM会把上一个时间的$h^t$接进下一个时间的input，也就说下一个时间点操控这些gate的值不是只看input $x^{t+1}$，还看前一个时间点的$h^t$。LSTM还可以把上一时间的存在memory cell的值$c^t$接进下一时间的input，这叫做“peephole”，这样就同时考虑了$x^{t+1},h^t,c^t$，把这三个vector合在一起乘上不同的transform得到四个不同的vector再放进LSTM中运算。 当然LSTM也可以有很多层，即Multiple-layer LSTM。目前人们使用RNN，通常就是在用LSTM。 Keras支持三种RNN：LSTM，GRU，SimpleRNN。GRU是LSTM稍微简化的版本，它只有两个gate，虽然少了一个gate，但是performance跟LSTM差不多，少了1/3的参数，比较不容易overfitting。文章第二节中讲的那种RNN就是simple RNN。 4. How to train RNN4.1 BPTT那如何训练一个RNN model呢，我们之前讲过如果要做learning的话，要定义一个cost function来evaluate model的好坏，选一个parameter让loss 最小。那在Recurrent Neural Network里面，要怎么定义这个loss呢，下面先举个比价直观的例子。 假设现在要做的是Slot Filling，train data是给一些sentence，sentence的label是其中的每个word分别属于哪个Slot。比如上图中这歌Training Sentence，告诉machine第一个word它是属于other slot，“Taipei是”Destination slot,”on”属于other slot，“November”和“2nd”属于time slot， 然后把“arrive”输入到RNN，RNN会得到一个output $y^1$，接下来$y^1$会和它的reference vector（other这个Slot的vector）来计算cross entropy。可以这样来定义reference vector，其长度等于slot的数目，某一个slot对应的vector中某一个dimension为1，其余为0。那我们就希望输入”arrive”后得到的$y^1$与”other”对应的reference vector越接近越好。 对于training RNN要注意的事是，我们不能把sentence的word顺序打散后输入到RNN中，而必须要按照原来的sequence order输入到RNN中，因为sequence order会影响memory中存的值，进而影响output $y$。 RNN的损失函数，即output和reference vector的entropy的和，就是要最小化的对象。这里也是用梯度下降来做，有了定义好的loss function $L$，要update这个neural network里面的某个参数$w$，就是计算$L$对$w$的偏微分，偏微分计算出来以后，就用梯度下降的方法去update里面的参数。在讲feedforward neural network的时候，可以用一个有效率的求梯度的方法，即Backpropagation（反向传播）。那Recurrent Neural Network领域，有开发一套Backpropagation的进阶版，叫做Backpropagation through time（BPTT）。它跟Backpropagation是很类似的，只是RNN它是在high sequence上运作，所以BPTT要考虑时间上的information。 4.2 Difficulty in training RNN不幸的是，RNN的training是比较困难的。一般在做training的时候，你会期待learning curve是像下图中蓝色这条线（纵轴是total loss，横轴是epoch的数目），即随着epoch的数目越来越多，随着参数不断的update，loss会慢慢的下降最后趋向收敛。但是不幸的是在训练Recurrent Neural Network的时候，你有时候会看到绿色这条线。如果是第一次train Recurrent Neural Network的人，看到绿色这条learning curve非常剧烈的抖动，到某个地方变成NaN，肯定会想这程序有bug啊(╬￣皿￣)。 Razvan分析了下RNN的性质，发现RNN的error surface的变化是非常陡峭的/崎岖的(error surface有一些地方非常的平坦，一些地方非常的陡峭，就像是悬崖峭壁一样)。这样会造成什么样的问题呢？假设橙色的点当做初始点，用Gradient Descent开始调整你的参数，updata参数时，可能会刚好跳过一个悬崖，这时loss会突然爆长，loss会非常上下剧烈的震荡)。更惨的情况是，如果在悬崖上之前的gradient会很小，那learning rate可能已经调的很大的，当下一步update时正好踩过悬崖，gradient爆长，很大的gradient乘上很大的learning rate结果参数就会update很多，参数就“飞”出去了。 作者是用工程的思想来解决了这个问题，这一招就是Clipping（当Gradient大于某一个threshold的时候，不要让它超过那个threshold，比如设定threshold为15，那当gradient大于15时，让gradient等于15。因为gradient不会太大，所以你要做Clipping的时候，就算是“踩着这个悬崖上”，也不飞出来，而是会updaye到一个比较近的地方，这样还可以继续做RNN的training。 为什么RNN会有这种奇特的特性呢。有人觉得是来自sigmoid function，我们之前讲Relu activation function的时候，讲过一个问题gradient vanish，这个问题是从sigmoid function来的，RNN会有很平滑的error surface是因为来自于gradient vanish，但其实问题并不是出在sigmoid function上，把RNN中的sigmoid function换成Relu， performance通常是比较差的，所以在train RNN时很少用Relu做activation function。 你应该会从BRTT的式子中更直观的看出为什么会有这个问题。当然也有更直观的方法来知道一个gradient的大小：把某一个参数做小小的变化，看它对network output的变化有多大，就可以估算出这个参数的gradient的大小。 举一个很简单的RNN例子，它只有一个neuron，这个neuron是linear。input没有bias，input的weight是1，output的weight也是1，transition的weight是w。也就是说从memory接到neuron的input的weight是w。 假设给neural network的input sequence是$1,0,0,0,\\dots$，共1000个数，很容易计算出最后的output $y^{1000}$是$w^{999}$。现在假设$w$是要learn的参数，我们想要知道它的gradient，即当我们改变$w$的值时候，对neural的output有多大的影响。 现在假设$w=1$，那现在$y^{1000}=1$；当$w=1.01$时，$y^{1000}\\approx 20000$，可见当$w \\in (1, 1+\\epsilon)$时，$w$有一点小小的变化，就会对output产生很大的影响，所以这时$w$有很大的gradient，那这时我们把learning rate设小一点就好了。 但当$w=0.99$时，那$y^{1000}\\approx0$；把$w$做一个较大的变化，设$w=0.01$，则有$y^{1000}\\approx0$。可见当$w\\in(0,1-\\epsilon)$时，gradient就突然变得非常非常的小，这个时候我们就需要一个很大的learning rate。这种gradient的突变，使得error surface很崎岖，设置learning rate很麻烦。 从这个例子可以看出RNN training的问题其实来自它把同样的东西在transition的时候反复使用。$w$变化时，它完全可能没有造成任何影响，而一旦造成影响，影响都是“天崩地裂”的。所以RNN不好训练的原因是来自于它有high sequence，同样的weight在不同的时间点被反复的使用，而不是来自activation function。 4.3 Helpful Techniques for RNN有什么技巧可以帮助我们解决这个问题呢？其实广泛被使用的技巧就是LSTM，LSTM可以让你的error surface不要那么崎岖。它可以做到的事情是，它会把那些平坦的地方拿掉，解决gradient vanish的问题，但不能解决gradient explode的问题。有些地方还是非常的崎岖的，有些地方仍然是变化非常剧烈的，但是不会有特别平坦的地方。做LSTM时，error surface大部分地方变化的很剧烈，所以你可以放心的把learning rate设置的小一点，保证在learning rate很小的情况下进行训练。 那为什么LSTM 可以解决梯度消失的问题，避免gradient特别小呢？RNN跟LSTM在面对memory的时候，它处理的操作其实是不一样的。在RNN里面，在每一个时间点，memory里面的值都是会被洗掉，neuron的output都要memory里面去，所以在每一个时间点，memory里面的值都是会被覆盖掉。但是在LSTM里面不一样，它是把原来memory里面的值乘上一个值再把input的值加起来放到cell里面。所以它的memory input是相加的。 所以LSTM和RNN不同的是，RNN在每个时间点的值都会被format掉，所以只要这个影响被format掉它就消失了。但是在LSTM里面，一旦对memory造成影响，那影响一直会被留着，memory一旦有改变，只会把新的东西加进来，不会把原来的值洗掉，除非forget gate要把memory的值洗掉，所以它不会有gradient vanishing的问题。 那你可能会想现在有forget gate，仍然会把memory的值洗掉。其实LSTM的第一个版本就是为了解决gradient vanishing的问题，所以它是没有forget gate，后来的版本中才加入了forget gate。甚至现在有个说法是：在训练LSTM的时候要给forget gate特别大的bias，你要确保forget gate在多数的情况下都是开启的（即不会洗掉memory），只要少数的情况是关闭的（洗掉memory）。 还有另外一个版本用gate操控memory cell，叫做Gates Recurrent Unit(GRU)，LSTM有三个Gate，而GRU有两个gate，所以GRU需要的参数是比较少的。因为它需要的参数量比较少，它在training时是比较鲁棒的。如果在train LSTM时，你觉得overfitting的情况很严重，不妨可以试下GRU。GRU的思想就是：“旧的不去，新的不来”。它会把input gate跟forget gate联动起来，当input gate打开的时候，forget gate会自动的关闭(format存在memory里面的值)，当forget gate没有要format里面的值（即forget gate开启），input gate就会被关起来，也就是说你要把memory里面的值清掉，才能把新的值放进来。 还有其他的technique可以handle gradient vanishing的问题。比如说clockwise RNN或者说是Structurally Constrained Recurrent Network (SCRN)等等。有一个蛮有趣的paper是这样的：一般的RNN用identity matrix（单位矩阵）来initialized transformation weight+ReLU activaton function它可以得到很好的performance。刚才不是说用ReLU的performance会比较拉胯，一般train的方法initiaed weight是random，那ReLU跟sigmoid function来比的话，sigmoid performance 会比较好。但是用了identity matrix来initialized的话，用ReLU performance会比较好。 5. More ApplicationsRNN有很多的application，前面举得那个solt filling的例子中，我们假设input跟output的数目是一样的，我们给input中的每一个word一个slot label。其实RNN可以做到更复杂的事情。 （下面老师举的例子基本都是NLP领域的，我还了解甚少，所以只根据老师上课讲的内容作一些简单的介绍，可能会有些纰漏。而且老师2020年的课程内容跳跃性挺大的，需要结合2017年的ML课程一起看，RNN这节课中有些知识点是在之前的2017年课程里讲到的，我需要看过之后再来对这篇笔记做一个补充更新） 5.1. Many to oneRNN可以做到更复杂的事情，比如说input是一个sequence，output是一个vector，这种Many to one 的常见的应用有sentiment analysis（情感分析）。sentiment analysis现在有很广泛的应用，比如某家公司想要知道，他们的产品在网上的评价是positive 还是negative。他们可能会写一个爬虫，把跟他们产品有关的文章都爬下来。那这一篇一篇的看太累了，所以你可以用一个machine learning 的方法learn一个classifier去分类哪些document的评价是正向的，哪些document是负向的。那你就可以去train一个Recurrent Neural Network，input是character sequence，然后RNN把这个sequence读过一遍，在最后一个时间点，把hidden layer拿出来，在通过几个transform，然后你就可以得到最后的sentiment analysis（这是一个分类的问题，但是因为input是sequence，所以用RNN来处理）。 还可以用RNN来作key term extraction（关键词抽取）。key term extraction意思是给machine看一个文章，machine要预测出这篇文章有哪些关键词。如果你能够收集到一些training data(一些document，这些document都有label，哪些词汇是关键词，那就可以直接train一个RNN)，RNN把document当做input，通过Embedding layer，然后用RNN把这个document读过一次，然后把出现在最后一个时间点的output拿过来做attention，你可以把这样的information抽出来再丢到feedforward neural network得到最后的output。 5.2. Many to Many5.2.1. Output is shorter - Speech RecognitionRNN的输入输出也可以是多对多的，比如input和output都是sequence，但是output sequence比input sequence短的时候，RNN可以处理这个问题。Speech Recognition（语音辨识）就是这样input sequence长，output sequence短的问题。比如在语音辨识这个任务里面input是acoustic sequence(说一句话，这句话就是一段声音讯号)。一般处理声音讯号的方式是，在这个声音讯号里面，每隔一小段时间，就把它用vector来表示。这个一小段时间是很短的(比如说，0.01秒)，那output sequence是character sequence。 如果你是原来的simple RNN(slot filling的那个RNN)，把这一串input丢进去，它只能做到告诉你每一个vector对应到哪一个character。比如对于一个中文的语音辨识系统，那你的output target理论上就是这个世界上所有可能中文的词汇，常用的可能是八千个，那你RNN classifier的数目可能就是八千个。虽然很大，但也是有办法做的。但是充其量只能做到：每一个声音的vector会得到一个character。每一个input对应的时间间隔是很小的(0.01秒)，所以通常是好多个vector对应到同一个character，那对一句内容是“好棒”的语音，辨识结果可能为“好好好棒棒棒棒棒”，那这显然不是语音辨识正确的结果。为了解决这个问题有一招叫做“Trimming”(修剪，切除的意思，这里指把重复的东西拿掉)，就变成“好棒”。但这样也会有一个较严重的问题，因为它没有辨识一些有含义的叠词，比如“好棒棒”（老师说“好棒棒”是贬义的，好机车哦）。 那要怎么把“好棒”跟“好棒棒”分开来呢，有一招叫做Connectionist temporal classification（CTC），主要是解决神经网络label 和output 不对齐的问题（Alignment problem）。它的思想可以这样理解：在output时候不只是output所有中文的character，还会output一个符号，叫做”null””(没有任何东西)。所以我今天input一段acoustic feature sequence,它的output是“好 null null 棒 null null null null”，然后我就把“null”的部分拿掉，它就变成“好棒”。如果我们输入另外一个sequence，它的output是“好 null null 棒 null 棒 null null”，然后把“null”拿掉，所以它的output就是“好棒棒”。这样就可以解决叠字的问题了。 CTC在做training的时候，你的train data就会告诉这一串acoustic features对应到这一串character sequence，但它不会告诉你说“好”是对应第几个character 到第几个character。这时可以穷举所有可能的alignments。简单来说就是，我们不知道“好”对应到那几个character，“棒”对应到哪几个character。假设我们所有的状况都是可能的。可能第一个是“好 null 棒 null null null”，可能是“好 null null 棒 null null”，也可能是“好 null null null 棒 null”。我们不知道哪个是对的，那假设全部都是对的。在training的时候，全部都当做是正确的，然后一起train。当然也有更巧妙的算法可以解决这个问题，这里就不细讲了。 在做英文辨识的时候，你的RNN output target就是character(英文的字母+空白)。直接output字母，然后如果字和字之间有boundary，就自动有空白。看下面的例子，第一个frame是output h，第二个frame是output null，第三个frame是output null，第四个frame是output I等等。如果你看到output是这样子话，那最后把“null”的地方拿掉，那这句话的辨识结果就是“HIS FRIEND’S”。你不需要告诉machine说：”HIS”是一个词汇，“FRIEND’s”是一个词汇,machine通过training data会自己学到这件事情。 据说Google的语音辨识系统已经全面换成CTC。如果你用CTC来做语音辨识的话，就算是有某一个词汇(比如是：英文中人名，地名)在training data中从来没有出现过，machine也是有机会把它辨识出来。 5.2.2. No limitation - Sequence to sequence learningRNN还有一个应用叫做sequence to sequence learning，在sequence to sequence learning里面，RNN的input跟output都是sequence(但是两者的长度是不一样的)。刚在在CTC时，input比较长，output比较短。在这边我们要考虑的是不确定input跟output谁比较长谁比较短。 比如做machine translation（机器翻译），input英文word sequence把它翻译成中文的character sequence。那我们并不知道说，英文跟中文谁比较长谁比较短(有可能是output比较长，output比较短)。 假如现在input一个 “machine learning” ，然后用RNN读过去，然后在最后一个时间点，这个memory里面就存了所有input sequence的information。接下来你让machine “吐”一个character(机)，然后就让它output下一个character，把之前的output出来的character当做input，再把memory里面的值读进来，它就会output “器”。那这个“机”怎么接到这个地方呢，有很多支支节节的技巧，还有很多不同的变形。在下一个时间input “器”，output“学”，然后output“习”，然后一直output下去，machine没办法知道它何时要停下来。 要怎么告诉machine何时停止呢？你可以多加一个symbol “===”，代表停止，那现在manchine不只是output可能的character，它还有一个可能的output 是“===”。这样我们的utput sequence中“习”后面是“===”(断)的话，就停下来了。那这中方法也是可以train的起来的。 还有篇papre是这样做的，对于sequence to sequence learning，我们原来是input 某种语言的文字，output是翻译成另外一种语言的文字(假设是做机器翻译的话)。那我们有没有可能直接input某种语言的声音讯号，output另外一种语言的文字呢？我们完全不做语音辨识。比如说你要把英文句子的语音翻译成中文句子的文本，你就可以收集一大堆英文句子的语音，以及它对应的中文翻译，完全不要做语音辨识，直接把英文的声音讯号丢到这个model里面去，看它能不能output正确的中文。这一招居然是行得通的。假设你今天要把台语转成英文，但是台语的语音辨识系统不好做，因为台语根本就没有standard文字系统，所以这项技术可以成功的话，未来你在训练台语转英文语音辨识系统的时候，你只需要收集台语的声音讯号跟它的英文翻译就可以刻了。你就不需要台语语音辨识的结果，你也不需要知道台语的文字，也可以做这件事。 Beyond Sequence 利用sequence to sequence的技术，甚至可以做到Beyond Sequence。这个技术也被用到syntactic parsing（句法分析）。synthetic parsing是指，让machine看一个句子，它要得到这个句子的结构树，得到一个树状的结构。过去你可能要用structured learning的技术能够解这个问题，但是现在有了 sequence to sequence learning的技术以后，只要把这个树状图描述成一个sequence（具体看图中 john has a dog）。你就可以直接learn 一个sequence to sequence model，它的output就是syntactic parsing tree。这个是可以train的起来的，performance也很好。你可能会担心machine的output的sequence不符合文法结构，比如它记得加左括号，却忘记加右括号，神奇的地方就是LSTM不会忘记右括号。 Sequence - to - sequence Auto - encoder - Text 当我们要将一个document表示成一个vector时，往往会用bag-of-word的方法，用这个方法的时候，往往会忽略掉 word order information。举例来说，有一个word sequence是“white blood cells destroying an infection”，另外一个word sequence是：“an infection destroying white blood cells”，这两句话的意思完全是相反的。但是用bag-of-word的方法来描述的话，它们的bag-of-word完全是一样的。虽然这两个word sequence里面有完全一摸一样的六个词汇，但因为词汇的order是不一样的，所以他们的意思一个变成positive，一个变成negative，它们的意思是很不一样的。那现在我们可以用sequence to sequence Auto-encoder这种做法来考虑word sequence order的情况下，把一个document变成一个vector。 我们可以input一个word sequence，通过Recurrent Neural Network变成一个invalid vector，然后把这个invalid vector当做decoder的输入，然后让这个decoder，找回一模一样的句子。如果今天Recurrent Neural Network可以做到这件事情的话，那Encoding这个vector就代表这个input sequence里面重要的information。在trian Sequence-to-sequence Auto-encoder的时候，不需要label data，你只需要收集大量的文章，然后直接train下去就好了。 Sequence-to-sequence 还有另外一个版本叫skip thought，如果用Sequence-to-sequence的，输入输出都是同一个句子，如果用skip thought的话，输出的目标就会是下一个句子，用sequence-to-sequence得到的结果通常容易表达，如果要得到语义的意思的，那么skip thought会得到比较好的结果。这个结构甚至可以是hierarchical，每一个句子都先得到一个vector（Mary was hungry得到一个vector，she didn’t find any food得到一个vector），然后把这些vector加起来，然后变成一个整个 document high label vector，在让这整个vector去产生一串sentence vector，在根据每一个sentence vector再去解回word sequence。这是一个四层的LSTM(从word 变成sentence sequence ，变成document lable，再解回sentence sequence，再解回word sequence。 Sequence - to - sequence Auto - encoder - Speech 这种Sequence to sequence Auto encoder的方法也可以用到语音上，在语音上，它可以把一段audio segment变成一个fixed length vector。比如说，左边有一段声音讯号，长长短短都不一样，那你把他们变成vector的话，可能dog跟dogs比较接近，never和ever比较接近，可以称之为audio auto vector。一般的auto vector它是把word变成vector，这里是把一段声音讯号变成一个vector。 这个东西有什么用呢？它可以做很多的事。比如可以拿来做语音的搜寻，假设你有一个声音的data base(比如说，上课的录音，你想要找跟美国白宫有关的东西，然后你说一句话-“美国白宫”，machine不需要做语音辨识，直接比对声音讯号的相似度，machine就可以从data base里面把提到“美国白宫”的部分找出来)。它的实现过程可以是这样的：你先把一个audio data base做segmentation，切成一段一段的。然后每一个段用刚才讲的audio segment to vector这个技术，通通变成vector。然后现再输入一个spoken query，可以通过audio segment to vector技术也变成vector，接下来计算它们的相似程度，找出最相似的就是要搜寻的结果。 那么如何把一个audio segment变成一个vector呢？把audio segment抽成acoustic features，然后把它输入到RNN里面去，这个RNN的角色就是Encoder，它读过acoustic features之后，最后一个时间点它存在memory里面的值就代表了input声音讯号的information。它存到memory里面的值是一个vector，这个其实就是我们要拿来表示整段声音讯号的vector。 但是只要RNN Encoder没有办法去train，同时你还要train一个RNN Decoder。Decoder的作用是把Encoder得到的值存到memory里面的值，拿进来当做input，然后产生一个acoustic features sequence。然后希望这个$y_1$跟$x_1$越接近越好。然后再根据$y_1$产生$y_2$，以此类推。今天训练的target$y_1,y_2,y_3,y_4$跟$x_1,x_2,x_3,x_4$越接近越好。在训练的时候，RNN Encoder跟RNN Decoder是一起train的。 我们在实验上得到一些有趣的结果，下图中的每个点其实都是一段声音讯号，你把声音讯号用刚才讲的 Sequence-to-sequence Auto-encoder技术变成平面上一个vector。发现说：fear这个位置在左上角，near的位置在右下角，他们中间这样的关系(fame在左上角，name在右下角)。发现把fear的开头f换成n，跟fame的开头f换成n，它们的word vector的变化方向是一样的。现在这个技术还没有把语义加进去。 我们可以用Sequence-to-sequence Auto-encoder来做很多有意思的工作，比如训练一个chat-bot(聊天机器人)。你可以收集很多的对话，比如说电影的台词，在电影中有一个台词是“How are you”，另外一个人接“I am fine”。那就告诉machine说这个sequence to sequence learning当它input是“How are you”的时候，这个model的output就要是“I am fine”。你可以收集到这种data，然后就让machine去 train。这里我们就收集了四万句和美国总统辩论的句子，然后让machine去学这个sequence to sequence这个model。 5.3. Attention - based Model现在除了RNN以外，还有另外一种有用到memory的network，叫做Attention-based Model（基于注意力的模型），这个可以想成是RNN的进阶的版本。我们知道人的大脑有非常强的记忆力，所以你可以记得非常非常多的东西。比如说，你现在同时记得早餐吃了什么，同时记得10年前夏天发生的事，同时记得在这几门课中学到的东西。那当然有人问你说什么是deep learning的时候，那你的脑中会去提取重要的information，然后再把这些information组织起来，产生答案。但是你的脑中会自动忽略那些无关的事情，比如说，10年前夏天发生的事情等等。 其实machine也可以做到类似的事情，machine也可以有很大的记忆的容量。它可以有很大的data base，在这个data base里面，每一个vector就代表了某种information被存在machine的记忆里面。当你输入一个input的时候，这个input会被丢进一个中央处理器，这个中央处理器可能是一个DNN/RNN，那这个中央处理器会操控一个Reading Head Controller，这个Reading Head Controller会去决定这个reading head放的位置。machine再从这个reading head 的位置去读取information，然后产生最后的output。 这个model还有一个2.0的版本，它会去操控writing head controller。这个writing head controller会去决定writing head 放的位置。然后machine会去把它的information通过这个writing head写进它的data base里面。所以，它不仅有读的功能，还可以discover出来的东西写入它的memory里面去。这个就是大名鼎鼎的Neural Turing Machine（神经图灵机）。 Reading Comprehension Attention-based Model 常常用在Reading Comprehension（阅读理解）里面。所谓Reading Comprehension就是让machine读一堆document，然后把这些document里面的内容(每一句话)变成一个vector。每一个vector就代表了每一句话的语义。比如你现在想问machine一个问题，然后这个问题被丢进中央处理器里面，那这个中央处理器去控制一个reading head controller，去决定现在在这个data base里面哪些句子是跟中央处理器有关的。假设machine发现这个句子是跟现在的问题是有关的，就把reading head放到这个地方，把information 读到中央处理器中。读取information这个过程可以是重复数次,也就是说machine并不会从一个地方读取information，它先从这里读取information以后，它还可以换一个位置读取information。它把所有读到的information收集起来，最后给你一个最终的答案。 下图是在baby corpus上的结果，baby corpus是一个Q&amp;A的一个简单的测试。我们需要做的事就是读过这五个句子，然后说：what color is Grey?，得到正确的答案。那你可以从machine attention的位置(也就是reading head 的位置)看出machine的思路。图中蓝色代表了machine reading head 的位置，Hop1，Hop2，Hop3代表的是时间，在第一个时间点，machine先把它的reading head放在“greg is a frog”，把这个information提取出来。接下来提取“brian is a frog” information ，再提取“brian is yellow”information。最后它得到结论说：greg 的颜色是yellow。这些事情是machine自动learn出来的，machine attention在哪个位置，这些通过neural network学到该怎么做，要先看哪个句子，再看哪个句子，这些都是machine自动去决定的。 Visual Question Answering Attention-Based model也可以做Visual Question Answering（VQA），比如让machine看下面的那张图片，问它这是什么，要它可以正确回答说是香蕉。 这个Visual Question Answering可以这样来实现：先让machine看一张图，然后通过CNN你可以把这张图的一小块region用一小块的vector来表示。接下来，输入一个query，这个query被丢到中央处理器中，这个中央处理器去操控reading head controller，这个reading head controller决定读取的位置（是跟现在输入的问题是有关系的），这个读取的process可能要好几个步骤，machine会分好几次把information读到中央处理器，最后得到答案。 Speech Question Answering Attention-Based model也可以做Speech Question Answering 。比如说：在语音处理实验上我们让machine做TOEFL Listening Comprehension Test 。让machine听一段声音，然后问它问题，从四个选项里面，machine选择出正确的选项，那machine做的事情其实是跟人类考生做的是一样的。 那用的Model Architecture跟我们上面讲到的其实大同小异，你让machine先读一个question，然后把question做语义的分析得到question的语义，声音的部分是让语音辨识先转成文字，在把这些文字做语义的分析，得到这段文字的语义。那machine了解question的语义然后就可以做attention，决定在audio story里面哪些部分是回答问题有关的。这就像画重点一样，machine画的重点就是答案，它也可以回头去修正它产生的答案。经过几个process以后，machine最后得到的答案跟其他几个选项计算相似度，然后看哪一个想项的相似度最高，它就选那一个选项。那整个test就是一个大的neural network，语音辨识，question semantic部分和audio semantic部分都是neural network，它们都是可以训练的。 下图是五种naive的方法得到的结果方法，也就是完全不管文章的内容，直接看问题跟选项就猜答案。random 的正确率是25 percent，有两个方法要比25%要好的：每次都选最短的那个选项就会得到35%的正确率；如果分析四个选项的semantic，用sequence-to-sequence autoencoder，去把一个选项的semantic找出来，然后再去看某个选项和另外三个选项的相似度，每次都选择相似度最高的那个选项，这样做正确率有36%左右。 另外还可以用memory network的方法，可以得到39.2 %正确率；如果用我们上面讲到的那个model的话，可以做到48.8%正确率。 （后面还有一部分内容是比较RNN和Structured learning，这部分等我学习了Structured learning后再来补充。）","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://nekomoon404.github.io/categories/Deep-Learning/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://nekomoon404.github.io/tags/RNN/"},{"name":"Bidirectional RNN","slug":"Bidirectional-RNN","permalink":"http://nekomoon404.github.io/tags/Bidirectional-RNN/"},{"name":"LSTM","slug":"LSTM","permalink":"http://nekomoon404.github.io/tags/LSTM/"},{"name":"BPTT","slug":"BPTT","permalink":"http://nekomoon404.github.io/tags/BPTT/"},{"name":"Sequence to sequence learning","slug":"Sequence-to-sequence-learning","permalink":"http://nekomoon404.github.io/tags/Sequence-to-sequence-learning/"},{"name":"Attention-based model","slug":"Attention-based-model","permalink":"http://nekomoon404.github.io/tags/Attention-based-model/"}]},{"title":"DL笔记（8）Optimization for Deep Learning","slug":"ML笔记（8）Optimization-for-Deep-Learning","date":"2020-07-12T01:57:18.000Z","updated":"2020-07-12T11:57:18.000Z","comments":true,"path":"2020/07/12/ML笔记（8）Optimization-for-Deep-Learning/","link":"","permalink":"http://nekomoon404.github.io/2020/07/12/ML%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89Optimization-for-Deep-Learning/","excerpt":"","text":"Background Konwledge: $\\mu$-strong convexity Lipschitz continuity Bregman proximal inequality 这一节课会比较概念上的讲解Optimization for Deep Learning这个主题，不会去详细证明每个算法在某些情况能达到convergence。 明确几个Notations： $\\theta_t$：model parameters at time step $t$ $\\nabla L(\\theta_t) $ or $g_t$：gradient at $\\theta_t$, used to compute $\\theta_{t+1}$ $m_{t+1}$：momentum accumulated form time step 0 to time step t, which is used to compute $\\theta_{t+1}$（可以理解为momentum记录了time step 0到t的梯度信息） 我们先要考虑对一个神经网络做Optimization是要做什么，就是要找到一组参数$\\theta$使得training data中所有的x算出来的loss之和最小，即让神经网络越贴近训练资料越好，把一个$x_t$输入到神经网络中得到的$y_t$越接近$\\hat y_t$越好；或者如果用特定的$x$，那我们就是要找在loss surface上的最小值。 On-line vs Off-line On-line: one pair of $(x_t, \\hat y_t)$ at a time step Off-line: pour all $(x_t,\\hat y_t)$ into the model at every time step 显然Off-line learning的训练方式会更好，我们一次可以看到所有的training data，但由于电脑硬件的限制等因素Off-line实际实施起来却有困难。但这节课中我们先暂且抛开这些因素，只关注Off-line的cases。 回顾之前讲过的几种优化算法（具体可以看之前的笔记）： SGD（Stochastic Gradient Descent，随机梯度下降） SGD with momentum Adagrad RMSProp Adam 来看一下提出这些方法的时间： Optimizers: Real Application 大家听过的很多著名的深度学习模型，比如NLP领域的BRET（Bidirectional Encoder Representation from Transformers，是一个预训练的语言表征模型），Transformer（sequence to sequence模型），Tacotron（端到端语言合成模型）；Big-GAN（图像生成模型）；MAML（Model-Agnostic Meta-Learning）都是用ADAM训练出来的。又如CV领域的YOLO（目标检测），Mask R-CNN（对象实例分割框架），ResNet（深度残差网络，用于图片分类）则是用到SGDM 。 那这些很知名的model都是用这些14年以前提出的optimizer来训练的呢，可以说是因为ADAM和SGDM已经做得非常好了，后面提出的optimizer都是去填补它们俩中间的空白，并没有很明显的超越，如下面几张图中的例子。 Adam和SGDM是有一定的差别的： Adam的generalization能力比较差，在testing data上的落差比较大；而SGDM的generalization能力比较好，在testing data上的落差比较小。 先来直观地理解一下generalization gap，上图中的黑色实线是Training loss function，红色虚线是Testing function，因为Training data和Testing data的分布有差异，所以它们的loss function可能会有点像，但也有差距。有两个值相同的Minimum，如果找到一个比较平坦的Minimum，那两个function在这里都比较平坦，差距会比较小；如果找到一个比较sharp的Minimum，两个function在这里都比价陡，差距会比较大。那造成Adam和SGDM在generalization上的差异的一个可能的原因就是Adam比较会找到Sharp Minimum，而SGDM比较会找到Flat Minimum，当然也还有其他的原因。 既然Adam比较快，SGDM比较稳，而且可以收敛到比较小的值，那就有人提出将这两种方法结合—SWATS。训练的前半部分用Adam，后半部分用SGDM，但论文中Adam和SGDM切换的点的设置并不是很科学。 那我们有没有办法来修正Adam，让Adam像SGDM一样收敛的又稳又好呢。先来考虑Adam存在什么问题， 首先我们假设公式中的$\\beta_1=0$，只关注adaptive learning rate，即分母$v_t$那一项的影响。设$\\beta_2=0.999$，那$v_t$会受到gradient的影响大概就有$1/(1-0.999)=1000$个step那么久。论文[Reddi, et al., ICLR’18里写到在training的最后阶段，大部分的gradient都会很小，且对descent的方向不能提供什么有用的信息。只有某几个minibatch的gradient会很大，会较明确地告诉参数应该往哪个方向update。那基于这一事实，在做Adam的时候会出现问题呢。 假设已经training了10W个step，10W之后的几个step的gradient都很小，比如说都是1；直到过了1000个step后突然出现一个很大的gradient，如100000；下一个step，gradient会变得很小，比如1。 考虑第100000个step，由于我们假设$\\beta_1=0$，那$m_t$就等于gradient的大小；假设前面在这之前的step的gradient有很多1，这时分母$\\sqrt{\\hat v_t}+\\varepsilon$也是1 ，那这一步的movement就是$\\eta$；之后几个step也是如此。当gradient为100000的step，$m_t=g_t=10^5$，$\\sqrt{\\hat v_t}+\\varepsilon=\\sqrt{0.001\\cdot(10^5)^2}=10^{3.5}$，那movement就是$10\\sqrt{10}\\eta$。到下一个step，gradient=1，movement=$10^{-3.5}\\eta$。那这会发生什么问题呢？ 可以看到前面这1000个step它们提供的movement是$1000\\eta$，而之后这个gradient=10W的真正有意义的step，它只会移动$10\\sqrt{10}\\eta \\approx 30\\eta$，可见真正提供比较多信息的gradient它在update的时候只能造成很小的影响。当前面gradient比较小的时候，network可能不知道后面会出现有很大的gradient，就以为现在的gradient已经不错了，结果造成了不好的影响。并且movement的大小是有上限的，为$\\sqrt{\\frac{1}{1-\\beta_2}}\\eta$。所以当大部分的gradient都很小，真正有意义的gradient的作用就会受影响。 作者提处的方法叫AMSGrad，目标就是减小这些无意义的gradient的影响，做法是令$\\hat v_t=\\max(\\hat v_{t-1},v_t)$。上面的例子来说明就是，假设在10W步之前出现过很大的gradient，但走到10W步这里已经忘了，给很小的gradient的step也走的比较大，那后面遇到很大的gradient反而会走的很小。作者的思路就是通过max operation来记住过去最大的$v_t$。 记得之前我们有讲过Adagrad的表达式，$w^{t+1}=w^t-\\frac{\\eta}{\\sqrt{\\sum\\limits_{i=0}^t(g^i)^2}}\\cdot g^t$，它会因为分母一直变大，导致learning rate一直减小，这是要解决的问题。那到AMSGrad这里反而后绕了回来，让分母变大，那可能AMSGrad并不是一个很好的解法。 下面来看另外一篇论文[Luo, et al., ICLR’19]提供的解决思路。 AMSGrad只考虑了解决learning rate很大的情况，这篇论文的作者的方法AdaBound，也考虑在gradient很大的时候，让learning rate不会很小。他将$\\frac{\\eta}{\\sqrt{\\hat v_t}+\\varepsilon}$做了一个clip（将这一项的值限制在一个最小值和最大值之间），但这个有点经验公式的意味，且这个最小值和最大值和loss并没有什么关系，不太符合我们做Adaptive learning rate的期望。 那我们现在换个角度，来看一看对SGDM可以做哪些调整。SGDM的优点是稳定，收敛得好，但它运算得很慢，造成这的原因是它每次update的时候用的是固定的learning rate，而不想Adaptive leaning rates的算法那样可以动态地调整learning rates的大小。 然而SGDM是无法做到learning rate的，那可不可以帮SGDM找一个最佳的learning rate呢，这个learning rate收敛得比较快，让SGDM的结果比较接近Adam。 我们在调learning rate的时候常会有这样的经验，当learning rate最小或最大的时候，结果都不会太好；一定是当learning rate适中的时候会得到较好的performance。 在论文[Smith, WACV’17]里面，作者提出了Cyclical LR，让learning rate在大小大小地周期性变化。learning rate大的时候就相当于做一个forward的动作，learning rate小的时候就相当于做一个fine tune或者是收敛的动作。通常我们在做的时候，learning rate会不断变小，这里让Learning rate变大其实是鼓励这个model“不要满足现状”，在适当的时候做更多尝试。这里有一些参数要决定：step size，以及最大和最小的learning rate。 下面是一个很类似的做法SGDR[Loshchilov,et al., ICLR‘17]，只是调整learning rate的方法不一样。 另一个方法One-cycle LR的思想是只做一个cycle，分为三个阶段：warm up—learning rate逐渐增大，直到找到一个还不错的minima附近；annealing—learning rate逐渐减小；fine-tuning—收敛。 那你可能会想Adam需要这样的三个过程嘛，比如Adam需要warm up嘛，有人也做了这方面的研究","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://nekomoon404.github.io/categories/Deep-Learning/"}],"tags":[{"name":"Adam","slug":"Adam","permalink":"http://nekomoon404.github.io/tags/Adam/"},{"name":"AMSGrad","slug":"AMSGrad","permalink":"http://nekomoon404.github.io/tags/AMSGrad/"},{"name":"AdaBound","slug":"AdaBound","permalink":"http://nekomoon404.github.io/tags/AdaBound/"},{"name":"SGDM","slug":"SGDM","permalink":"http://nekomoon404.github.io/tags/SGDM/"},{"name":"Cyclical LR","slug":"Cyclical-LR","permalink":"http://nekomoon404.github.io/tags/Cyclical-LR/"}]},{"title":"DL笔记（7）Tips for Deep Learning","slug":"ML笔记（7）Tips-for-Deep-Learning","date":"2020-07-11T08:55:52.000Z","updated":"2020-07-11T10:55:52.000Z","comments":true,"path":"2020/07/11/ML笔记（7）Tips-for-Deep-Learning/","link":"","permalink":"http://nekomoon404.github.io/2020/07/11/ML%E7%AC%94%E8%AE%B0%EF%BC%887%EF%BC%89Tips-for-Deep-Learning/","excerpt":"","text":"1. Recipe of Deep Learning回顾之前讲过的Deep Learning的三个步骤： define the function set(network structure) goodness of function(loss function — cross entropy) pick the best function(gradient descent — optimization) 做完这些步骤以后会得到一个更好的neural network，那接下来我们要做什么事情呢？ Good Results on Training Data？ 要做的第一件事是，提高model在training set上的正确率。 先检查training set的performance其实是deep learning一个非常unique的地方，如果使用的是k-nearest neighbor或decision tree这类非deep learning的方法，做完以后其实不用去检查training set的结果，因为在training set上的performance正确率就是100。Deep Learning的model里这么多参数，但实际上Deep Learning不容易出现overfitting，overfitting是指在training set上performance很好，但在testing set上performance没有那么好。像k nearest neighbor，decision tree这类方法，它们在training set上正确率都是100，是非常容易overfitting的；而对deep learning来说，overfitting往往不会是你遇到的第一个问题。 因为deep learning并不是像k nearest neighbor这种方法一样，一训练就可以得到非常好的正确率，它有可能在training set上没有办法给一个很好的正确率，所以，这个时候要回头去检查在前面的step里面要做什么样的修改，使得模型在training set上可以得到较高的正确率。 Good Results on Testing Data？ 接下来要做的事是，提高model在testing set上的正确率。 假设现在已经在training set上得到好的performance了，那接下来就把model apply到testing set上，我们最后真正关心的，是testing set上的performance。如果得到的结果不好，这时就是Overfitting了，也就是在training set上得到好的结果，却在testing set上得到不好的结果。 那你要回过头试着解决overfitting，但有时候你加了新的technique，想要overcome overfitting这个problem的时候，其实反而会让training set上的结果变坏；所以你在做完这一步的修改以后，要先回头去检查新的model在training set上的结果，如果这个结果变坏的话，你就要从头对network training的process做一些调整，那如果你同时在training set还有testing set上都得到好结果的话，你就成功了，最后就可以把你的系统真正用在application上面了。 Do not always blame overfitting 不要看到不好的performance就归责于overfitting。先看右边testing data的图，横坐标是model做gradient descent所update的次数，纵坐标则是error rate(越低说明model表现得越好)，黄线表示的是20层的neural network，红色表示56层的neural network。发现56层network的error rate比较高，它的performance比较差，而20层network的performance则是比较好的，有些人看到这就会马上得到一个结论：56层的network参数太多了，56层果然没有必要，这个是overfitting。 但是检查一下模型在training set上的performance，如上面左边的图，在training set上20层的network本来就要比56层的network表现得更好，所以testing set得到的结果并不能说明56层的case就是发生了overfitting。 在做neural network training的时候，有太多的问题会让training set表现的不好，比如local minimum的问题，saddle point的问题，有plateau的问题…所以这个56层的neural network有可能在train的时候就卡在了一个local minimum的地方，于是得到了一个差的参数，但这并不是overfitting，而是在training的时候就没有train好。有人认为这个问题叫做underfitting，但underfitting的本意应该是指这个model的complexity不足，这个model的参数不够多，所以它的能力不足以解出这个问题；但这个56层的network，它的参数是比20层的network要来得多的，所以它明明有能力比20层的network要做的更好，却没有得到理想的结果，这种情况不应该被称为underfitting，其实就只是没有train好而已。 当你在deep learning的文献上看到某种方法的时候，永远要想一下，这个方法是要解决什么样的问题： 在training set上的performance不够好 在testing set上的performance不够好 当只有一个方法propose(提出)的时候，它往往只针对这两个问题的其中一个来做处理，比如dropout的方法，它是在testing data的结果不好时用，如果你的问题只是training的结果不好，那去apply dropout只会越train越差。所以我们必须要先想清楚现在的问题到底是什么，然后再根据这个问题去找针对性的方法，而不是病急乱投医，甚至是盲目诊断。下面我们分别从Training data和Testing data两个问题出发，来讲述一些针对性优化的方法。 2. Good Results on Training Data？这一章主要介绍如何在Training data上得到更好的performance，分为两个模块，New activation function和Adaptive Learning Rate。 2.1. New activation functionactivation function 如果你今天的training结果不好，很有可能是因为你的network架构设计得不好。可能你用的activation function是对training比较不利的，那就要尝试着换一些新的activation function，也许可以带来比较好的结果。在1980年代，比较常用的activation function是sigmoid function，如果现在我们使用sigmoid function，你会发现network越deeper不一定imply better，下图是在MNIST手写数字识别上的结果，当layer越来越多的时候，accuracy一开始持平，后来就会逐渐降低，在layer是9层、10层的时候，整个结果就崩溃了。但注意9层、10层的情况并不能被认为是因为参数太多而导致overfitting，实际上这张图就只是training set的结果。 2.1.1. Vanishing Gradient Problem上面这个问题的原因不是overfitting，而是Vanishing Gradient(梯度消失)，解释如下：当你把network叠得很深的时候，在靠近input的地方，loss function对这些参数的微分（即梯度）是比较小的；而在比较靠近output的地方，它对loss的微分值会是比较大的。 因此当你设定同样learning rate的时候，靠近input的地方，它参数的update是很慢的；而靠近output的地方，它参数的update是比较快的。所以在靠近input的地方，参数几乎还是random的时候，output就已经根据这些random的结果找到了一个local minima，然后就converge(收敛)了。这时loss下降的速度变得很慢，你就会觉得gradient已经接近于0了，于是把程序停掉了，由于这个converge，是几乎base on random的参数，所以model的参数并没有被训练充分，那在training data上得到的结果肯定是很差的。 如果把Backpropagation的式子写出来，就可以很轻易地发现用sigmoid function会导致这件事情的发生；或者其实从直觉上来想也可以了解这件事情发生的原因：某一个参数$w$对total loss $l$的偏微分，即gradient $\\frac{\\partial l}{\\partial w}$，它的含义可以理解为：当把这个参数做小小的变化的时候，它对loss的影响有多大；那我们就把第一个layer里的某一个参数$w$加上$\\Delta w$，看看它对network的output和target之间的loss有什么样的影响。 $\\Delta w$通过sigmoid function之后，得到output是会变小的，改变某一个参数的weight，会对某个neuron的output值产生影响，但是这个影响是会随着层数的递增而衰减的，sigmoid function的形状如下所示，它会把负无穷大到正无穷大之间的值都硬压到0~1之间，把较大的input压缩成较小的output。 因此即使$\\Delta w$值很大，但每经过一个sigmoid function就会被缩小一次，所以network越深，$\\Delta w$被衰减的次数就越多，直到最后，它对output的影响就是比较小的，相应的也导致input对loss的影响会比较小，于是靠近input的那些weight对loss的gradient $\\frac{\\partial l}{\\partial w}$远小于靠近output的gradient。 那如何解决这个问题呢？比较早年的做法是去train RBM（Restricted Boltzmann Machine，受限玻尔兹曼机），它的思想是：先把第一个layer train好，再去train第二个，然后再第三个…所以最后你在做Backpropagation的时候，尽管第一个layer几乎没有被train到，但一开始在做pre-train的时候就已经把它给train好了，这样RBM就可以在一定程度上解决问题。但其实改一下activation function可能就可以handle这个问题了。 2.1.2. ReLU现在比较常用的activation function叫做Rectified Linear Unit(整流线性单元函数，又称修正线性单元)，它的缩写是ReLU，该函数形状如下图所示，$z$为input，$a$为output，如果input&gt;0则output = input，如果input&lt;0则output = 0。 选择ReLU作为activation function的理由如下： 跟sigmoid function比起来，ReLU的运算快很多； ReLU的想法结合了生物上的观察( Pengel的paper )； 无穷多bias不同的sigmoid function叠加的结果会变成ReLU； ReLU可以处理Vanishing gradient的问题( the most important thing )； handle Vanishing gradient problem 下图是以ReLU作为activation function的neuron组成的network，它的output要么等于0，要么等于input。当output=input的时候，这个activation function就是linear的；而output=0的neuron对整个network是没有任何作用的，因此可以把它们从network中拿掉。 拿掉所有output为0的neuron后如下图所示，此时整个network就变成了一个瘦长的linear network，linear的好处是，output=input，不会像sigmoid function一样使input产生的影响逐层递减。 Q：这里就会有一个问题，我们之所以使用deep learning，就是想要一个non-linear、比较复杂的model，而使用ReLU不就会让它变成一个linear function吗？这样得到的function不是会变得很弱吗？ A：其实，使用ReLU之后的network整体来说还是non-linear的，如果你对input做小小的改变，不改变neuron的operation region的话，那network就是一个linear function；但是，如果你对input做比较大的改变，导致neuron的operation region被改变的话，比如从output=0转变到了output=input，network整体上就变成了non-linear function。（这里的region是指input z0的两个范围） Q：还有另外一个问题，我们对loss function做gradient descent，要求neural network是可以做微分的，但ReLU是一个分段函数，它是不能微分的(至少在z=0这个点是不可微的)，那该怎么办呢？ A：在实际操作上，当region的范围处于z&gt;0时，微分值gradient就是1；当region的范围处于z&lt;0时，微分值gradient就是0；当z为0时，就不要管它，相当于把它从network里面拿掉 ReLU-variant 其实ReLU还存在一定的问题，比如当input&lt;0的时候，output=0，此时微分值gradient也为0，你就没有办法去update参数了，那我们可以让input&lt;0的时候，微分后还能有一定的值，比如令$a=0.01z$，这个函数叫做Leaky ReLU。 那自然可以想到我们也可以把$z$前的系数当作参数来训练，这就是Parametric ReLU，令$a=\\alpha \\cdot z$，其中$\\alpha$并不是固定的值，而是network的一个参数，它可以通过training data学出来，甚至每个neuron都可以有不同的$\\alpha$值。 又有人想，activation function为什么一定要是ReLU这样子呢，于是就有了一个更进阶的想法—Maxout network。 2.1.3. MaxoutMaxout的思想是，让network去学习它的activation function，那Maxout network就可以学出ReLU，也可以学出其他的activation function，这一切都是由training data来决定的。假设现在有input $x_1,x_2$，它们乘上几组不同的weight分别得到5,7,-1,1，这些值本来是不同neuron的input；但在Maxout network里，我们事先决定好将某几个“neuron”的input分为一个group，比如5,7分为一个group，然后在这个group里选取一个最大值7作为output。 这个过程就好像在一个layer上做Max Pooling一样，它和原来的network不同之处在于，它把原来几个“neuron”的input按一定规则组成了一个group，然后并没有使它们通过activation function，而是选取其中的最大值当做这几个“neuron”的output。 当然，实际上原来的”neuron“早就已经不存在了，这几个被合并的“neuron”应当被看做是一个新的neuron（下图中红框框住的部分），这个新的neuron的input是原来几个“neuron”的input组成的vector，output则取input的最大值，而并非由activation function产生。 在实际操作上，几个element被分为一个group是由你自己决定的，它就是network structure里一个需要被调的参数，不一定要跟上图一样两个分为一组。 Maxout -&gt; RELU Maxout是如何能学习出ReLU这个activation function的呢？ 下图左上角是一个ReLU的neuron，它的input $x$会乘上neuron的weight $w$，再加上bias $b$，然后通过activation function-ReLU，得到output $a$。 neuron的input为$z=wx+b$，为下图左下角的紫线； neuron的output为$a=z\\ (z&gt;0);\\ a=0\\ (z&lt;0)$，为下图左下角的绿线。 如果我们使用的是上图右上角所示的Maxout network，假设$z_1$的参数w和b与ReLU的参数一致，而$z_2$的参数w和b全部设为0，然后做Max，选取$z_1,z_2$较大值作为a： neuron的input为$\\begin{bmatrix}z_1 \\ z_2 \\end{bmatrix}$ $z_1=wx+b$，为上图右下角紫线 $z_2=0$，为上图右下角红线 neuron的output为$\\max{\\begin{bmatrix}z_1 \\ z_2 \\end{bmatrix}}$，为上图右下角绿线 你会发现，此时ReLU和Maxout所得到的output是一模一样的，它们是相同的activation function。 Maxout -&gt; More than ReLU 除了ReLU，Maxout还可以实现更多不同的activation function。比如$z_2$的参数w和b不是0，而是$w’,b’$，此时 neuron的input为$\\begin{bmatrix}z_1 \\ z_2 \\end{bmatrix}$ $z_1=wx+b$，为下图右下角紫线 $z_2=w’x+b’$，为下图右下角红线 neuron的output为$\\max{\\begin{bmatrix}z_1 \\ z_2 \\end{bmatrix}}$，为下图右下角绿线 这个时候你得到的activation function的形状(绿线形状)，是由network的参数$w,b,w’,b’$决定的，因此它是一个Learnable Activation Function，具体的形状可以根据training data去generate出来。 property Maxout可以实现任何piecewise linear convex activation function(分段线性凸激活函数)，其中这个activation function被分为多少段，取决于你把多少个element z放到一个group里，下图分别是2个element一组和3个element一组的activation function的不同形状。 How to train Maxout 接下来我们要面对的是，怎么去train一个Maxout network，如何解决Max不能微分的问题。假设在下面的Maxout network中，红框圈起来的部分为每个neuron的output。 其实Max operation就是linear的operation，只是它仅接在前面这个group里的某一个element上，因此我们可以把那些并没有被Max连接到的element拿掉，从而得到一个比较细长的linear network。 那么可以理解为，实际上我们真正训练的并不是一个含有max函数的network，而是一个化简后如下图所示的linear network；当我们还没有真正开始训练模型的时候，此时这个network含有max函数无法微分，但是只要输入data，network就会根据这些data确定具体的形状，此时max函数的问题已经被实际数据给解决了，所以我们完全可以根据training data使用Backpropagation的方法去训练被network留下来的参数。 所以我们担心的max函数无法微分，它只是理论上的问题；在具体的实践上，完全可以先根据data把max函数转化为某个具体的函数，再对这个转化后的thiner linear network进行微分 这个时候你也许会有一个问题，如果按照上面的做法，那岂不是只会train留在network里面的那些参数，剩下的参数该怎么办？那些被拿掉的直线(weight)岂不是永远也train不到了吗？ 其实这也只是个理论上的问题，在实际操作上，每个linear network的structure都是由input的那一笔data来决定的，当input不同data的时候，得到的network structure是不同的，留在network里面的参数也是不同的，由于我们有很多笔training data，所以network的structure在训练中不断地变换，实际上最后每一个weight参数都会被train到。 所以，我们回到Max Pooling的问题上来，由于Max Pooling跟Maxout是一模一样的operation，那么Max Pooling有关max函数的微分问题就可以采用跟Maxout一样的方案来解决，至此我们已经解决了CNN部分的第一个问题—New activation function。 2.2. Adaptive learning rate2.2.1. Review - Adagrad回顾之前学过的梯度下降中的一个Tip: Adagrad，它让每一个parameter都要有不同的learning rate。Adagrad的思想是，假设我们考虑两个参数$w_1,w_2$，如果在$w_1$这个方向上，gradient都比较小，那它是比较平坦的，于是就给它比较大的learning rate；而在$w_2$这个方向上，gradient都比较大，那它是比较陡峭的，于是给它比较小的learning rate。 但我们实际面对的问题，很有可能远比Adagrad所能解决的问题要来的复杂，回顾之前学的Linear Regression，我们做optimization的对象，也就是loss function，它是convex的；但实际上我们在做deep learning的时候，这个loss function可以是任何形状。 2.2.2. RMSProplearning rate Deep learning的loss function可以是任何形状，对convex loss function来说，在每个方向上它会一直保持平坦或陡峭的状态，所以你只需要针对平坦的情况设置较大的learning rate，对陡峭的情况设置较小的learning rate即可。但在下图的情况中，即使是在同一个方向上(如w1方向)，loss function也有可能一会儿平坦一会儿陡峭，所以就需要我们随时根据gradient的大小来快速地调整learning rate。 这个问题可以通过Adagrad的进阶版——RMSProp来解决。（RMSprop并不是在paper里提出来的，而是Hinton在他的网课里提出来的一个方法） how to do RMSProp RMSProp的做法如下： learning rate依旧设置为一个固定的值 $\\eta$ 除掉一个变化的值 $\\sigma$，这个$\\sigma$等于上一个$\\sigma$和当前梯度$g$的加权方均根（特别的是，在第一个时间点，$\\sigma^0$就是第一个算出来的gradient值$g^0$），即： w^{t+1}=w^t-\\frac{\\eta}{\\sigma^t}g^t \\\\ \\sigma^t=\\sqrt{\\alpha(\\sigma^{t-1})^2+(1-\\alpha)(g^t)^2}这里的$\\alpha$值是可以自由调整的，RMSProp跟Adagrad不同之处在于，Adagrad的分母是对过程中所有的gradient取平方和开根号，也就是说Adagrad考虑的是整个过程平均的gradient信息；而RMSProp虽然也是对所有的gradient进行平方和开根号，但是它用$\\alpha$来调整对不同gradient的权重，比如你把α的值设的小一点，意思就是你更倾向于相信新的gradient所告诉你的error surface的平滑或陡峭程度，而比较无视于旧的gradient所提供给你的information。 所以当做RMSProp的时候，你可以给现在已经看到的gradient比较大的weight，给过去看到的gradient比较小的weight，来调整对gradient信息的使用程度。 2.2.3. Momentumoptimization - local minima？ 除了learning rate的问题以外，在做deep learning的时候，也会出现卡在local minimum、saddle point或是plateau的地方，很多人都会担心，deep learning这么复杂的model，可能会非常容易在这些地方“卡住”。但其实Yann LeCun在07年的时候，就提出了这样的观点：我们不需要太担心local minima的问题，因为一旦出现local minima，它就必须在每一个dimension都是下图中这种山谷的低谷形状，假设山谷的低谷出现的概率为p，由于我们的network有非常多的参数，比如有1000个参数，每一个参数都要位于山谷的低谷之处，这件事发生的概率为$p^{1000}$，当你的network越复杂，参数越多，这件事发生的概率就越低。 所以在一个很复杂的neural network里，其实并没有那么多的local minima，所以当算法运行到一个你觉得是local minima的地方被卡住了，那它八成就是global minima，或者是很接近global minima的地方。 where is Momentum from 有一个heuristic(启发性)的方法可以稍微处理一下上面所说的“卡住”的问题，它的灵感来自于物理中的惯性的概念。如果是在现实世界中，在有一个球从左上角滚下来，它会滚到plateau的地方、local minima的地方，但是由于惯性它还会继续往前走一段路程，假设前面的坡没有很陡，这个球就很有可能翻过山坡，走到比local minima还要好的地方。 所以我们可以按照这个思路，把“惯性项”加到gradient descent里面，这就叫做Momentum。 how to do Momentum 当我们在gradient descent里加上Momentum的时候，每一次update的方向，不再只考虑gradient的方向，还要考虑上一次update的方向，这里用变量$v$去记录前一个时间点update的方向。 随机选一个初始值$\\theta^0$，初始化$v^0=0$，接下来计算$\\theta^0$处的gradient，我们要移动的方向是由前一个时间点的移动方向$v^0$和gradient的反方向$\\nabla L(\\theta^0)$来决定的，即 v^1=\\lambda v^0-\\eta \\nabla L(\\theta^0)注：这里的$\\lambda$也是一个可以调整的参数，它表示惯性对前进方向的影响有多大。 接下来我们第二个时间点要走的方向$v^2$，它是由第一个时间点移动的方向$v^1$和gradient的反方向$\\nabla L(\\theta^1)$共同决定的；$\\lambda v$是图中的绿色虚线，它代表由于上一次的惯性想要继续走的方向；$\\eta \\nabla L(\\theta)$是图中的红色虚线，它代表这次gradient告诉你所要移动的方向；它们的矢量和就是这一次真实移动的方向，为蓝色实线 我们还可以用另一种方法来理解Momentum，其实你在每一个时间点移动的步伐$v^i$，包括大小和方向，就是过去所有gradient的加权和。 具体推导如下图所示，第一个时间点移动的步伐$v^1$是$\\theta^0$处的gradient加权，第二个时间点移动的步伐$v^2$是$\\theta^0$和$\\theta^1$处的gradient加权和…以此类推；由于$\\lambda$的值小于1，因此该加权意味着越是之前的gradient，它的权重就越小，即你更在意的是现在的gradient，但是过去的所有gradient也要对你现在update的方向有一定程度的影响，这就是Momentum。 当然也可以从直觉上来想一下加入Momentum之后是怎么运作的。下图中，红色实线是gradient建议我们走的方向，直观上看就是根据坡度要走的方向；绿色虚线是Momentum建议我们走的方向，实际上就是上一次移动的方向；蓝色实线则是最终真正走的方向。 如果我们今天走到local minimum的地方，此时gradient是0，但是Momentum，它指向右侧就是告诉你之前是要走向右边的，所以你仍然应该要继续往右走，所以最后你参数update的方向仍然会继续向右；你甚至可以期待Momentum比较强，惯性的力量可以支撑着你走出这个谷底，去到loss更低的地方。 2.2.4. AdamRMSProp加上Momentum，就是Adam。根据下面的paper来快速描述一下Adam的algorithm： 先初始化$m_0=0$，$m_0$就是Momentum中，前一个时间点的movement 再初始化$v_0=0$，$v_0$就是RMSProp里计算gradient的root mean square的$\\sigma$ 最后初始化$t=0$，t用来表示时间点 先算出gradient $g_t$ g_t=\\nabla _{\\theta}f_t(\\theta_{t-1}) 再根据过去要走的方向$m_{t-1}$和gradient $g_t$，算出现在要走的方向 $m_t$——Momentum m_t=\\beta_1 m_{t-1}+(1-\\beta_1) g_t 然后根据前一个时间点的$v_{t-1}$和gradient $g_t$的平方，算一下放在分母的$v_t$——RMSProp v_t=\\beta_2 v_{t-1}+(1-\\beta_2) g_t^2 接下来做了一个原来RMSProp和Momentum里没有的东西，就是bias correction，它使$m_t$和$v_t$都除上一个值，这个值本来比较小，后来会越来越接近于1 (原理详见paper) \\hat{m}_t=\\frac{m_t}{1-\\beta_1^t} \\\\ \\hat{v}_t=\\frac{v_t}{1-\\beta_2^t} 最后做update，把Momentum建议你的方向$\\hat{m_t}$乘上learning rate $\\alpha$，再除掉RMSProp normalize后建议的learning rate分母，然后得到update的方向 \\theta_t=\\theta_{t-1}-\\frac{\\alpha \\cdot \\hat{m}_t}{\\sqrt{\\hat{v}_t}+\\epsilon} 到这里我们就了解了改善Deep learning在training data上的表现的基本方法。下面来介绍如何改善Deep learning在testing data上的表现。 3. Good Results on Testing Data？在Testing data上得到更好的performance的基本方法，大致可以分为三种：Early Stopping、Regularization和Dropout。值得注意的是，Early Stopping和Regularization是很typical的做法，它们不是特别为deep learning所设计的；而Dropout是一个蛮有deep learning特色的做法。 3.1. Early Stopping假设你今天的learning rate调的比较好，那随着训练的进行，total loss通常会越来越小，但是Training set和Testing set的情况并不是完全一样的，很有可能当你在Training set上的loss逐渐减小的时候，在Testing set上的loss反而上升了。理想上假如我们知道testing data上的loss变化情况，就会在testing set的loss最小的时候停下来，即提前停止训练—Early Stopping，而不是在training set的loss最小的时候停下来；但testing set实际上是未知的东西，所以我们需要用validation set来替代它去做这件事情。 注：很多时候，我们所讲的“testing set”并不是指代那个未知的数据集，而是一些已知的被你拿来做测试之用的数据集，比如kaggle上的public set，或者是你自己切出来的validation set。 3.2. Regularizationregularization就是在原来的loss function上额外增加几个term，比如我们要minimize的loss function原先应该是square error或cross entropy，那在做Regularization的时候，就在后面加一个Regularization的term。 3.2.1. L2 regularizationregularization term可以是参数的L2 norm(L2范数)，L2 norm是把model参数集$\\theta$里的每一个参数都取平方然后求和，这称作L2 regularization（L2 正则化），即 L2 \\ regularization:||\\theta||_2=(w_1)^2+(w_2)^2+... 通常我们在做regularization的时候，新加的term里是不考虑bias这一项的，因为加regularization的目的是为了让我们的function更平滑，而bias通常是跟function的平滑程度没有关系的。 L2 regularization具体流程如下： 加上regularization term之后得到了一个新的loss function：$L’(\\theta)=L(\\theta)+\\lambda \\frac{1}{2}||\\theta||_2$ 将这个loss function对参数$w_i$求微分：$\\frac{\\partial L’}{\\partial w_i}=\\frac{\\partial L}{\\partial w_i}+\\lambda w_i$ 然后update参数$w_i$：$w_i^{t+1}=w_i^t-\\eta \\frac{\\partial L’}{\\partial w_i}=w_i^t-\\eta(\\frac{\\partial L}{\\partial w_i}+\\lambda w_i^t)=(1-\\eta \\lambda)w_i^t-\\eta \\frac{\\partial L}{\\partial w_i}$ 如果把这个推导出来的式子和原式作比较，你会发现参数$w_i$在每次update之前，都会乘上一个$(1-\\eta \\lambda)$，而$\\eta$和$\\lambda$通常会被设为一个很小的值，因此$(1-\\eta \\lambda)$通常是一个接近于1的值，比如0.99,；也就是说，regularization做的事情是，每次update参数$w_i$之前，就先对原来的$w_i$乘个小于1的数，这意味着，随着update次数增加，参数$w_i$会越来越接近于0。 Q：你可能会问，要是所有的参数都越来越靠近0，那最后岂不是$w_i$通通变成0，得到的network还有什么用？ A：其实不会出现最后所有参数都变为0的情况，因为通过微分得到的$\\eta \\frac{\\partial L}{\\partial w_i}$这一项是会和前面$(1-\\eta \\lambda)w_i^t$这一项最后取得平衡的。 使用L2 regularization可以让weight每次都变得更小一点，这就叫做Weight Decay(权重衰减)，意思就是如果有一些weight它每次都会越来越小，最后就接近0然后不见了。 3.2.2. L1 regularization除了L2 regularization中使用平方项作为new term之外，还可以使用L1 regularization，把平方项换成每一个参数的绝对值，即 ||\\theta||_1=|w_1|+|w_2|+...Q：绝对值不能微分啊，该怎么处理呢？ A：实际上绝对值就是一个V字形的函数，在V的左边微分值是-1，在V的右边微分值是1，只有在0的地方是不能微分的，那在0处就可以设定一个微分值，比如0。 如果w是正的，那微分出来就是+1，如果w是负的，那微分出来就是-1，所以这边用了一个$w$的sign function，它的意思是，如果w是正数的话，这个function output就是+1，w是负数的话，这个function output就是-1。 L1 regularization的流程如下： 加上regularization term之后得到了一个新的loss function：$L’(\\theta)=L(\\theta)+\\lambda \\frac{1}{2}||\\theta||_1$ 将这个loss function对参数$w_i$求微分：$\\frac{\\partial L’}{\\partial w_i}=\\frac{\\partial L}{\\partial w_i}+\\lambda \\ sgn(w_i)$ 然后update参数$w_i$：$w_i^{t+1}=w_i^t-\\eta \\frac{\\partial L’}{\\partial w_i}=w_i^t-\\eta(\\frac{\\partial L}{\\partial w_i}+\\lambda \\ sgn(w_i^t))=w_i^t-\\eta \\frac{\\partial L}{\\partial w_i}-\\eta \\lambda \\ sgn(w_i^t)$ 这个式子告诉我们，每次update参数的时候，都要减去一个$\\eta \\lambda \\ sgn(w_i^t)$，如果w是正的，sgn是+1，就会变成减一个positive的值让你的参数变小；如果w是负的，sgn是-1，就会变成加一个值让你的参数变大；总之就是让它们的绝对值减小至接近于0。 3.2.3. L1 V.s. L2来对比一下L1和L2的update过程： L1: w_i^{t+1}=w_i^t-\\eta \\frac{\\partial L}{\\partial w_i}-\\eta \\lambda \\ sgn(w_i^t)\\\\ L2: w_i^{t+1}=(1-\\eta \\lambda)w_i^t-\\eta \\frac{\\partial L}{\\partial w_i}L1和L2，虽然它们同样是让参数的绝对值变小，但它们做的事情其实略有不同： L1使参数绝对值变小的方式是每次update减掉一个固定的值 L2使参数绝对值变小的方式是每次update乘上一个小于1的固定值 因此，当参数w的绝对值比较大的时候，L2会让w下降得更快，而L1每次update只让w减去一个固定的值，train完以后可能还会有很多比较大的参数；当参数w的绝对值比较小的时候，L2的下降速度就会变得很慢，train出来的参数平均都是比较小的，而L1每次下降一个固定的value，train出来的参数是比较sparse的，这些参数有很多是接近0的值，也会有很大的值。在之前所讲的CNN的task里，用L1做出来的效果是比较合适的，是比较sparse的。 some tips ps：在deep learning里面，regularization虽然有帮助，但它的重要性往往没有在SVM这类方法的作用来得高，因为我们在做neural network的时候，通常都是从一个很小的、接近于0的值开始初始参数的，而做update的时候，通常都是让参数离0越来越远，但是regularization要达到的目的，就是希望我们的参数不要离0太远。 如果你做的是Early Stopping，它会减少update的次数，其实也会避免你的参数离0太远，这跟regularization做的事情是很接近的。所以在neural network里面，regularization的作用并没有在SVM来的重要，SVM其实是explicitly把regularization这件事情写在了它的objective function(目标函数)里面，SVM是要去解一个convex optimization problem，因此它解的时候不一定会有iteration的过程，它不会有Early Stopping这件事，而是一步就可以走到那个最好的结果了，所以你没有办法用Early Stopping防止它离目标太远，必须要把regularization explicitly加到loss function里面去。 3.3. Dropout我们先来介绍dropout是怎么做的，然后再来解释为什么这样做。 3.3.1 How to do DropoutTraining 在training的时候，每次update参数之前，我们对每一个neuron(也包括input layer的“neuron”)做sampling(抽样) ，每个neuron都有p%的几率会被丢掉，如果某个neuron被丢掉的话，跟它相连的weight也都要被丢掉，实际上就是每次update参数之前都通过抽样只保留network中的一部分neuron来做训练。做完sampling以后，network structure就会变得比较细长了，然后再去train这个细长的network。 注：每次update参数之前都要做一遍sampling，所以每次update参数的时候，拿来training的network structure都是不一样的；你可能会觉得这个方法跟前面提到的Maxout会有一点像，但实际上，Maxout是每一笔data对应的network structure不同，而Dropout是每一次update参数时（每一个minibatch）的network structure都是不同的。 当你在training的时候使用dropout，得到的performance其实是会变差的，因为某些neuron在training的时候莫名其妙就会消失不见，但这并不是问题，因为： Dropout真正要做的事情，就是要让你在training set上的结果变差，但是在testing set上的结果是变好的。 所以如果你遇到的问题是在training set上得到的performance不够好，你再加dropout，就只会越做越差；我们应该要懂得，不同的problem需要用不同的方法去解决，而不是胡乱使用，dropout就是针对testing set的方法，当然不能够拿来解决training set上的问题。 Testing 在使用dropout方法做testing的时候要注意两件事情： testing的时候不做dropout，所有的neuron都要被用到； 假设在training的时候，dropout rate是p%，从training data中被learn出来的所有weight都要乘上(1-p%)才能被当做testing的weight使用。 3.3.2. Why Dropout？为什么dropout会有用？ 直觉的想法是：在training的时候，会丢掉一些neuron，就好像是你要练轻功的时候，会在脚上绑一些重物；然后，你在实际战斗的时候，就是实际testing的时候，是没有dropout的，就相当于把重物拿下来，所以你就会变得很强（老火影忍者了）。 另一个直觉的想法是：neural network里面的每一个neuron就是一个学生，那大家被连接在一起就好像要组队做final project，总是有人会拖后腿，就是他会dropout，所以假设你觉得自己的队友会dropout，这个时候你就会想要好好做，然后去carry这个队友，这就是training的过程。那实际在testing的时候，其实大家都有好好做，没有人需要被carry，由于每个人都比一般情况下更努力，所以得到的结果会是更好的，这也就是testing的时候不做dropout的原因。 为什么training和testing使用的weight是不一样的呢？ 直觉的解释是这样的：假设现在的dropout rate是50%，那在training的时候，你总是期望每次update之前会丢掉一半的neuron，就像下图左侧所示，在这种情况下你learn好了一组weight参数，然后拿去testing。但是在testing的时候是没有dropout的，所以如果testing使用的是和training同一组weight，那左侧得到的output z和右侧得到的output z‘，它们的值其实是会相差两倍的，即$z’≈2z$，这样会造成testing的结果与training的结果并不match，最终的performance反而会变差。这时就需要把右侧testing中所有的weight乘上0.5，然后做normalization，这样z就会等于z’，使得testing的结果与training的结果是比较match的。 Dropout is a kind of ensemble 在文献上有很多不同的观点来解释为什么dropout会work，其中一种比较令人信服的解释是：dropout是一种终极的ensemble（集成）的方法。ensemble的方法在比赛的时候经常用得到，它的思想是，我们有一个很大的training set，那你每次都只从这个training set里面sample一部分的data出来，像下图一样，抽取了set1,set2,set3,set4。 我们之前在讲bias和variance的trade off的时候说过，打靶有两种情况： 一种是因为bias大而导致打不准(参数过少) 另一种是因为variance大而导致打不准(参数过多) 假设我们今天有一个很复杂的model，它往往是bias比较准，但variance很大的情况，如果你有很多个笨重复杂的model，虽然它们的variance都很大，但取平均后，结果往往就会很准。所以ensemble做的事情，就是利用这个特性，我们从原来的training data里面sample出很多subset，然后train很多个model，每一个model的structure甚至都可以不一样；在testing的时候，丢了一笔testing data进来，使它通过所有的model，分别得到结果，然后把这些结果平均起来当做最后的output。 如果你的model很复杂，这一招往往是很有用的。random forest(随机森林)也是实践这种思想的一个方法，也就是如果用一个decision tree，它的效果会很弱，也很容易overfitting，而如果采用random forest，它就没有那么容易overfitting。 为什么dropout是一个终极的ensemble方法呢？ 在training network的时候，每次拿一个minibatch出来就做一次update，而根据dropout的特性，每次update之前都要对所有的neuron进行sample，因此每一个minibatch所训练的network structure都是不同的。 假设我们有M个neuron，每个neuron都有可能drop或不drop，所以总共可能的network数量有$2^M$个；所以当你在做dropout的时候，相当于是在用很多个minibatch分别去训练很多个network(一个minibatch一般设置为100笔data)，当做了有限次的update之后，就相当于train了很多种不同的network，最多可以训练到$2^M$个network。 每个network都只用一个minibatch的data来train，可能会让人感到不安，一个batch才100笔data，怎么train一个network呢？其实没有关系，因为这些不同的network之间的参数是shared，也就是说，虽然一个network只能用一个minibatch来train，但同一个weight可以在不同的network里被不同的minibatch train，所以同一个weight实际上是被所有没有丢掉它的network一起share的，它是拿所有这些network的minibatch合起来一起train的结果。 如果按照这个思路来实际操作会遇到这样的问题：我们train出来的network实在太多了，没有办法每一个network都丢一个input进去，再把它们的output平均起来，这样运算量太大了。 那dropout最神奇的地方就在于，它没有把这些network分开考虑，而是用一个完整的network，这个network的weight是用之前那一把network train出来的对应weight乘上(1-p%)，然后再把手上这笔testing data丢进这个完整的network，得到的output跟network分开考虑的ensemble的output，是惊人的相近。也就是说下图左侧ensemble的做法和右侧dropout的做法，得到的结果是approximate(近似)的。 举例说明dropout和ensemble的关系 这里用一个例子来解释：我们train一个下图右上角所示的简单的network，它只有一个neuron，activation function是linear的，并且不考虑bias，这个network经过dropout训练以后得到的参数分别为$w_1,w_2$，那给它input $x_1,x_2$，得到的output就是$z=w_1 x_1+w_2 x_2$ 如果我们今天要做ensemble的话，theoretically就是像下图这么做，每一个neuron都有可能被drop或不drop，这里只有两个input的neuron，所以我们一共可以得到2^2=4种network。我们把手上这笔testing data $x_1,x_2$丢到这四个network中，分别得到4个output：$w_1x_1+w_2x_2,w_2x_2,w_1x_1,0$，然后根据ensemble的思路，把这四个network的output相加然后取平均，得到的结果是$\\frac{1}{2}(w_1x_1+w_2x_2)$。 那根据dropout的想法，我们把从training中得到的参数$w_1,w_2$乘上(1-50%)，作为testing network里的参数，也就是$w’_1,w’_2=(1-50\\%)(w_1,w_2)=0.5w_1,0.5w_2$。那可以发现，在这个最简单的case里面，用不同的network structure做ensemble这件事情，跟我们用一整个network，并且把weight乘上一个值而不做ensemble所得到的output，其实是一样的。 值得注意的是，只有是linear的network，才会得到上述的等价关系，如果network是非linear的，ensemble和dropout是不equivalent的；但是，dropout最后一个很神奇的地方是，虽然在non-linear的情况下，它是跟ensemble不相等的，但最后的结果还是会work，会很接近。 如果network很接近linear的话，dropout所得到的performance会比较好，而ReLU和Maxout的network相对来说是比较接近于linear的，所以我们通常会把含有ReLU或Maxout的network与Dropout配合起来使用。","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://nekomoon404.github.io/categories/Deep-Learning/"}],"tags":[{"name":"ReLU","slug":"ReLU","permalink":"http://nekomoon404.github.io/tags/ReLU/"},{"name":"Maxout","slug":"Maxout","permalink":"http://nekomoon404.github.io/tags/Maxout/"},{"name":"Adam","slug":"Adam","permalink":"http://nekomoon404.github.io/tags/Adam/"},{"name":"Regularization","slug":"Regularization","permalink":"http://nekomoon404.github.io/tags/Regularization/"},{"name":"Dropout","slug":"Dropout","permalink":"http://nekomoon404.github.io/tags/Dropout/"}]},{"title":"DL笔记（6）Convolutional Neural Network（CNN）","slug":"ML笔记（6）Convolutional-Neural-Network（CNN）","date":"2020-07-10T13:38:47.000Z","updated":"2020-07-10T14:38:47.000Z","comments":true,"path":"2020/07/10/ML笔记（6）Convolutional-Neural-Network（CNN）/","link":"","permalink":"http://nekomoon404.github.io/2020/07/10/ML%E7%AC%94%E8%AE%B0%EF%BC%886%EF%BC%89Convolutional-Neural-Network%EF%BC%88CNN%EF%BC%89/","excerpt":"","text":"1.Why CNN for Image?1.1. CNN vs DNNConvolutional Neural Network，CNN（卷积神经网络）常常被用在影像辨识领域。当然也可以用一般的neural network来做影像处理，不一定要用CNN，比如做图像的分类，就可以去train一个neural network，它的input是一张图片，用pixel来表示这张图片，即一个很长的vector，而output则是由图像类别组成的vector，假设你有1000个类别，那output就有1000个dimension。 但是我们会遇到这样的问题：实际上，在train neural network的时候，我们会有一种期待：在这个network structure里面的每一个neuron，都应该代表了一个最基本的classifier；事实上，在文献中，根据训练的结果，也有很多人得到这样的结论，举例来说，下图中： 第一层layer的neuron，它就是最简单的classifier，它做的事情就是detect有没有绿色出现、有没有黄色出现、有没有斜的条纹出现等等； 第二层layer，它detect更复杂的东西，根据第一个layer的output，它如果看到直线横线，就是窗框的一部分；如果看到棕色的直条纹就是木纹；看到斜条纹加灰色的，这个有可能是轮胎的一部分等等； 第三层hidden layer再根据第二层hidden layer的output，会做更复杂的事情，比如当某一个neuron看到蜂巢，它就会被activate；当某一个neuron看到车子，它就会被activate；当某一个neuron看到人的上半身，它就会被activate等等。 但如果直接用一般的fully connected feedforward network来做图像处理的时候，往往会需要太多的参数。举例来说，假设我们的网络需要辨识的是100*100 pixel的彩色图片，它的分辨率是100*100，那这其实是很小张的image了。然后需要把他转换成一个vector，总共有100*100*3个pixel(如果是彩色的图的话，每个pixel其实需要3个value，即RGB值来描述它的)，那input vector就是30000维；如果input vector是三万维，又假设hidden layer有1000个neuron，那仅仅是第一层hidden layer的参数就已经有30000*1000个了，这样参数就太多了，train network的成本太高。 那CNN做的事情其实是，来简化这个neural network的架构，我们可以根据自己的知识和对图像处理的理解，一开始就把某些实际上用不到的参数给过滤掉。虽然CNN看起来它的运作比较复杂，但事实上，它的模型比DNN还要更简单，我们就是用prior knowledge，去把原来fully connected的layer里面的一些参数拿掉，就变成CNN。 1.2. Three Property for CNN theory base为什么我们有可能把一些参数拿掉？为什么我们有可能只用比较少的参数就可以来做图像处理这件事情？下面列出三个对影像处理的观察：(这也是CNN架构提出的基础所在) Some patterns are much smaller than the whole image 在影像处理中，如果network的第一层hidden layer中的neuron要做的事情是侦测有没有一种东西、一种pattern(图案样式)出现，那大部分的pattern其实是比整张image要小的（比如鸟会出现而图片的中央，而其他地方是背景），所以对一个neuron来说，想要侦测有没有某一个pattern出现，它其实并不需要看整张image，只需要看这张image的一小部分。 比如现在有一张鸟的图片，那第一层hidden layer的某一个neuron的工作是，检测有没有鸟嘴的存在（可能还有一些neuron侦测有没有鸟嘴的存在、有一些neuron侦测有没有爪子的存在、有一些neuron侦测有没有翅膀的存在、有没有尾巴的存在，之后合起来，就可以辨识图片中有没有一只鸟），那它其实并不需要看整张图，只要给neuron看这个小的红色杠杠里面的区域，它就可以知道这是不是一个鸟嘴。所以，每一个neuron其实只要连接到一个小块的区域就好，它不需要连接到整张完整的图，因此也对应着更少的参数。 The same patterns appear in different regions 同样的pattern，可能会出现在image的不同部分，但是它们有同样的形状、代表的是同样的含义，因此它们也可以用同样的neuron、同样的参数，被同一个detector检测出来。 比如上面两张图中分别有一个处于左上角的鸟嘴和一个处于中央的鸟嘴，但我们并不需要训练两个不同的detector去专门侦测左上角有没有鸟嘴和中央有没有鸟嘴这两件事情，这样做太冗余了，我们要cost down(降低成本)，我们并不需要有两个neuron、两组不同的参数来做duplicate(重复一样)的事情，所以我们可以要求这些功能几乎一致的neuron共用一组参数，它们share同一组参数就可以帮助减少总参数的量。 Subsampling the pixels will not change the object 我们可以对一张image做subsampling(二次抽样)，假如你把它奇数行、偶数列的pixel拿掉，image就可以变成原来的四分之一大小，而且并不会影响人对这张image的理解。所以，可以利用subsampling把image变小，从而减少需要的参数量。 2. The whole CNN structureCNN的结构可以这样理解：首先，input data以后，它会先通过Convolution的layer，接下来做Max Pooling，然后再去做Convolution，再做Maxi Pooling…，这个process可以反复进行多次(重复次数需要事先决定)，这就是network的架构。当做完先前决定的convolution和max pooling的次数后，接着做Flatten（将矩阵展开成vector），最后把Flatten output放到一般的Fully connected network里面去，最终得到输出的结果。 我们基于之前提到的三个对影像处理的观察，设计了CNN这样的架构，第一个是要侦测一个pattern，不需要看整张image，只要看image的一个小部分；第二个是同样的pattern会出现在一张图片的不同区域；第三个是我们可以对整张image做subsampling。那前面这两个property，是用convolution的layer来处理的；而第三个property，是用max pooling来处理的。 2.1. Convolution假设现在我们的network的input是一张6*6的image，图像是黑白的，因此每个pixel只需要用一个value来表示，而在convolution layer里面，有很多Filter，每一个Filter，其实就相当于是Fully connected layer里的一个neuron。 Property 1 每一个Filter其实就是一个matrix，这个matrix里面每一个element的值，就跟那些neuron的weight和bias一样，是network的parameter，它们具体的值都是通过Training data学出来的，而不是人去设计的。上图中每一个Filter是3*3的size，意味着它就是在侦测一个3*3的pattern，当它侦测的时候，并不会去看整张image，它只看一个3*3范围内的pixel，就可以判断某一个pattern有没有出现，这就实现了property 1。 Property 2 这个Filter是从image的左上角开始，做一个slide window，每次向右挪动一定的距离，这个距离就叫做stride，由你自己设定，每次filter停下的时候就跟image中对应的3*3的matrix做一个内积(相当于展开成vector再作内积，其实就是相同位置的值相乘并累计求和)。 这里假设stride=1，那么我们的Filter每次移动一格，当它碰到image最右边的时候，就从下一行的最左边开始重复进行上述操作，经过一整个convolution的process，最终得到下图所示的红色的4*4 matrix。 观察上图中的Filter1，它斜对角的地方是1,1,1，所以它的工作就是detect有没有连续的从左上角到右下角的1,1,1出现在这个image里面，检测到的结果已在上图中用蓝线标识出来，此时Filter得到的卷积结果的左上和左下得到了最大的值，这就代表说，该Filter所要侦测的pattern出现在image的左上角和左下角。同一个pattern出现在image左上角的位置和左下角的位置，并不需要用到不同的Filter，我们用Filter1就可以侦测出来，这就实现了property 2。 Feature Map 在一个convolution的layer里面，它会有很多的filter，不一样的filter会有不一样的参数，但是这些filter做卷积的过程都是一模一样的，把filter2和image做完convolution以后，就会得到另外一个的4*4 matrix，那这个蓝色的4*4 matrix跟之前红色的4*4matrix合起来，就叫做Feature Map(特征映射)，有多少个filter，对应就有多少个映射后的image。 CNN对不同scale的相同pattern的处理上存在一定的困难，由于现在每一个filter size都是一样的，这意味着，如果你今天有同一个pattern，它有不同的size，有大的鸟嘴，也有小的鸟嘴，CNN并不能够自动处理这个问题；DeepMind曾经发过一篇paper，上面提到了当你input一张image的时候，它在CNN前面，再接另外一个network，这个network做的事情是，它会output一些scalar，告诉你说，它要把这个image的里面的哪些位置做旋转、缩放，然后，再丢到CNN里面，这样你其实会得到比较好的performance。 Colorful image 刚才举的例子是黑白的image，input的是一个2维的matrix。彩色的image的每一个pixel就是由RGB组成的，相当于3个matrix叠在一起，可以理解是多了一维“深度”。这时Filter也会跟着input作相应的变化，假如上面的例子中输入的是彩色图片，input就是3*6*6的，Filter相应就是3*3*3。在做convolution的时候，就是把这个Filter的27个值跟image里对应区块的27个值做内积，即三个channel是一起处理的，经过convolution之后仍然得到一个$4*4$的matrix（注意不是$443$），这样在做完这一层的convolution之后，有多少个Filter，得到的Feature Map就有多少“层”，和处理黑白image是一样的。 2.2. Convolution V.s. Fully connectedfilter是特殊的”neuron“ 接下来要讲的是，convolution跟fully connected有什么关系。你可能觉得会觉得它是一个很特别的operation，感觉跟neural network没半毛钱关系┐(ﾟ～ﾟ)┌ ，但其实它就是一个neural network。 convolution其实就相当于是fully connected的layer把一些weight删掉，下图中绿色方框标识出的feature map的output，其实就是hidden layer的neuron的output。 那为什么是这样的呢(。-`ω´-)？如下图，我们在做convolution的时候，把Filter放在image的左上角，然后再去做inner product，得到一个值3；这件事情等同于，我们现在把这个image的6*6的matrix拉直变成右边这个用于input的vector（竖着的一长条），然后有一个红色的neuron，这些input经过这个neuron之后，得到的output是3。 每个“neuron”只检测image的部分区域 那这个neuron的output怎么来的呢？这个neuron实际上就是由Filter转化而来的，我们把filter放在image的左上角，此时filter考虑的就是和它重合的9个pixel，假设你把这一个6*6的image的36个pixel拉成直的vector作为input，那这9个pixel分别就对应着右侧编号1，2，3的pixel，编号7，8，9的pixel跟编号13，14，15的pixel。 如果我们说这个filter和image matrix做inner product以后得到的output 3，就是input vector经过某个neuron得到的output 3的话，这就代表说存在这样一个neuron，这个neuron带weight的连线，就只连接到编号为1，2，3，7，8，9，13，14，15的这9个pixel而已，而这个neuron和这9个pixel连线上所标注的的weight就是filter matrix里面的这9个数值。 作为对比，Fully connected的neuron是必须连接到所有36个input上的，但是，我们现在只用连接9个input，因为我们知道要detect一个pattern，不需要看整张image，看9个input pixel就够了，所以当我们这么做的时候，就用了比较少的参数 “neuron”之间共享参数 当我们把filter做stride = 1的移动后，通过filter和image matrix的内积得到另外一个output值-1，我们假设这个-1是另外一个neuron的output，那这个neuron会连接到哪些input呢？下图中这个框起来的地方正好就对应到pixel 2，3，4，pixel 8，9，10跟pixel 14，15，16 你会发现output为3和-1的这两个neuron，它们的作用是分别去检测在image的两个不同位置上是否存在某个相同pattern，它们的weight是一样的。但在Fully connected layer里它们通常做的是两件不同的事情，每一个neuron应该有自己独立的weight。 来看我们的convolution，首先是把每一个neuron前面连接的weight减少了，然后我们强迫某些neuron(比如上图中output为3和-1的两个neuron)，它们一定要共享一组weight，虽然这两个neuron连接到的pixel对象各不相同，但它们用的weight都必须是一样的(9=3*3对应着filter的元素个数，这些weight也就是filter内部的元素值，上图中圆圈的颜色与连线的颜色一一对应，PS：李老师讲课真的很用心)，等于filter里面的元素值，这件事情就叫做weight share。这样的两个方法就会让convolution里的参数数量相比于fully connected layer的大大减少。可以说CNN的本质，就是减少参数的过程。 看到这里你可能会问(´ω｀)，这样的network该怎么搭建，又该怎么去train呢？其实现在这些都是用toolkit（Tensorflow, Pytorch等等）来完成的，所以你大概不会自己去写；如果要自己写的话，它其实就是跟原来的Backpropagation用一模一样的做法，只是有一些weight就永远是0，你就不用去train它，它就永远是0。然后，怎么让某些neuron的weight值永远都是一样呢？就可以先用一般的Backpropagation的方法，对每个weight都去算出gradient，再把本来要tight在一起、要share weight的那些weight的gradient取平均，然后让这些weight用这个相同的平均值取update就好了。 2.3. Max PoolingOperation of max pooling 相较于convolution，max pooling是比较简单的，它相当于实现了property3，即subsampling，由Filter 1得到一个4*4的matrix，由Filter 2得到另外一个4*4的matrix。接下来，我们把output四个分为一组，每一组里面通过选取平均值或最大值的方式，把原来4个value合成一个 value，这种subsampling的方式就可以让你的“image”缩小。 但是如果取Maximum放到network里面，不就没法微分了吗？其实是可以的，后面的章节会讲到Maxout network，会告诉你怎么用微分的方式来处理它。 Convolution + Max Pooling 做完一次convolution加一次max pooling，我们就把原来6*6的image，变成了一个 Filter个数2\\2 的image；如下图中是两个Filter，那经过一次convolution+max pooling得到的image就是$222$维的。这是一个新的比较小的image，它表示的是不同区域上提取到的特征，不同的Filter检测的是该image同一区域上的不同特征属性。 Convolution + Max Pooling可以repeat很多次，你可以把得到的这个比较小的image，再次进行convolution和max pooling的操作，得到一个更小的image，依次类推。 你可能会问：假设我第一个convolution有25个filter，通过这些filter得到25个feature map，然后repeat的时候第二个convolution也有25个filter，那这样做完，我是不是会得到25*25个feature map呢？ 其实不是这样的，convolution的input是会考虑“深度”的，比如在第一次convolution+max pooling后得到的是$2566$的matrix，那么第二层convolution的FIlter就可以是$2533$了，这样第二次得到的feature map也是25个。所以每次convolution+max pooling得到的matrix的“层数”或者说“深度”，只和这一层的convolution中的Filter数有关，即等于这一层的convolution中的Filter数。 2.4. Flatten做完convolution和max pooling之后，就是FLatten和Fully connected Feedforward network的部分。Flatten的意思是，把左边的feature map“拉直”成一个vector。然后把这个vector丢进一个Fully connected Feedforward network，最后得到输出的结果。 也就是说，我们之前通过CNN提取出了image的feature，它相较于原先一整个image的vetor，少了很大一部分内容，因此需要的参数也大幅度地减少了，但最终也还是要丢到一个Fully connected的network中去做最后的分类工作。 2.5. Example: CNN in Keras过程可以概括为： 假设我们input是一个1*28*28的image； 通过25个Filter的convolution layer，每个Filter的size是33，如果不考虑image边缘处的处理的话，得到的每个channel会是26\\26的，因此通过第一个convolution得到25*26*26的“cubic image”； 接着做Max pooling，把2*2的pixel分为一组，然后从里面选一个最大的组成新的image，得到25*13*13的image； 再做一次convolution，假设这次选择50个filter，每个filter size是253\\3，output得到50*11*11的image； 再做一次Max Pooling，也是把2*2的pixel分为一组，然后从里面选最大值，output得到50*5*5的image； 最后用Flatten将这个image“拉直”成一个1250维的vector，把这个vector作为一个Fully Connected Feedforward network的输入，这样我们的CNN的structure就搭建完成了。 3. What does CNN learn?很多人会说Deep Learning就是一个黑盒子，你learn出来以后，根本就不知道为什么是这样子，于是你会感觉它很intelligent⊙(・◇・)？，但其实我们可以从CNN设计的思路出发来分析它在learn的过程中学到了什么。 3.1. what does filter do还是用上面的例子，第一层convolution的Filter比较容易理解，每一个Filter是一个3*3的matrix，它对应到image中3*3范围内的9个pixel，所以这个Filter的值就反映着它在detect什么东西。 但第二层convolution中的Filter的作用就比较难理解了，每个Filter的size是$2533$，这些Filter的input并不是pixel，而是做完一次convolution+max pooling的结果，因此Filter考虑的范围并不是3*3=9个pixel，而是一个长宽为3*3，高为25的“cubic”，filter实际在image上看到的范围是远大于9个pixel的，就算把它的weight拿出来看，也不知道它在做什么。 那我们可以通过下面的方法来分析一个Filter它做的事情是什么： 第二层convolution里面的50个Filter，每一个Filter的output是一个11*11的matrix，假设现在把第k个filter的output拿出来，如下图所示，这个matrix里的每一个element，记为$a^k_{ij}$，上标$k$表示这是第$k$个filter，下标$ij$表示它在这个matrix里的第$i$行，第$j$列。 定义$a^k$为Degree of the activation of the k-th filter，这个值表示现在的第k个filter，它有多被activate，直观来讲就是描述现在input的东西跟第k个filter有多接近，它对filter的“激活程度”有多少。$a^k$等于，第k个filter与input进行卷积所输出的output里所有element的summation，以上图为例，就是这11*11的output matrix里所有元素之和： a^k=\\sum\\limits^{11}_{i=1}\\sum\\limits^{11}_{j=1} a^k_{ij}接下来我们想要知道第k个filter的作用是什么，那就找一张image，这张image可以让第k个filter被activate的程度最大；即找一个image x*，它可以让我们定义的activation的degree $a^k$最大： x^*=\\arg \\max\\limits_x a^k之前我们求minimize用的是gradient descent，那现在我们求Maximum用gradient ascent(梯度上升法)。 这个方法还是颇为神妙的(๑´ㅂ`๑) ，现在是把input x作为要找的参数，对它去用gradient descent或ascent进行update，在这个task里面model的参数是固定的，要用gradient ascent去update这个x，让它可以使degree of activation最大。 上图就是得到的结果，50个filter理论上可以分别找50张image使对应的activation最大，这里仅挑选了其中的12张image作为展示，这些image有一个共同的特征，它们里面都是一些反复出现的某种texture(纹路)，比如说第三张image上布满了小小的斜条纹，这意味着第三个filter的工作就可以理解为是在detect图上有没有斜条纹，要知道现在每个filter检测的都只是图上一个小小的范围而已，所以图中一旦出现一个小小的斜条纹，这个filter就会被activate，相应的output也会比较大，所以如果整张image上布满这种斜条纹的话，这个filter的activation程度是最大的，相应的output值也会达到最大。 因此我们可以用这种方法去研究CNN中的每个filter的作用是去detect怎样的pattern，或者说texture。 3.2. what does neuron do做完convolution和max pooling之后，会将结果用Flatten展开，然后丢到Fully connected neural network里面去，之前已经搞清楚了filter是做什么的，那我们也想要知道在这个neural network里的每一个neuron是做什么的，那就可以对刚才的做法如法炮制。 我们定义第$j$个neuron的output就是$a_j$，接下来就用gradient ascent的方法去找一张image $x$，这个$x$作为输入可以使$a_j$的值最大，即： x^*=\\arg \\max\\limits_x a^j找到的结果如上图所示，同理这里仅取出其中的9张image作为展示，你会发现这9张图跟之前filter所观察到的情形是很不一样的，刚才我们观察到的是类似纹路的东西，那是因为每个filter考虑的只是图上一部分的vision，所以它detect的是一种texture；但是在做完Flatten以后，每一个neuron不再是只看整张图的一小部分，它现在的工作是看整张图。 3.3. what about output接下来考虑CNN的output，由于是手写数字识别的demo，因此这里的output就是10维。同样地，我们把某一维拿出来，然后同样去找一张image x，使这个维度的output值最大，即 x^*=\\arg \\max_x y^i可以想象说既然现在每一个output的每一个dimension就对应到一个数字，那如果我们去找一张image x，它可以让对应到数字1的那个output layer的neuron的output值最大，那这张image显然应该看起来会像是数字1，你甚至可以期待搞不好用这个方法就可以让machine自动画出数字⊙(・◇・)？ 但实际上得到的结果却是如下图所示： 上面的每一张图分别对应着数字0-8，发现可以让数字1对应neuron的output值最大的image看起来一点也不像1；为了验证程序有没有bug，把上述得到的image作为testing data丢到CNN里面，结果classify的结果确实对应着的数字0-8。看来这个CNN所学到的东西跟人类一般的想象认知还是有差别的。 那有没有办法让上面这个图看起来更像数字呢？想法是这样的：一张图是不是一个数字，它会有一些基本的假设，比如这些image，显然人类手写出来的东西就不是长这个样子的。所以要对x做一些regularization，做一些constraint(限制约束)，我们应该告诉machine说，虽然有一些x可以让y很大，但是它们不是数字。 最简单的加constraint的想法是：image中白色代表的是有墨水、有笔画的地方，而对于一个digit来说，整张image上涂白的区域是有限的，像上面这些整张图都是白白的，它一定不会是数字。假设image里的每一个pixel都用$x_{ij}$表示，我们把所有pixel值取绝对值并求和$\\sum\\limits_{i,j}|x_{ij}|$，这一项其实就是之前提到过的L1的regularization，再用$y^i$减去这一项，得到： x^*=\\arg \\max\\limits_x (y^i-\\sum\\limits_{i,j} |x_{ij}|)这样我们找出的让上式最大的$x*$，它可以让$y^i$很大的同时，也是让$|x_ij|$的summation越小越好，那我们希望找出来的image，大部分的地方是没有涂颜色的，只有少数数字笔画在的地方才有颜色出现。加上这个constraint以后，得到的结果会像下图右侧所示一样，已经隐约有些可以看出来是数字的形状了 如果再加上一些额外的constraint，比如希望相邻的pixel是同样的颜色等等，应该可以得到更好的结果。 3.4. Deep Dream那上面讲到的有点“逆向”的思路，我们就可以实现这一一件事：如果你给network一张image，它会在这个image里面加上它看到的东西，做法就是：找一张image输入到CNN训练，然后把某一个convolution layer里面的filter或是fully connected layer里的某一个hidden layer的output拿出来；接下来把本来是positive的dimension值调大，negative的dimension值调小，即让绝对值更大，将它作为我们要找的image的target。 然后用gradient descent的方法找一张image x，让它通过这个hidden layer后的output就是你调整后的target，这么做的目的是让CNN夸大化它看到的东西——make CNN exaggerates what is sees。也就是说，如果某个filter有被activate，那你让它被activate的更剧烈，CNN可能本来看到了某一样东西的“端倪”，那现在你就让它更“具象化”，或者说“夸大化”。如果把上面这张image拿去做Deep Dream的话，看到的结果就会像下面这个样子（着实是有点吓人｀Д´|）： 3.5. Deep StyleDeep Dream还有一个进阶的版本，叫做Deep Style，或者叫Neural Style，图片风格迁移。我们input一张image，Deep Style做的事情就是让machine去修改这张图，让它有另外一张图的风格，如下所示： 实际上机器做出来的效果惊人的好，具体的做法参考reference：A Neural Algorithm of Artistic Style Deep Style的大致思路是，把原本的image输入到CNN，得到CNN filter的output，代表这张image里面有什么样的content。然后你把呐喊这张图也丢到CNN里面得到filter的output，这时我们在意的是，filter和filter的output之间的correlation，这个correlation代表了这张image的style。 接下来就用一个CNN去找一张image，这张image的content像左边的图片，比如使这张image的filter output的value接近左边的图片的value；同时让这张image的style像右边的图片，比如使这张image output的filter之间的correlation像右边这张图片。最终用gradient descent找到一张image，同时可以maximize左边的content和右边的style，它的样子就像上图左下角所示： 4. More Application4.1. Playing Go除了图像处理，CNN可以被运用到不同的领域，比如广为人知的alphaGo。想要让machine来下围棋，其实一般的neural network也可以帮我们做到这件事情，但采用CNN的话会得到更好的performance。 之前举的例子都是把CNN用在图像上面，input是一个matrix，而棋盘就可以表示成一个19*19的matrix，那对CNN来说，就是直接把它当成一个image来看待，然后再output下一步要落子的位置，training process可以通过搜集很多棋谱来train CNN，看到这样的棋局应该有什么样的output，即下一步应该在哪里落子。其实这时supervised的部分，AlphaGo还有reinforcement learning的部分，后面的章节会讲到。 回想之前讲过的设计在图像识别中，设计CNN的结构时考虑的三个property： Some patterns are much smaller than the whole image The same patterns appear in different regions Subsampling the pixels will not change the object CNN能够应用在Alpha-Go上，是因为围棋有一些特性和图像处理是很相似的。对于property 1，有一些pattern是比整张image要小得多，在围棋中也有同样的现象。在AlphaGo里面，它第一层convolution是用5*5的filter，显然做设计的人了解围棋上最基本的pattern可能都是在5*5的范围内就可以被侦测出来。对于property 2，同样的pattern可能会出现在不同的region，这在围棋中也是存在的，所以可以用同一个detector，来处理这些在不同位置的同样的pattern。 但是对于property3，对棋谱做subsampling之后，其所表达的内容明显就和之前的不同的，通过阅读Alpha-go的论文，我们可以知道AlphaGo的network structure中并没有Max pooling层，即没有做subampling。可见根据围棋的特性，并不需要在围棋的CNN里面使用用Max pooling这样的构架。 4.2. SpeechCNN也可以用在语音处理上，我们可以把一段声音表示成spectrogram，spectrogram的横轴是时间，纵轴则是这一段时间里声音的频率，如下图中是一段“你好”的音频，偏红色代表这段时间里该频率的energy是比较大的，也就对应着“你”和“好”这两个字，spectrogram用颜色来描述某一个时刻不同频率的能量。我们可以把这个spectrogram当作一张image，然后用CNN来判断input的这张image对应着什么样的声音信号，通常用来判断结果的单位可以用phoneme（类似音标）。 用CNN处理语言的一个比较特别的点是：在convolution中通常只考虑在frequency(频率)方向上移动Filter，而不在时间的序列上移动。 这是因为在语音里面，CNN的output后面都还会再接别的东西，比如接LSTM之类，它们都已经有考虑时间有关的信息。那在频率上的filter有帮助的原因是，在声音讯号上，比如虽然男生和女生说同样的话看起来这个spectrogram是非常不一样的，但实际上他们的不同只是表现在一个频率的shift而已(整体在频率上的位移)，男生说的“你好”跟女生说的“你好”，它们的pattern其实是一样的，它们的差别可能只是所在的频率范围不同而已，所以filter在frequency的direction上移动是有效的。 4.3. TextCNN也可以用在文字处理上，假设input是一个word sequence，你要做的事情是让machine侦测这个word sequence代表的意思是positive的还是negative的。 首先把这个word sequence里面的每一个word都用一个vector来表示，vector代表的这个word本身的semantic (语义)，那如果两个word本身含义越接近的话，它们的vector在高维的空间上就越接近，这叫做word embedding。 把一个sentence里面所有word的vector排在一起，它就变成了一张image（一个matrxi），然后把CNN套用到这个image上，但是这时Filter的“高”和image的高是一样的，然后把Filter沿着句子里词汇的顺序来移动，每个filter移动完成之后都会得到一个由内积结果组成的vector，不同的filter就会得到不同的vector，接下来做Max pooling，然后把Max pooling的结果丢到fully connected layer里面，得到最后的output。 与语音处理不同的是，在文字处理上，filter只在时间的序列(按照word的顺序)上移动，而不在embedding的dimension上移动；因为在word embedding里面，不同dimension是independent的，它们是相互独立的，不会出现有两个相同的pattern的情况，所以在这个方向上面移动filter，是没有意义的。 —————— 这一章举这三个例子是为了说明，虽然CNN很powerful，但在设计network structure去解决新的task的时候，都要根据所要解决的问题的特性，设计出合理合适的strcuture，包括Filter的尺寸，移动的方向，stride的大小；有无Max pooling等等很多因素，而不是生搬硬套已有的模式。所谓应用之道，存乎一心。","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://nekomoon404.github.io/categories/Deep-Learning/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://nekomoon404.github.io/tags/CNN/"},{"name":"Convolution","slug":"Convolution","permalink":"http://nekomoon404.github.io/tags/Convolution/"},{"name":"Max pooling","slug":"Max-pooling","permalink":"http://nekomoon404.github.io/tags/Max-pooling/"},{"name":"Filter","slug":"Filter","permalink":"http://nekomoon404.github.io/tags/Filter/"}]},{"title":"DL笔记（5）Backpropagation","slug":"ML笔记（5）Backpropagation","date":"2020-07-09T10:58:41.000Z","updated":"2020-07-09T11:58:41.000Z","comments":true,"path":"2020/07/09/ML笔记（5）Backpropagation/","link":"","permalink":"http://nekomoon404.github.io/2020/07/09/ML%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89Backpropagation/","excerpt":"","text":"训练Neural Network依然可以使用此前学过的Gradient Descent的方法来更新参数，基本步骤与linear Regression或Logistic Regression中是相同的，但Neural network中的parameters $\\theta=w_1,w_2,…,b_1,b_2,…$里面可能会有将近million个参数。所以现在最大的困难是，如何有效地把这个近百万维的vector给计算出来，这就是Backpropagation要做的事情，Backpropagation并不是一个和gradient descent不同的training的方法，它只是一个比较有效率的算法，使得求gradient更有效率。 Chain Rule Backpropagation的数学原理其实很简单，只需记住Chain Rule 链式法则。 Backpropagation对整个neural network，定义一个loss function：$L(\\theta)=\\sum\\limits_{n=1}^N l^n(\\theta)$，它等于所有training data的loss之和。我们把training data里任意一个样本点$x^n$代到neural network里面，它会output一个$y^n$，我们把这个output跟样本点本身的label标注的target $\\hat{y}^n$作cross entropy，这个交叉熵定义了output $y^n$和target $\\hat{y}^n$之间的距离$l^n(\\theta)$，如果cross entropy比较大的话，说明output和target之间距离很远，这个network的parameter的loss是比较大的，反之则说明这组parameter是比较好的。然后summation over所有training data的cross entropy $l^n(\\theta)$，得到total loss $L(\\theta)$，这就是我们的loss function，用这个$L(\\theta)$对某一个参数w做偏微分，表达式如下： \\frac{\\partial L(\\theta)}{\\partial w}=\\sum\\limits_{n=1}^N\\frac{\\partial l^n(\\theta)}{\\partial w}从表达式中可知，只需要考虑如何计算对某一笔data的$\\frac{\\partial l^n(\\theta)}{\\partial w}$，再将所有training data的cross entropy对参数w的偏微分累计求和，就可以把total loss对某一个参数w的偏微分给计算出来。 我们先考虑某一个neuron（图中被红色三角形圈住的neuron），假设只有两个input $x_1,x_2$，通过这个neuron，我们先得到$z=b+w_1 x_1+w_2 x_2$，然后经过activation function从这个neuron中output出来，作为后续neuron的input，再经过了很多层的运算之后，会得到最终的output $y_1,y_2$ 接着考虑计算$\\frac{\\partial l}{\\partial w}$，按照chain rule，可以把它拆分成两项，$\\frac{\\partial l}{\\partial w}=\\frac{\\partial z}{\\partial w} \\frac{\\partial l}{\\partial z}$，这两项分别去把它计算出来： 计算前一项$\\frac{\\partial z}{\\partial w}$的process，称之为Forward pass； 计算后一项$\\frac{\\partial l}{\\partial z}$的process，称之为Backward pass。 Forward Pass$\\frac{\\partial z}{\\partial w}$这一项计算很简单，$\\frac{\\partial z}{\\partial w_1}=x_1 ,\\ \\frac{\\partial z}{\\partial w_2}=x_2$。它的规律是这样的：求$\\frac{\\partial z}{\\partial w}$，就是看$w$前面连接的input是什么，那微分后的$\\frac{\\partial z}{\\partial w}$值就是什么，因此只要计算出neural network里面每一个neuron的output就可以知道任意的$z$对$w$的偏微分： 比如input layer作为neuron的输入时，$w_1$前面连接的是$x_1$，所以微分值就是$x_1$；$w_2$前面连接的是$x_2$，所以微分值就是$x_2$ 比如hidden layer作为neuron的输入时，那该neuron的input就是前一层neuron的output，于是$\\frac{\\partial z}{\\partial w}$的值就是前一层的z经过activation function之后输出的值(下图中的数据是假定activation function为sigmoid function得到的) Backward Pass$\\frac{\\partial l}{\\partial z}$这一项比较复杂的，这里我们依旧假设activation function是sigmoid function。$z$通过activation function后得到$a$，这个neuron的output是$a=\\sigma(z)$，接下来这个a会乘上某一个weight $w_3$，再加上其它的value得到$z’$，它是下一个neuron activation function的input，然后a又会乘上另一个weight $w_4$，再加上其它value得到$z’’$，后面还会经过很多运算，这里我们先考虑下一步会发生什么事情。 由Chain Rule，$\\frac{\\partial l}{\\partial z}$可以写成 \\frac{\\partial l}{\\partial z}=\\frac{\\partial a}{\\partial z} \\frac{\\partial l}{\\partial a}这里的$\\frac{\\partial a}{\\partial z}$实际上就是activation function的微分(在这里就是sigmoid function的微分)，接下来的问题是$\\frac{\\partial l}{\\partial a}$应该长什么样子呢？a会影响$z’$和$z’’$，而$z’$和$z’’$会影响$l$，所以通过chain rule可以得到 \\frac{\\partial l}{\\partial a}=\\frac{\\partial z'}{\\partial a} \\frac{\\partial l}{\\partial z'}+\\frac{\\partial z''}{\\partial a} \\frac{\\partial l}{\\partial z''}这里的$\\frac{\\partial z’}{\\partial a}=w_3$，$\\frac{\\partial z’’}{\\partial a}=w_4$，那$\\frac{\\partial l}{\\partial z’}$和$\\frac{\\partial l}{\\partial z’’}$又该怎么算呢？这里先假设我们已经通过某种方法把$\\frac{\\partial l}{\\partial z’}$和$\\frac{\\partial l}{\\partial z’’}$这两项给算出来了，然后回过头去就可以把$\\frac{\\partial l}{\\partial z}$给轻易地算出来 \\frac{\\partial l}{\\partial z}=\\frac{\\partial a}{\\partial z} \\frac{\\partial l}{\\partial a}=\\sigma'(z)[w_3 \\frac{\\partial l}{\\partial z'}+w_4 \\frac{\\partial l}{\\partial z''}]那么由Forward pass和Backward就可以得到$\\frac{\\partial l}{\\partial w_1}$： \\frac{\\partial l}{\\partial w_1}=\\frac{\\partial z}{\\partial w_1} \\frac{\\partial l}{\\partial z}=x_1 \\cdot \\sigma'(z)[w_3 \\frac{\\partial l}{\\partial z'}+w_4 \\frac{\\partial l}{\\partial z''}]换个角度我们可以从另外一个观点来看待这个式子。想象现在有另外一个neuron，它不在我们原来的network里面，在下图中它被画成三角形，这个neuron的input就是$\\frac{\\partial l}{\\partial z’}$和$\\frac{\\partial l}{\\partial z’’}$，那input $\\frac{\\partial l}{\\partial z’}$乘上$w_3$，input $\\frac{\\partial l}{\\partial z’’}$乘上$w_4$，它们两个相加再乘上activation function的微分 $\\sigma’(z)$，就可以得到output $\\frac{\\partial l}{\\partial z}$ 这张图描述了一个新的“neuron”，它的含义跟图下方的表达式是一模一样的，作这张图的目的是为了方便理解。值得注意的是，这里的$\\sigma’(z)$是一个constant常数，它并不是一个function，因为$z$其实在计算forward pass的时候就已经被计算好了（$z=w_1x_1+w_2x_2+b$），在这里$z$是一个固定的值。 所以这个neuron其实跟我们之前看到的sigmoid function是不一样的，它并不是把input通过一个non-linear进行转换，而是直接把input乘上一个constant $\\sigma’(z)$，就得到了output，因此这个neuron被画成三角形，代表它跟我们之前看到的圆形的neuron的运作方式是不一样的，它是直接乘上一个constant(这里的三角形有点像电路里的运算放大器op-amp，它也是乘上一个constant) 现在需要解决的问题是，怎么计算$\\frac{\\partial l}{\\partial z’}$和$\\frac{\\partial l}{\\partial z’’}$这两项，这里需要考虑两种case： case 1：Output Layer 假设蓝色的这个neuron已经是hidden layer的最后一层了，也就是说连接在$z’$和$z’’$后的这两个红色的neuron已经是output layer，它的output就已经是整个network的output了，这个时候计算就比较简单： \\frac{\\partial l}{\\partial z'}=\\frac{\\partial y_1}{\\partial z'} \\frac{\\partial l}{\\partial y_1}其中$\\frac{\\partial y_1}{\\partial z’}$就是output layer的activation function (softmax) 对$z’$的偏微分 而$\\frac{\\partial l}{\\partial y_1}$就是loss对$y_1$的偏微分，它取决于你的loss function是怎么定义的，也就是你的output和target之间是怎么evaluate的，可以是cross entropy或者是mean square error，用不同的定义，$\\frac{\\partial l}{\\partial y_1}$的值就不一样。这样就可以把$l$对$w_1$和$w_2$的偏微分$\\frac{\\partial l}{\\partial w_1}$、$\\frac{\\partial l}{\\partial w_2}$算出来了。 Case 2：Not Output Layer 假设现在红色的neuron并不是整个network的output，那$z’$经过红色neuron的activation function得到$a’$，然后output $a’$和$w_5$、$w_6$相乘并加上一堆其他东西分别得到$z_a$和$z_b$，如下图所示 根据之前的推导证明类比，如果知道$\\frac{\\partial l}{\\partial z_a}$和$\\frac{\\partial l}{\\partial z_b}$，我们就可以计算$\\frac{\\partial l}{\\partial z’}$，如下图所示，借助运算放大器的辅助理解，将$\\frac{\\partial l}{\\partial z_a}$乘上$w_5$和$\\frac{\\partial l}{\\partial z_b}$乘上$w_6$的值加起来再通过op-amp，乘上放大系数$\\sigma’(z’)$，就可以得到output $\\frac{\\partial l}{\\partial z’}$ \\frac{\\partial l}{\\partial z'}=\\sigma'(z')[w_5 \\frac{\\partial l}{\\partial z_a} + w_6 \\frac{\\partial l}{\\partial z_b}] 知道$z’$和$z’’$就可以知道$z$，知道$z_a$和$z_b$就可以知道$z’$，…… ，现在这个过程就可以反复进行下去，直到找到output layer，我们可以算出确切的值，然后再一层一层反推回去。 这个方法听起来挺让人崩溃的，每次要算一个微分的值，都要一路往后走，一直走到network的output，如果写成表达式的话，一层一层往后展开，感觉会是一个很可怕的式子，但是实际上并不是这样做的。 你只要换一个方向，从output layer的$\\frac{\\partial l}{\\partial z}$开始算，你就会发现它的运算量跟原来的network的Feedforward path其实是一样的。假设现在有6个neuron，每一个neuron的activation function的input分别是$z_1$、$z_2$、$z_3$、$z_4$、$z_5$、$z_6$，我们要计算$l$对这些$z$的偏微分，按照原来的思路，我们想要知道$z_1$的偏微分，就要去算$z_3$和$z_4$的偏微分，想要知道$z_3$和$z_4$的偏微分，就又要去计算两遍$z_5$和$z_6$的偏微分，这样做没有效率。 但是，如果反过来先去计算$z_5$和$z_6$的偏微分的话，这个process就会变得有效率。我们先去计算$\\frac{\\partial l}{\\partial z_5}$和$\\frac{\\partial l}{\\partial z_6}$，然后就可以算出$\\frac{\\partial l}{\\partial z_3}$和$\\frac{\\partial l}{\\partial z_4}$，最后就可以算出$\\frac{\\partial l}{\\partial z_1}$和$\\frac{\\partial l}{\\partial z_2}$，而这一整个过程，就可以转化为op-amp运算放大器的那张图 这里每一个op-amp的放大系数就是$\\sigma’(z_1)$、$\\sigma’(z_2)$、$\\sigma’(z_3)$、$\\sigma’(z_4)$，所以整一个流程就是，先快速地计算出$\\frac{\\partial l}{\\partial z_5}$和$\\frac{\\partial l}{\\partial z_6}$，然后再把这两个偏微分的值乘上路径上的weight汇集到neuron上面，再通过op-amp的放大，就可以得到$\\frac{\\partial l}{\\partial z_3}$和$\\frac{\\partial l}{\\partial z_4}$这两个偏微分的值，再让它们乘上一些weight，并且通过一个op-amp，就得到$\\frac{\\partial l}{\\partial z_1}$和$\\frac{\\partial l}{\\partial z_2}$这两个偏微分的值，这样就计算完了，这个步骤，就叫做Backward pass。 以上面这个图中的neural network为例，network共有12个$w$，写一下反向传播求梯度的过程，假如现在要求$\\frac{\\partial l}{\\partial w_1}$和$\\frac{\\partial l}{\\partial w_2}$，由Chain Rule知： \\frac{\\partial l}{\\partial w_1}=\\frac{\\partial l}{\\partial z_1} \\cdot\\frac{\\partial z_1}{\\partial w_1}\\\\ \\frac{\\partial l}{\\partial w_2}=\\frac{\\partial l}{\\partial z_1} \\cdot\\frac{\\partial z_1}{\\partial w_2}Forward Pass： \\frac{\\partial z_1}{\\partial w_1}=x_1\\\\ \\frac{\\partial z_1}{\\partial w_2}=x_2Backward Pass： Step 1： \\frac{\\partial l}{\\partial z_5}=\\frac{\\partial l}{\\partial y_1}\\cdot\\frac{\\partial y_1}{\\partial z_5} \\\\ \\frac{\\partial l}{\\partial z_6}=\\frac{\\partial l}{\\partial y_2}\\cdot\\frac{\\partial y_2}{\\partial z_5} Step 2: \\frac{\\partial l}{\\partial z_3}=\\sigma'(z_3)[w_9 \\frac{\\partial l}{\\partial z_5} + w_{11} \\frac{\\partial l}{\\partial z_6}] \\\\ \\frac{\\partial l}{\\partial z_4}=\\sigma'(z_4)[w_{10} \\frac{\\partial l}{\\partial z_5} + w_{12} \\frac{\\partial l}{\\partial z_6}] Step 3: \\frac{\\partial l}{\\partial z_1}=\\sigma'(z_1)[w_5 \\frac{\\partial l}{\\partial z_3} + w_7 \\frac{\\partial l}{\\partial z_4}] \\\\ \\frac{\\partial l}{\\partial z_2}=\\sigma'(z_2)[w_6 \\frac{\\partial l}{\\partial z_3} + w_8 \\frac{\\partial l}{\\partial z_4}]最后就可以得到损失函数$l$对参数$w_1$，$w_2$的梯度： \\frac{\\partial l}{\\partial w_1}=\\frac{\\partial l}{\\partial z_1} \\cdot\\frac{\\partial z_1}{\\partial w_1}=x_1 \\cdot \\sigma'(z_1)[w_5 \\frac{\\partial l}{\\partial z_3} + w_7 \\frac{\\partial l}{\\partial z_4}] \\\\ \\frac{\\partial l}{\\partial w_2}=\\frac{\\partial l}{\\partial z_1} \\cdot\\frac{\\partial z_1}{\\partial w_2}=x_2 \\cdot \\sigma'(z_1)[w_6 \\frac{\\partial l}{\\partial z_3} + w_8 \\frac{\\partial l}{\\partial z_4}]在做Backward pass的时候，实际上的做法就是建另外一个neural network，本来正向neural network里面的activation function都是sigmoid function，而现在计算Backward pass的时候，就是建一个反向的neural network，它的activation function就是一个运算放大器op-amp，每一个反向neuron的input是loss $l$对后面一层layer的$z$的偏微分$\\frac{\\partial l}{\\partial z}$，output则是loss $l$对这个neuron的$z$的偏微分$\\frac{\\partial l}{\\partial z}$，做Backward pass就是通过这样一个反向neural network的运算，把loss $l$对每一个neuron的$z$的偏微分$\\frac{\\partial l}{\\partial z}$都给算出来。 （注：如果是正向做Backward pass的话，实际上每次计算一个$\\frac{\\partial l}{\\partial z}$，就需要把该neuron后面所有的$\\frac{\\partial l}{\\partial z}$都给计算一遍，会造成很多不必要的重复运算，如果写成code的形式，就相当于调用了很多次重复的函数；而如果是反向做Backward pass，实际上就是把这些调用函数的过程都变成调用“值”的过程，因此可以直接计算出结果，而不需要占用过多的堆栈空间。） Summary最后来总结一下Backpropagation是怎么做的 Forward pass，每个neuron的activation function的$w$前连接的input，就是$\\frac{\\partial z}{\\partial w}$； Backward pass，建一个与原来方向相反的neural network，它的三角形neuron的output就是$\\frac{\\partial l}{\\partial z}$ 把通过forward pass得到的$\\frac{\\partial z}{\\partial w}$和通过backward pass得到的$\\frac{\\partial l}{\\partial z}$乘起来就可以得到$l$对$w$的梯度$\\frac{\\partial l}{\\partial w}$ \\frac{\\partial l}{\\partial w} = \\frac{\\partial z}{\\partial w}|_{forward\\ pass} \\cdot \\frac{\\partial l}{\\partial z}|_{backward \\ pass}","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://nekomoon404.github.io/categories/Deep-Learning/"}],"tags":[{"name":"Backpropagation","slug":"Backpropagation","permalink":"http://nekomoon404.github.io/tags/Backpropagation/"}]},{"title":"DL笔记（4）Brief Introduction of Deep Learning","slug":"ML笔记（4）Brief-Introduction-of-Deep-Learning","date":"2020-07-09T07:24:47.000Z","updated":"2020-07-09T08:24:47.000Z","comments":true,"path":"2020/07/09/ML笔记（4）Brief-Introduction-of-Deep-Learning/","link":"","permalink":"http://nekomoon404.github.io/2020/07/09/ML%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89Brief-Introduction-of-Deep-Learning/","excerpt":"","text":"首先简单了解一下Deep Learning 深度学习的发展历程： 1958：Perceptron(linear model)，感知机的提出 和Logistic Regression类似，只是少了sigmoid的部分 1969：Perceptron has limitation，from MIT 1980s：Multi-layer Perceptron，多层感知机 和今天的DNN很像 1986：Backpropagation，反向传播 Hinton propose的Backpropagation 存在problem：通常超过3个layer的neural network，就train不出好的结果 1989: 1 hidden layer is “good enough”，why deep？ 有人提出一个理论：只要neural network有一个hidden layer，它就可以model出任何的function，所以根本没有必要叠加很多个hidden layer，所以Multi-layer Perceptron的方法又坏掉了，这段时间Multi-layer Perceptron这个东西是受到抵制的 2006：RBM initialization(breakthrough)：Restricted Boltzmann Machine，受限玻尔兹曼机 Deep learning -&gt; another Multi-layer Perceptron ？在当时看来，它们的不同之处在于在做gradient descent的时候选取初始值的方法如果是用RBM，那就是Deep learning；如果没有用RBM，就是传统的Multi-layer Perceptron 那实际上呢，RBM用的不是neural network base的方法，而是graphical model，后来大家试验得多了发现RBM并没有什么太大的帮助，因此现在基本上没有人使用RBM做initialization了 RBM最大的贡献是，它让大家重新对Deep learning这个model有了兴趣(石头汤的故事) 2009：GPU加速的发现 2011：start to be popular in speech recognition，语音识别领域 2012：win ILSVRC image competition，Deep learning开始在图像领域流行开来 1.Neural Network实际上，Deep learning跟machine learning一样，也是“大象放进冰箱”的三个步骤。在Deep learning的step1里定义的function，就是neural network。 把多个Logistic Regression前后connect在一起，然后把一个Logistic Regression称之为neuron，整个称之为neural network。我们可以用不同的方法连接这些neuron，就可以得到不同的structure，neural network里的每一个Logistic Regression都有自己的weight和bias，这些weight和bias集合起来，就是这个network的parameter，我们用$\\theta$来描述。 Fully Connect Feedforward Network 这些神经元Neuron之间的连接方式需要我们自己去设计的，最常见的连接方式叫做Fully Connect Feedforward Network(全连接前馈网络)。如果一个neural network的参数weight和bias已知的话，它就是一个function，它的input是一个vector，output是另一个vector，这个vector里面放的是样本点的feature，vector的dimension就是feature的个数 如果现在我们还不知道网络中每个神经元中的参数，只是定出了这个network的structure，只是决定好这些neuron该怎么连接在一起，这样的一个network structure其实是定义了一个function set(model)，给这个network设不同的参数，它就变成了不同的function，把这些可能的function集合起来，就得到了一个function set。用neural network决定function set的时候，这个function set是比较大的，它包含了很多原来你做Logistic Regression、做linear Regression所没有办法包含的function。 上图中，每一排表示一个layer，每个layer里面的每一个球都代表一个neuron layer和layer之间neuron是两两互相连接的，layer 1的neuron output会连接给layer 2的每一个neuron作为input 对整个neural network来说，它需要一个input，这个input就是一个feature vector，而对layer 1的每一个neuron来说，它的input就是input layer的每一个dimension 最后那个layer L，由于它后面没有接其它东西了，所以它的output就是整个network的output 这里每一个layer都是有名字的 input layer，输入层(严格来说input layer其实不是一个layer，它跟其他layer不一样，不是由neuron所组成的) output layer，输出层 中间层，叫做hidden layer，隐藏层 每一个neuron里面的sigmoid function，在Deep Learning中被称为activation function(激励函数)，事实上它不见得一定是sigmoid function，还可以是其他function(sigmoid function是从Logistic Regression迁移过来的，现在已经较少在Deep learning里使用了) 有很多层layers的neural network，被称为DNN(Deep Neural Network) 上图中的神经网络中layer和layer之间，所有的neuron都是两两连接，所以它叫Fully connected network；因为现在传递的方向是从layer 1-&gt;2-&gt;3，由前向后传，所以它叫做Feedforward network。 那所谓的Deep其实就是指有很多层hidden layer，具体的层数并没有规定。但好像现在只要是neural network base的方法，都被称为Deep Learning（苦笑）。 Matrix Operation network的运作过程，我们通常会用Matrix Operation来表示，以下图为例，假设第一层hidden layers的两个neuron，它们的weight分别是$w_1=1,w_2=-2,w_1’=-1,w_2’=1$，那就可以把它们排成一个matrix：$\\begin{bmatrix}1 \\ \\ \\ -2\\\\ -1 \\ \\ \\ 1 \\end{bmatrix}$，而我们的input又是一个2*1的vector：$\\begin{bmatrix}1\\-1 \\end{bmatrix}$，将$w$和$x$相乘，再加上bias的vector：$\\begin{bmatrix}1\\\\0 \\end{bmatrix}$，就可以得到这一层的vector $z$，再经过activation function得到这一层的output：(activation function可以是很多类型的function，这里还是用Logistic Regression迁移过来的sigmoid function作为运算) \\sigma(\\begin{bmatrix}1 \\ \\ \\ -2\\\\ -1 \\ \\ \\ 1 \\end{bmatrix} \\begin{bmatrix}1\\\\-1 \\end{bmatrix}+\\begin{bmatrix}1\\\\0 \\end{bmatrix})=\\sigma(\\begin{bmatrix}4\\\\-2 \\end{bmatrix})=\\begin{bmatrix}0.98\\\\0.12 \\end{bmatrix} 这里我们把所有的变量都以matrix的形式表示出来，注意$W^i$的matrix，每一行对应的是一个neuron的weight，行数就是neuron的个数，而input $x$，bias $b$和output $y$都是一个列向量，行数就是feature的个数(也是neuron的个数，neuron的本质就是把feature transform到另一个space)。 写成矩阵运算的好处是不仅在于形式简洁，还在于可以用GPU加速，GPU对matrix的运算是比CPU要来的快的，所以写neural network的时候，习惯把它写成matrix operation，然后call GPU来加速它。 Output Layer 我们可以把hidden layers这部分，看做是一个feature extractor(特征提取器)，这个feature extractor就r代替了我们之前手动做feature engineering，feature transformation这些事情，经过这个feature extractor得到的$x_1,x_2,…,x_k$就可以被当作一组新的feature。 output layer做的事情，其实就是把它当做一个Multi-class classifier，它是拿经过feature extractor转换后的那一组比较好的feature(能够被很好地separate)进行分类的，如果我们想把把output layer看做是一个Multi-class classifier，那么可以在最后一个layer加上softmax。 2.Example Application: Handwriting Digit Recognition这里举一个手写数字识别的例子，input是一张image，对机器来说一张image实际上就是一个vector，假设这是一张16*16的image，那它有256个pixel，对machine来说，它是一个256维的vector，image中的每一个pixel都对应到vector中的一个dimension，我们把黑色的pixel的值设为1，白色的pixel的值设为0。 而neural network的output，如果在output layer使用了softmax，那它的output就是一个突出极大值的Probability distribution，假设output是10维(10个数字，0~9)，这个output的每一维都对应到它可能是某一个数字的几率，实际上这个neural network的作用就是计算这张image成为10个数字的几率各自有多少，几率最大(softmax突出极大值的意义所在)的那个数字，就是机器的预测值。 Step 1：Neural Network 在这个手写字体识别的demo里，我们需要的就是一个function，这个function的input是一个256的vector，output是一个10维的vector，这个function就是neural network(这里我们用简单的Feedforward network)。 input固定为256维，output固定为10维的feedforward neural network，实际上这个network structure就已经确定了一个function set(model)的形状，在这个function set里的每一个function都可以拿来做手写数字识别，接下来我们要做的事情是用gradient descent去计算出一组参数，挑一个最适合拿来做手写数字识别的function。（input、output的dimension，加上network structure，就可以确定一个model的形状，前两个是容易知道的，而确定这个network的structure则是整个Deep Learning中最为关键的步骤） 所以这里很重要的一件事情是，我们要对network structure进行design，之前在做Logistic Regression或者是linear Regression的时候，我们对model的structure是没有什么好设计的，但是对neural network来说，我们现在已知的constraint只有input是256维，output是10维，而中间要有几个hidden layer，每个layer要有几个neuron，都是需要我们自己去设计的，它们近乎是决定了function set长什么样子。如果network structure设计的很差，这个function set里面根本就没有好的function，那就会像大海捞针一样，结果针并不在海里(=´ω｀=) input 256维，output 10维，以及自己design的network structure =》function set(model) Step 2：Goodness of function 定义一个function的好坏，由于我们做的是一个Multi-class classification，所以image为数字1的label “1”告诉我们，现在的target是一个10维的vector，只有在第一维对应数字1的地方，它的值是1，其他都是0 input这张image的256个pixel，通过这个neural network之后，会得到一个output，称之为y；而从这张image的label中转化而来的target，称之为$\\hat{y}$，有了output $y$和target $\\hat{y}$之后，要做的事情是计算它们之间的cross entropy(交叉熵)，这个做法跟我们之前做Multi-class classification的时候是一模一样的 Cross \\ Entropy :l(y,\\hat{y})=-\\sum\\limits_{i=1}^{10}\\hat{y}_i lny_iStep 3：Pick the best function 接下来就去调整参数，让这个cross entropy越小越好，当然整个training data里面不会只有一笔data，你需要把所有data的cross entropy都sum起来，得到一个total loss $L=\\sum\\limits_{n=1}^Nl^n$，得到loss function之后要做的就是找一组network的parameters：$\\theta^*$，它可以minimize这个total loss，这组parameter对应的function就是我们最终训练好的model。 那如何去找这个使total loss minimize的$\\theta^$呢？我们依然可以使用之前学过的——*Gradient Descent 实际上在deep learning里面用gradient descent，跟在linear regression里面使用完全没有什么差别，只是function和parameter变得更复杂了而已，其他事情都是一模一样的。现在的$\\theta$里面是一大堆的weight、bias参数，先random找一个初始值，接下来去计算每一个参数对total loss的偏微分，把这些偏微分全部集合起来，就叫做gradient，有了这些偏微分以后，就可以更新所有的参数，都减掉learning rate乘上偏微分的值，这个process反复进行下去，最终找到一组好的参数，就做完deep learning的training了。 Design network structure V.s. Feature Engineering 其实network structure的design是一件蛮难的事情，我们到底要怎么决定layer的数目和每一个layer的neuron的数目呢？其实这个只能够凭着经验和直觉、多方面的尝试，有时候甚至会需要一些domain knowledge(专业领域的知识)，从非deep learning的方法到deep learning的方法，并不是说machine learning比较简单，而是我们把一个问题转化成了另一个问题。 本来不是deep learning的model，要得到一个好的结果，往往需要做feature engineering(特征工程)，也就是做feature transform，然后找一组好的feature；一开始学习deep learning的时候，好像会觉得deep learning的layers之间也是在做feature transform，但实际上在做deep learning的时候，往往不需要一个好的feature ，比如说在做影像辨识的时候，你可以把所有的pixel直接丢进去，但是在过去做图像识别，你是需要对图像抽取出一些人定的feature出来的，这件事情就是feature transform，但是有了deep learning之后，你完全可以直接丢pixel进去硬做 但是，今天deep learning制造了一个新的问题，它所制造的问题就是，你需要去design network的structure，所以你的问题从本来的如何抽取feature转化成怎么design network structure，所以deep learning是不是真的好用，取决于你觉得哪一个问题比较容易。 如果是影像辨识或者是语音辨识的话，design network structure可能比feature engineering要来的容易，因为，虽然我们人都会看、会听，但是这件事情，它太过潜意识了，它离我们意识的层次太远，我们无法意识到，我们到底是怎么做语音辨识这件事情，所以对人来说，你要抽一组好的feature，让机器可以很方便地用linear的方法做语音辨识，其实是很难的，因为人根本就不知道好的feature到底长什么样子；所以还不如design一个network structure，或者是尝试各种network structure，让machine自己去找出好的feature，这件事情反而变得比较容易，对影像来说也是一样的。 3.Why Deep？前面在介绍Deep Learning发展历程的时候有提到“只要neural network有一个hidden layer，它就可以model出任何的function”，那么是不是意味着我们并不不需要费尽心思地设计”Deep”的Network，只要用有一层layer的Network就可以解决问题了？显然不是这样的（要不然Deep Learning怎么发展到今天啊喂ヽ(#`Д´)ﾉ┌┛〃），那么Deep好在哪呢？ Fat + Short v.s. Thin + Tall论上只要这一层里neuron的数目足够多，有足够的参数，就可以表示出任何函数。那用大量的data加上参数足够多的model就可以实现这个效果，那为什么一定要用DNN呢？我们完全可以用一层的shallow neural network来做同样的事情，其实“Deep”和”Shallow”这两种结构的Network的performance是会不一样的，这里我们就拿下面这两种结构的network做一下比较。 如果要给Deep和Shallow的model一个公平的评比，就要调整它们的形状，让它们的参数是一样多的，在这个情况下Shallow的model就会是一个“矮胖”的model，Deep的model就会是一个“瘦高”的model。有学者在这个公平的评比之下，得到的结果如下图所示。左侧表示的是deep network的情况，右侧表示的是shallow network的情况，为了保证两种情况下参数的数量是比较接近的，因此设置了右侧1*3772和1*4634这两种size大小，它们分别对应比较左侧5*2k和7*2k这两种情况下的network(注意参数数目和neuron的数目并不是等价的)。 发现，在参数数量接近的情况下，只有1层的network，它的error rate是远大于好几层的network的；1层的shallow network的performance甚至都比不过很多参数比它少但层数比它多的deep network。根据上面的对比可知，deep learning显然是在结构上存在着某种优势，不然无法解释它会比参数数量相同的shallow learning表现得更好这个现象。 ModularizationDNN结构一个很大的优势是，Modularization(模块化)，它用的是结构化的架构。就像写程序一样，shallow network实际上就是把所有的程序都写在了同一个main函数中，所以它去检测不同的class使用的方法是相互独立的；而deep network则是把整个任务分为了一个个小任务，每个小任务又可以不断细分下去，以形成modularization。 在DNN的架构中，实际上每一层layer里的neuron都像是在解决同一个级别的任务，它们的output作为下一层layer处理更高级别任务的数据来源，低层layer里的neuron做的是对不同小特征的检测，高层layer里的neuron则根据需要挑选低层neuron所抽取出来的不同小特征，去检测一个范围更大的特征；neuron就像是一个个classifier ，后面的classifier共享前面classifier的参数。 这样做的好处是，低层的neuron输出的信息可以被高层不同的neuron重复使用，而并不需要像shallow network一样，每次在用到的时候都要重新去检测一遍，因此大大降低了程序的复杂度。 这里举一个分类的例子，我们要把input的人物分为四类：长头发女生、长头发男生、短头发女生、短头发男生。如果按照shallow network的想法，我们分别独立地train四个classifier(其实就相当于训练四个独立的model)，然后就可以解决这个分类的问题；但是这里有一个问题，长头发男生的data是比较少的，没有太多的training data，所以，你train出来的classifier就比较weak，去detect长头发男生的performance就比较差。 其实我们的input并不是没有关联的，长头发的男生和长头发的女生都有一个共同的特征，就是长头发，因此如果我们分别独立地训练四个model作为分类器，实际上就是忽视了这个共同特征，也就是没有高效地用到data提供的全部信息，这恰恰是shallow network的弊端。 而利用modularization的思想，使用deep network的架构，我们可以训练一个model作为分类器就可以完成所有的任务，我们可以把整个任务分为两个子任务： Classifier1：检测是男生或女生 Classifier2：检测是长头发或短头发 虽然长头发的男生data很少，但长头发的人的data就很多，这样就真正做到了充分、高效地利用数据，最终的Classifier再根据Classifier1和Classifier2提供的信息给出四类人的分类结果。 经过层层layer的任务分解，其实每一个Classifier要做的事情都是比较简单的，又因为这种分层的、模组化的方式充分利用了data，并提高了信息利用的效率，所以只要用比较少的training data就可以把结果train好。 deep -&gt; modularization 做modularization的好处是把原来比较复杂的问题变得简单，比如原来的任务是检测一个长头发的女生，但现在你的任务是检测长头发和检测性别，而当检测对象变简单的时候，就算training data没有那么多，我们也可以把这个task做好，并且所有的classifier都用同一组参数检测子特征，提高了参数使用效率，这就是modularization的作用。由于deep learning的deep就是在做modularization这件事，所以它需要的training data反而是比较少的，这可能会跟你的认知相反，AI=big data+deep learning，但deep learning其实是为了解决less data的问题才提出的。 以图像识别为例，每一个neuron其实就是一个basic的classifier： 第一层neuron，它是一个最basic的classifier，检测的是颜色、线条这样的小特征 第二层neuron是比较复杂的classifier，它用第一层basic的classifier的output当作input，也就是把第一层的classifier当作module，利用第一层得到的小特征分类出不同样式的花纹 而第三层的neuron又把第二层的neuron当作它module，利用第二层得到的特征分类出蜂窝、轮胎、人 以此类推 这边要强调的是，在做deep learning的时候，怎么做模块化这件事情是machine自动学到的，也就是说，第一层要检测什么特征、第二层要检测什么特征…这些都不是人为指定的，人只定好有几层layer、每层layer有几个neuron，剩下的事情都是machine自己学到的。 传统的机器学习算法，是人为地根据domain knowledge指定特征来进行提取，这种指定的提取方式，甚至是提取到的特征，也许并不是实际最优的，所以它的识别成功率并没有那么高；但是在Deep Learning中，提取什么特征、怎么提取这件事让机器自己去学，它所提取的就会是那个最优解，因此识别成功率普遍会比人为指定要来的高。 后面老师举了一个DNN在语音识别领域的例子，传统的HMM-GMM方法是默认把所有的phone或者state都看做是无关联的，对它们分别训练independent model，没有充分利用data提供的信息。而DNN的做法是把所有的state通通用同一个model来做分类，比较lower的layer会先观察人是用什么样的方式在发这个声音，人的舌头位置应该在哪里，是高是低，是前是后；接下来的layer再根据这个结果，去决定现在的发音是属于哪一个state或哪一个phone。这些lower的layer是一个人类发音方式的detector，而所有phone的检测都share这同一组detector的结果，因此最终的这些classifier是share了同一组用来detect发音方式的参数，这就做到了模块化，同一个参数被更多的地方share，因此显得更有效率。 Deep is betterUniversality Theorem告诉我们任何的continuous的function都可以用一层足够宽的neural network来实现，在90年代，这是很多人放弃做deep learning的一个原因。但是这个理论只告诉了我们可能性，却没有说明这件事的效率问题；根据上面的几个例子我们已经知道，只用一个hidden layer来描述function其实是没有效率的；当你用multi-layer，用hierarchy structure来描述function的时候，才会是比较有效率的。 老师分别用逻辑电路和剪窗花的小例子来说明”Deep”相比于”Shallow”的优势，这里就不详细展开了。下面是老师做的一个小例子，左边的图是training data，右边则是1层hidden layer与3层hidden layer的不同network的情况对比，这里已经控制它们的参数数量趋于相同，试验结果是，当training data为10w笔的时候，两个network学到的样子是比较接近原图的，而如果只给2w笔training data，1层hidden layer的情况就完全崩掉了，而3层hidden layer的情况会比较好一些。 End-to-end Learning所谓的End-to-end learning 端到端的学习，指的是只给model input和output，而不告诉它中间每一个function要怎么分工，让它自己去学会知道在生产线的每一站，自己应该要做什么事情；在DNN里，就是叠一个很深的neural network，每一层layer就是生产线上的一个站，我们不需要告诉机器生产线上的每一站是干什么的，而是是让机器自己去完成整条生产线的学习。 相对于深度学习，传统机器学习的流程往往由多个独立的模块组成，比如在一个典型的自然语言处理（Natural Language Processing）问题中，包括分词、词性标注、句法分析、语义分析等多个独立步骤，每个步骤是一个独立的任务，其结果的好坏会影响到下一步骤，从而影响整个训练的结果，这是非端到端的。 而深度学习模型在训练过程中，从输入端（输入数据）到输出端会得到一个预测结果，与真实结果相比较会得到一个误差，这个误差会在模型中的每一层传递（反向传播），每一层的表示都会根据这个误差来做调整，直到模型收敛或达到预期的效果才结束，这是端到端的。 CNN就是比较典型的end2end模型。在图像分类里输入image各通道像素，输出图像类别。 相比于非end2end，conv层的卷积核可以充当feature extractor部分而不需要额外的工作去做特征工程的内容。尽管每一层需要自己设计，但如何得到feature并不需要额外的操作。 End-to-end Learning在语音识别上也有很好的应用，在传统的Speech Recognition里，只有最后GMM这个蓝色的block，才是由training data学出来的，前面绿色的“生产线”部分都是由过去的“五圣先贤”(°ω°｣∠)_ 手动制订出来的，这些function其实是很有效的，可以说是增一分则太肥，减一分则太瘦这样子，以至于在这个阶段卡了将近20年。后来有了deep learning，就可以用neural network把DCT、log这些部分取代掉，甚至从spectrogram开始都拿deep neural network取代掉，也可以得到更好的结果，如果你分析DNN的weight，它其实可以自动学到要做filter bank这件事情(filter bank是模拟人类的听觉器官所制定出来的filter)。 Complex Task有时候我们会遇到非常复杂的task： Very similar input, but different output Very different input, but similar output 如果你的network只有一层的话，就只能做简单的transform，没有办法把一样的东西变得很不一样，把不一样的东西变得很像；如果要实现这些，就需要做很多层次的转换，就像前面那个剪窗花的例子，在二维、三维空间上看起来很难辨别，但是到了高维空间就完全有可能把它们给辨别出来。 这里以MNIST手写数字识别为例，展示一下DNN中，在高维空间上对这些Complex Task的处理能力。如果把28*28个pixel组成的vector投影到二维平面上就像左上角所示，你会发现4跟9的pixel几乎是叠在一起的，因为4跟9很像，都是一个圈圈再加一条线，所以如果你光看input的pixel的话，4跟9几乎是叠在一起的，你几乎没有办法把它分开。但是，等到第二个、第三个layer的output，就会发现4、7、9逐渐就被分开了，这也是Deep Learning的”Deep”的一个优势所在。","categories":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://nekomoon404.github.io/categories/Deep-Learning/"}],"tags":[{"name":"Deep Neural Network","slug":"Deep-Neural-Network","permalink":"http://nekomoon404.github.io/tags/Deep-Neural-Network/"}]},{"title":"ML笔记（3）Logistic_Regression","slug":"ML笔记（3）Logistic-Regression","date":"2020-07-08T10:48:14.000Z","updated":"2020-07-08T12:49:03.747Z","comments":true,"path":"2020/07/08/ML笔记（3）Logistic-Regression/","link":"","permalink":"http://nekomoon404.github.io/2020/07/08/ML%E7%AC%94%E8%AE%B0%EF%BC%883%EF%BC%89Logistic-Regression/","excerpt":"","text":"回顾 Review 在上一节课 Classification中，讨论了如何通过样本点的均值$u$和协方差$\\Sigma$来计算$P(C_1),P(C_2),P(x|C_1),P(x|C_2)$，进而利用$P(C_1|x)=\\frac{P(C_1)P(x|C_1)}{P(C_1)P(x|C_1)+P(C_2)P(x|C_2)}$计算得到新的样本点$x$属于class 1的概率，由于是二元分类，属于class 2的概率$P(C_2|x)=1-P(C_1|x)$。 然后推导了$P(C_1|x)=\\sigma(z)=\\frac{1}{1+e^{-z}}$，并且在Gaussian distribution下考虑class 1和class 2共用$\\Sigma$，可以得到一个线性的z(其实很多其他的Probability model经过化简以后也都可以得到同样的结果) P_{w,b}(C_1|x)=\\sigma(z)=\\frac{1}{1+e^{-z}} \\\\ z=w\\cdot x+b=\\sum\\limits_i w_ix_i+b \\\\从上式中我们可以看出，现在这个model(function set)是受$w$和$b$控制的，因此我们不必要再去像前面一样计算那些与概率相关的东西，而是直接计算$w$和$b$，用这个全新的由$w$和$b$决定的model——Logistic Regression逻辑回归。 1.Three Steps of Logistic Regression现在来用机器学习的“三步走”分析一下逻辑回归。 Step 1: Function Set 由所有不同的$w$和$b$组成的函数的集合就是Logistic Regression的Function set。 $w_i$：weight，$b$：bias，$\\sigma(z)$：sigmoid function，$x_i$：input Step 2：Goodness of a function 现在我们有N笔Training data，每一笔data都要标注它是属于哪一个class。假设这些Training data是从我们定义的posterior Probability中产生的(后置概率，某种意义上就是概率密度函数)，而w和b就决定了这个posterior Probability，那么就可以去计算某一组w和b去产生这N笔Training data的概率，利用极大似然估计的思想，求出使得似然函数取最大值时的那组参数$w^$和$b^$。 似然函数只需要将每一个点产生的概率相乘即可，注意，这里假定是二元分类，class 2的概率为1减去class 1的概率。 由于$L(w,b)$是乘积项的形式，为了方便计算，我们将上式做个变换，求$L$的最大值相当于求$-\\ln L$的最小值： \\begin{split} &w^*,b^*=\\arg \\max\\limits_{w,b} L(w,b)=\\arg\\min\\limits_{w,b}(-\\ln L(w,b)) \\\\ &\\begin{equation} \\begin{split} -\\ln L(w,b)=&-\\ln f_{w,b}(x^1)\\\\ &-\\ln f_{w,b}(x^2)\\\\ &-\\ln(1-f_{w,b}(x^3))\\\\ &\\ -... \\end{split} \\end{equation} \\end{split}由于class 1和class 2的概率表达式不统一，上面的式子无法写成统一的形式，为了统一格式，这里将Logistic Regression里的所有Training data都打上0和1的标签，即output $\\hat{y}=1$代表class 1，output $\\hat{y}=0$代表class 2，于是上式进一步改写成： \\begin{split} -\\ln L(w,b)=&-[\\hat{y}^1 \\ln f_{w,b}(x^1)+(1-\\hat{y}^1)ln(1-f_{w,b}(x^1))]\\\\ &-[\\hat{y}^2 \\ln f_{w,b}(x^2)+(1-\\hat{y}^2)ln(1-f_{w,b}(x^2))]\\\\ &-[\\hat{y}^3 \\ln f_{w,b}(x^3)+(1-\\hat{y}^3)ln(1-f_{w,b}(x^3))]\\\\ &\\ -... \\end{split}现在已经有了统一的格式，我们就可以把要minimize的对象写成一个summation的形式： -\\ln L(w,b)=\\sum\\limits_n -[\\hat{y}^n \\ln f_{w,b}(x^n)+(1-\\hat{y}^n) \\ln(1-f_{w,b}(x^n))]这里$x^n$表示第n个样本点，$\\hat{y}^n$表示第n个样本点的class标签(1表示class 1,0表示class 2)，最终这个summation的形式，里面其实是两个Bernouli distribution(两点分布)的cross entropy(交叉熵)。 假设有如上图所示的两个distribution $p$和$q$，它们的交叉熵就是$H(p,q)=-\\sum\\limits_{x} p(x) \\ln (q(x))$，这也就是之前的推导中在$-\\ln L(w,b)$前加一个负号的原因。 cross entropy 交叉熵的含义是表达这两个distribution有多接近，如果$p$和$q$这两个distribution一模一样的话，那它们算出来的cross entropy就是0，而这里$f(x^n)$表示function的output，$\\hat{y}^n$表示预期 的target，因此交叉熵实际上表达的是希望这个function的output和它的target越接近越好 总之，我们要找的参数实际上就是： w^*,b^*=\\arg \\max\\limits_{w,b} L(w,b)=\\arg\\min\\limits_{w,b}(-\\ln L(w,b)=\\sum\\limits_n -[\\hat{y}^n \\ln f_{w,b}(x^n)+(1-\\hat{y}^n) \\ln(1-f_{w,b}(x^n))]step 3：Find the best function 实际上就是去找到使loss function即交叉熵之和最小的那组参数$w^,b^$，这里依然可以使用gradient descent的方法。 sigmoid function $\\sigma(z)=\\frac{1}{1+e^{-z}}$的微分可以直接作为公式记下来：$\\frac{\\partial \\sigma(z)}{\\partial z}=\\sigma(z)(1-\\sigma(z))$。 先计算$-\\ln L(w,b)=\\sum\\limits_n -[\\hat{y}^n \\ln f_{w,b}(x^n)+(1-\\hat{y}^n) \\ln(1-f_{w,b}(x^n))]$对$w_i$的偏微分，这里$\\hat{y}^n$和$1-\\hat{y}^n$是常数先不用管它，只需要分别求出$\\ln f_{w,b}(x^n)$和$\\ln (1-f_{w,b}(x^n))$对$w_i$的偏微分即可，整体推导过程如下： 进一步化简得： 我们发现最终的结果竟然异常的简洁，gradient descent每次update只需要做： w_i=w_i-\\eta \\sum\\limits_{n}-(\\hat{y}^n-f_{w,b}(x^n))x_i^n那这个式子到底代表着什么意思呢？现在你的update取决于三件事： learning rate，是你自己设定的； $x_i$，来自于data； $\\hat{y}^n-f_{w,b}(x^n)$，代表function的output跟理想target的差距有多大，如果离目标越远，update的步伐就要越大。 通过上面的分析，我们可以将Logistic Regression和Linear Regression的三个步骤作一个对比： 2.Logistic Regression + Square error？这里可能会有一个疑惑，为什么Logistic Regression的loss function不能像linear Regression一样用square error来表示呢？我们试着用square error来定义Loss function重新写一下Logistic Regression的三个步骤： 这样就会遇到一个问题：如果第n个点的目标target是class 1，则$\\hat{y}^n=1$，此时如果function的output $f_{w,b}(x^n)=1$的话，说明现在离target很接近了，$f_{w,b}(x)-\\hat{y}$这一项是0，于是得到的微分$\\frac{\\partial L}{\\partial w_i}$会变成0，这件事情是很合理的；但是当function的output $f_{w,b}(x^n)=0$的时候，说明离target还很遥远，但是由于在step3中求出来的update表达式中有一个$f_{w,b}(x^n)$，因此这个时候也会导致得到的微分$\\frac{\\partial L}{\\partial w_i}$变成0，这样无论function的输出是1还是0，微分项都会是0，导致在做gradient descent时参数无法获得更新。如果举class 2的例子，得到的结果与class 1是一样的。 如果我们把参数的变化对total loss作图的话，loss function选择cross entropy或square error，参数的变化跟loss的变化情况可视化出来如下所示：(黑色的是cross entropy，红色的是square error) 假设中心点就是距离目标很近的地方，如果是cross entropy的话，距离目标越远，微分值就越大，参数update的时候变化量就越大，迈出去的步伐也就越大。但当选择square error的时候，过程就会很卡，因为距离目标远的时候，微分也是非常小的，移动的速度是非常慢的。我们之前提到过，实际操作的时候，当gradient接近于0的时候，其实就很有可能会停下来，因此使用square error很有可能在一开始的时候就卡住不动了，而且这里也不能随意地增大learning rate，因为在做gradient descent的时候，你的gradient接近于0，有可能离target很近也有可能很远，因此不知道learning rate应该设大还是设小。 综上，尽管square error可以使用，但是会出现update十分缓慢的现象，而使用cross entropy可以让你的Training更顺利。 3. Discriminative v.s. Generativesame model but different currency Logistic Regression的方法，称之为Discriminative的方法；而上节课中用Gaussian来描述posterior Probability来建立Generative model的方法，称之为Generative的方法。 实际上它们用的model(function set)是一模一样的，都是$P(C_1|x)=\\sigma(w\\cdot x+b)$，如果是用Logistic Regression的话，可以用gradient descent的方法直接去把b和w找出来；如果是用Generative model的话，我们要先去算$u_1,u_2,\\Sigma^{-1}$，然后算出b和w。 但是用这两种方法得到的b和w是不同的，尽管我们的function set是同一个，但是由于做了不同的假设，最终从同样的Training data里找出来的参数会是不一样的。 在Logistic Regression里面，我们没有做任何实质性的假设，没有对Probability distribution有任何的描述，我们就是单纯地去找b和w(推导过程中的假设只是便于理解和计算，对实际结果没有影响)。而在Generative model里面，我们对Probability distribution是有实质性的假设的，之前我们假设的是Gaussian(高斯分布)，甚至假设在相互独立的前提下是否可以是naive bayes(朴素贝叶斯)，根据这些假设我们才找到最终的b和w 下图是宝可梦属性分类例子中Generative model和discriminative model的预测结果比较： 实际上Discriminative的方法常常会比Generative的方法表现得更好，这里举一个简单的例子来解释一下： Testing data的两个feature都是1，凭直觉来说会认为它肯定是class 1。但是如果用naive bayes的方法(朴素贝叶斯假设所有的feature相互独立，方便计算)，却会得到相反的结果： Discriminative model在data充足的情况下，它训练出来的model的准确率一般是比Generative model要来的高的。但是Generative的方法也有它自己的优势：它对data的依赖并没有像discriminative model那么严重，在data数量少或者data本身就存在noise的情况下受到的影响会更小，而它还可以做到Prior部分与class-dependent部分分开处理，如果可以借助其他方式提高Prior model的准确率，对整一个model是有所帮助的(比如前面提到的语音辨识)。 4.Multi-class Classification之前讲的都是二元分类的情况，这里讨论一下多元分类问题，其原理的推导过程与二元分类基本一致 假设有三个class：$C_1,C_2,C_3$，每一个class都有自己的weight和bias，这里$w_1,w_2,w_3$分布代表三个vector，$b_1,b_2,b_3$分别代表三个const，input x也是一个vector softmax的意思是对最大值做强化，因为在做第一步的时候，对$z$取exponential会使大的值和小的值之间的差距被拉得更开，也就是强化大的值。 我们把$z_1,z_2,z_3$丢进一个softmax的function，softmax做的事情是这样三步： 取exponential，得到$e^{z_1},e^{z_2},e^{z_3}$ 把三个exponential累计求和，得到total sum=$\\sum\\limits_{j=1}^3 e^{z_j}$ 将total sum分别除去这三项(归一化)，得到$y_1=\\frac{e^{z_1}}{\\sum\\limits_{j=1}^3 e^{z_j}}$、$y_2=\\frac{e^{z_2}}{\\sum\\limits_{j=1}^3 e^{z_j}}$、$y_3=\\frac{e^{z_3}}{\\sum\\limits_{j=1}^3 e^{z_j}}$ 原来的output $z$可以是任何值，但是做完softmax之后，output $y_i$的值一定是介于0~1之间，并且其和一定是1，$\\sum\\limits_i y_i=1$。softmax的output，就是拿来当z的posterior probability。 假设我们用的是Gaussian distribution(共用covariance)，经过一般推导以后可以得到softmax的function，而从information theory也可以推导出softmax function，Maximum entropy本质内容和Logistic Regression是一样的，它是从另一个观点来切入为什么我们的classifier长这样子。 multi-class classification的过程：如下图所示，input $x$经过三个式子分别生成$z_1,z_2,z_3$，经过softmax转化成output $y_1,y_2,y_3$，它们分别是这三个class的posterior probability，由于summation=1，因此做完softmax之后就可以把y的分布当做是一个probability contribution，我们在训练的时候还需要有一个target，因为是三个class，output是三维的，对应的target也是三维的，为了满足交叉熵的条件，target $\\hat{y}$也必须是probability distribution，这里我们不能使用1,2,3作为class的区分，为了保证所有class之间的关系是一样的，这里使用类似于one-hot编码的方式，即: \\hat{y}= \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}_{x \\ ∈ \\ class 1} \\hat{y}= \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}_{x \\ ∈ \\ class 2} \\hat{y}= \\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix}_{x \\ ∈ \\ class 3} 这个时候就可以计算一下output $y$和 target $\\hat{y}$之间的交叉熵，即$-\\sum\\limits_{i=1}^3 \\hat{y}_i \\ln y_i$，同二元分类一样，多元分类问题也是通过极大似然估计法得到最终的交叉熵表达式的，这里不再赘述。 Limitation of Logistic Regression Logistic Regression其实有很强的限制，给出下图的例子中的Training data，想要用Logistic Regression对它进行分类，其实是做不到的。 因为Logistic Regression在两个class之间的boundary就是一条直线，但是在这个平面上无论怎么画直线都不可能把图中的两个class分隔开来。关于这种不可分问题，还有几个点最多可以分几类的问题的深入分析可以看林轩田老师的《机器学习基石》课程的Lecture 5和Lecture 6。 Feature Transformation 如果坚持要用Logistic Regression的话，可以使用Feature Transformation的方法，原来的feature分布不好划分，那我们可以将之转化以后，找一个比较好的feature space，让Logistic Regression能够处理。 比如我们可以这样做：假设这里定义$x_1’$是原来的点到$\\begin{bmatrix}0\\\\0 \\end{bmatrix}$之间的距离，$x_2’$是原来的点到$\\begin{bmatrix}1\\\\ 1 \\end{bmatrix}$之间的距离，重新映射之后如下图右侧(红色两个点重合)，此时Logistic Regression就可以把它们划分开来。 但麻烦的是，我们通常并不知道怎么做到有效的feature Transformation，如果在这上面花费太多的时间就得不偿失了，于是我们会希望这个Transformation是机器自己产生的，怎么让机器自己产生呢？ 我们可以让很多Logistic Regression cascade(连接)起来，让一个input $x$的两个feature $x_1,x_2$经过两个Logistic Regression的transform，得到新的feature $x_1’,x_2’$，在这个新的feature space上，class 1和class 2是可以用一条直线分开的，那么最后只要再接另外一个Logistic Regression的model(对它来说，$x_1’,x_2’$才是每一个样本点的”feature”，而不是原先的$x_1,x_2$)，它根据新的feature，就可以把class 1和class 2分开。 因此着整个流程是，先用n个Logistic Regression做feature Transformation(n为每个样本点的feature数量)，生成n个新的feature，然后再用一个Logistic Regression作classifier Logistic Regression的boundary一定是一条直线，它可以有任何的画法，但肯定是按照某个方向从高到低的等高线分布，具体的分布是由Logistic Regression的参数决定的，每一条直线都是由$z=b+\\sum\\limits_i^nw_ix_i$组成的(二维feature的直线画在二维平面上，多维feature的直线则是画在多维空间上) 下图是二维feature的例子，分别表示四个点经过transform之后的$x_1’$和$x_2’$，在新的feature space中可以通过最后的Logistic Regression划分开来。 注意，这里的Logistic Regression只是一条直线，它指的是“属于这个类”或“不属于这个类”这两种情况，因此最后的这个Logistic Regression是跟要检测的目标类相关的，当只是二元分类的时候，最后只需要一个Logistic Regression即可，当面对多元分类问题，需要用到多个Logistic Regression来画出多条直线划分所有的类，每一个Logistic Regression对应它要检测的那个类。 Powerful Cascading Logistic Regression 通过上面的例子，我们发现，多个Logistic Regression连接起来会产生很powerful的效果(:3_ヽ)_，如果我们把每一个Logistic Regression叫做一个neuron(神经元)，把这些Logistic Regression串起来所形成的network，就叫做Neural Network 类神经网路，那我们已经开始接触到Deep Learning了∠(ᐛ」∠)＿。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://nekomoon404.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://nekomoon404.github.io/tags/Machine-Learning/"},{"name":"Logistic Regression","slug":"Logistic-Regression","permalink":"http://nekomoon404.github.io/tags/Logistic-Regression/"},{"name":"Discriminaive model","slug":"Discriminaive-model","permalink":"http://nekomoon404.github.io/tags/Discriminaive-model/"}]},{"title":"ML笔记（2）Classification","slug":"ML笔记（2）Classification","date":"2020-07-01T03:06:27.000Z","updated":"2020-07-01T13:44:49.271Z","comments":true,"path":"2020/07/01/ML笔记（2）Classification/","link":"","permalink":"http://nekomoon404.github.io/2020/07/01/ML%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89Classification/","excerpt":"","text":"1.Classification这节课的例子是对宝可梦进行分类（老宝可梦训练大师了），宝可梦有18种属性，现在要解决的分类问题就是做一个宝可梦种类的分类器，我们要找一个function，这个function的input是某一只宝可梦，它的output就是这只宝可梦属于这18类别中的哪一个type。 对每一只宝可梦（比如皮卡丘），可以用7个feature的数值组成的vector来描述它。 把编号400以下的宝可梦当做training data，编号400以上的当做testing data。这里我们先只考虑二分类，只考虑水系Water和一般系Normal宝可梦。 1.1.Classification as Regression？以binary classification为例，我们在Training时让输入为class 1的输出为1，输入为class 2的输出为-1；那么在testing的时候，regression的output是一个数值，它接近1则说明它是class 1，它接近-1则说明它是class 2。但Regression中对“好，坏”的定义并不适用于Classification。 1.2.Ideal Alternatives理想的方法是这样的：我们要找的function f(x)里面会有另外一个function g(x)，当我们的input x输入后，如果g(x)&gt;0，那f(x)的输出就是class 1，如果g(x)&lt;0，那f(x)的输出就是class 2，这个方法保证了function的output都是离散的表示class的数值。 把loss function定义成$L(f)=\\sum\\limits_n\\delta(f(x^n)≠\\hat{y}^n)$，即这个model在所有的training data上predict预测错误的次数，也就是说分类错误的次数越少，这个function表现得就越好 但是这个loss function没有办法微分，是无法用gradient descent的方法去解的，当然有Perceptron、SVM这些方法可以用（感知机和支持向量机的内容，在林轩田老师的机器学习基石和技法课程中都有详细的讲解），但这里先用另外一个solution来解决这个问题，即从概率的角度来考虑二分类问题。 2.Solution: Generative model 这两张图的内容很好理解，就是贝叶斯公式啦。这种想法叫做Generative model(生成模型)，因为有这个model，就可以拿它来generate $x$(如果你可以计算出每一个$x$出现的概率，就可以用这个distribution分布来生成$x$、sample $x$出来)。 接下来考虑怎么计算其中的四项：$P(C_1)，P(C_2)，P(x|C_1)，P(x|C_2)$。 2.1.Prior$P(C_1)$和$P(C_2)$这两个概率，被称为Prior，计算这两个值还是很简单的。 假设我们还是考虑二元分类问题，编号小于400的data用来Training，编号大于400的data用来testing，如果想要严谨一点，可以在Training data里面分一部分validation出来模拟testing的情况 在Training data里面，有79只水系宝可梦，61只一般系宝可梦，那么$P(C_1)=79/(79+61)=0.56$，$P(C_2)=61/(79+61)=0.44$ 2.2.Probability from Class计算$P(x|C_1)$和$P(x|C_2)$的值，假设$x$是一只新来的海龟，它显然是水系的，但是在79只水系的宝可梦training data里面根本就没有海龟，那要如何计算$P(x|C_1)$呢。 每一只宝可梦可以用由特征值组成的向量来表示，每个sample有7个feature，为了方便可视化，这里只考虑Defense和SP Defence两种feature。我们需要用已有的数据去估测海龟出现的可能性，你可以想象说这已有的79只水系宝可梦的data其实只是冰山一角，假定水系神奇宝贝的Defense和SP Defense是从一个Gaussian的distribution里面sample出来的，下图只是采样了79个点之后得到的分布，但是从高斯分布里采样出海龟这个点的几率并不是0。现在的问题是怎么从这79个已有的点计算出Gaussian distribution函数。 2.3.Gaussian distribution函数 f_{u,\\Sigma}(x)=\\frac{1}{(2\\pi)^{\\frac{D}{2}}}\\frac{1}{|\\Sigma|^{\\frac{1}{2}}} \\exp \\{ -\\frac{1}{2}(x-u)^T\\Sigma^{-1}(x-u) \\}Input: vector $x$, output: probability of sampling x ；其中$\\mu$表示方差均值mean，$\\Sigma$表示协方差矩阵covariance matrix。 从下图中可以看出，同样的$\\Sigma$，不同的$u$，概率分布最高点的地方是不一样的： 如果是同样的$u$，不同的$\\Sigma$，概率分布最高点的地方是一样的，但是分布的密集程度是不一样的： Gaussian distribution函数是由$\\mu$和$\\Sigma$决定的，估计$\\mu$和$\\Sigma$可以使用极大似然估计(Maximum Likelihood)的方法，其思路是找出最特殊的那对$u$和$\\Sigma$，从它们决定的高斯函数中再次采样出79个点，使”得到的分布情况与当前已知79点的分布情况相同“这件事情发生的可能性最大。 实际上任意一组$u$和$\\Sigma$对应的高斯函数($u$表示该Gaussian的中心点，$\\Sigma$表示该Gaussian的分散程度)都有可能sample出跟当前分布一致的样本点，就像上图中的两个红色圆圈所代表的高斯函数，但肯定存在着发生概率最大的哪一个Gaussian，而这个函数就是我们要找的。 似然函数： L(u,\\Sigma)=f_{u,\\Sigma}(x^1)\\cdot f_{u,\\Sigma}(x^2)...f_{u,\\Sigma}(x^{79})实际上就是该事件发生的概率就等于每个点都发生的概率之积，要求的$\\mu$和$\\Sigma$就是使似然函数取极大值的$\\mu$和$\\Sigma$： u^*,\\Sigma^*=\\arg \\max\\limits_{u,\\Sigma} L(u,\\Sigma)对$\\mu$和$\\Sigma$分别偏导，使微分等于0，得到的高斯函数的$u$和$\\Sigma$的最优解如下： u^*=\\frac{1}{79}\\sum\\limits_{n=1}^{79}x^n \\\\ \\Sigma^*=\\frac{1}{79}\\sum\\limits_{n=1}^{79}(x^n-u^*)(x^n-u^*)^T这样我们由Training Data就可以分别求出Class 1和Class 2的$\\mu$和$\\Sigma$，从而计算出海龟这个sample的$P(x|C_1),P(x|C_2)$。 2.4.Do Classification接着将$P(C_1),P(x|C_1),P(C_2),P(x|C_2)$代入公式计算出$P(C_1|x)$，若大于0.5，则说明sample $x$属于Class 1啦。 将得到的结果在图中表示出来，横轴是Defense，纵轴是SP Defense，蓝色的点是水系的宝可梦的分布，红色的点是一般系的宝可梦的分布，对图中的每一个点都计算出它是class 1的概率$P(C_1|x)$，这个概率用颜色来表示，如果某点在红色区域，表示它是水系宝可梦的概率更大；如果该点在其他颜色的区域，表示它是水系宝可梦的概率比较小 因为我们做的是分类问题，因此令几率&gt;0.5的点为类别1，几率&lt;0.5的点为类别2，也就是右上角的图中的红色和蓝色两块区域，再把testing data上得到的结果可视化出来，即右下角的图，发现分的不是太好，正确率才是47%。 我们之前用的只是Defense和SP Defense这两个feature，在二维空间上得到的效果不太好，但实际上一开始就提到了宝可梦总共是有7个features的，也许在二维空间上它们是重叠在一起的，但是在六维空间上看它们也许会区分得很好。​考虑7个feature时，$\\mu$是一个7-dim的vector，$\\Sigma$则是一个7*7的matrix，发现得到的准确率也才54%，这个分类器表现并不好，接下来考虑如何改进这个分类器。 2.5.Modify Model上面用到的Gaussian distribution函数，我们给每一个Class的Gaussian都计算了自己的mean和convariance，这种做法其实并不实用，常用的做法是，不同的class可以share同一个cocovariance matrix。variance是跟input的feature size的平方成正比的，所以当feature的数量很大的时候，$\\Sigma$大小的增长是可以非常快的，在这种情况下，给不同的Gaussian以不同的covariance matrix，会造成model的参数太多，而参数多会导致该model的variance过大，出现overfitting的现象，因此对不同的class使用同一个covariance matrix，可以有效减少参数，减小overfitting。 这样用$\\mu_1$、$\\mu_2$和$\\Sigma$来决定一个总的似然函数，求出其取极大值时的$\\mu_1$、$\\mu_2$和$\\Sigma$，发现得到的$u_1$和$u_2$和原来一样，还是各自的均值，而$\\Sigma$则是原先两个$\\Sigma_1$和$\\Sigma_2$的加权。 将结果表示在图中，你会发现，class 1和class 2在没有共用covariance matrix之前，它们的分界线是一条曲线；如果共用covariance matrix的话，它们之间的分界线就会变成一条直线，这样的model，我们也称之为linear model(尽管Gaussian不是linear的，但是它分两个class的boundary是linear)。 如果考虑宝可梦的所有的7个feature，且共用covariance的话，正确率会从原来的54%提升到73%。 2.6.Three Steps of classification回顾一下做classification的三个步骤，实际上也就是做machine learning的三个步骤： Find a function set(model) 这些required probability $P(C)$和probability distribution $P(x|C)$就是model的参数，选择不同的Probability distribution(比如不同的分布函数，或者是不同参数的Gaussian distribution)，就会得到不同的function，把这些不同参数的Gaussian distribution集合起来，就是一个model，如果不适用高斯函数而选择其他分布函数，就是一个新的model了 当这个posterior Probability $P(C|x)&gt;0.5$的话，就output class 1，反之就output class 2 Goodness of function 对于Gaussian distribution这个model来说，要评价的是决定这个高斯函数形状的均值$u$和协方差$\\Sigma$这两个参数的好坏，而极大似然函数$L(u,\\Sigma)$的输出值，就评价了这组参数的好坏 Find the best function 找到的那个最好的function，就是使$L(u,\\Sigma)$值最大的那组参数，实际上就是所有样本点的均值和协方差： u^*=\\frac{1}{n}\\sum\\limits_{i=0}^n x^i \\ \\ \\ \\ \\Sigma^*=\\frac{1}{n}\\sum\\limits_{i=0}^n (x^i-u^*)(x^i-u^*)^T式中的$x$表示一个feature的vector，上标$i$表示第$i$个sample 2.7.Probability distribution上面的讨论中我们使用的是Gaussian distribution函数，当然你也可以使用其他分布函数，这通常要根据数据集的具体情况来确定，比如一个特征是binary feature，那选择使用伯努利分布Bernoulli distribution比较好。如果选择的是简单的分布函数(参数比较少)，那么bias就偏大，variance就小；如果选择复杂的分布函数，那么bias就偏小，variance就偏大。 Naive Bayes Classifier(朴素贝叶斯分类法)考虑这样一件事情，假设$x=[x_1 \\ x_2 \\ x_3 \\ … \\ x_k \\ … \\ ]$中每一个dimension $x_k$的分布都是相互独立的，它们之间的covariance都是0，那我们就可以把x产生的几率拆解成$x_1,x_2,…,x_k$产生的几率之积。 这里每一个dimension的分布函数都是一维的Gaussian distribution，就相当于原高维的Gaussian，它的covariance matrix变成是diagonal(对角的)，在不是对角线的地方，值都是0，这样就可以更加减少需要的参数量，就可以得到一个更简单的model。 这种方法叫做Naive Bayes Classifier(朴素贝叶斯分类法)，如果真的明确了所有的feature之间是相互独立的，是不相关的，使用朴素贝叶斯分类法的performance是会很好的；如果这个假设是不成立的，那么Naive bayes classfier的bias就会很大，它就不是一个好的classifier(朴素贝叶斯分类法本质就是减少参数)。 在宝可梦这个例子中（李老师也是个老二次元了|ू･ω･` )），使用朴素贝叶斯的效果并不好，因为不同feature之间还是有相关的，各种feature之间的covariance还是必要的，比如战斗力和防御力它们之间是正相关的，covariance不能等于0。 2.8.Analysis Posterior Probability接下来我们来分析一下这个后置概率的表达式，会发现一些有趣的现象 表达式上下同除以分子，再引入变量$z$，得到$\\sigma(z)=\\frac{1}{1+e^{-z}}$，这个function叫做sigmoid function sigmoid函数真是再熟悉不过了，接下来推导一下$z$的具体表达式，Warning of Math（老师原话：下面这部分推导听不懂的同学可以先睡一会⊙(・◇・)？，之后直接听结论） 推导是有些复杂，但当$\\Sigma_1$和$\\Sigma_2$相同时，经过化简$z$就变成了一个linear的function，$x$前的一项可以当做vector $w$，后面的几项相加是一个scalar，当做常数$b$。 $P(C_1|x)=\\sigma (w\\cdot x+b)$这个式子就解释了，当class 1和class 2共用$\\Sigma$的时候，它们之间的boundary会是线性的。 这样在Generative model里，我们要做的就是用某些方法去找出$N_1,N_2,u_1,u_2,\\Sigma$，找出这些以后就算出$w$和$b$，把它们代进$P(C_1|x)=\\sigma(w\\cdot x+b)$这个式子，计算概率。 但是为什么要这么麻烦呢(ｷ｀ﾟДﾟ´)？我们的最终目标都是要找一个vector $w$和const $b$，何必先去计算概率，算出一些$u,\\Sigma$，然后再回过头来又去算$w$和$b$。那么能不能直接把$w$和$b$计算出来呢，下一节课Logistic Regression将会解决这个问题。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://nekomoon404.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://nekomoon404.github.io/tags/Machine-Learning/"},{"name":"Classification","slug":"Classification","permalink":"http://nekomoon404.github.io/tags/Classification/"}]},{"title":"ML笔记（1）Gradient_Descent","slug":"ML笔记（1）Gradient-Descent","date":"2020-06-30T03:05:18.000Z","updated":"2020-06-30T04:05:18.000Z","comments":true,"path":"2020/06/30/ML笔记（1）Gradient-Descent/","link":"","permalink":"http://nekomoon404.github.io/2020/06/30/ML%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89Gradient-Descent/","excerpt":"","text":"这周开始学李宏毅老师的Machine Learning 2020的课程了，从第一节课的课程概述来看，这门课除了会介绍传统的Machine Learning算法外，还会介绍很多Deep Learning的内容，时长也是非常感人(ㄒoㄒ)，希望自己能坚持学下去吧。 前面几节课（p3-p7）分别讲了回归 Regression，误差来源 Where does the error come from，梯度下降 Gradient Descent，老师用的预测宝可梦pokemon的CP值的例子还是蛮有趣的hhh。这部分的理论内容自己是比较熟悉，也有之前做的纸质笔记，在这里就不详细展开了，这里主要写一下使用Gradient Descent的Tips，以及其背后的Theory。 1. Gradient Descent Tips1.1.Tip 1: Tuning your learning ratesgradient descent过程中，影响结果的一个很关键的因素就是learning rate的大小 当参数有很多个的时候(&gt;3)，其实我们很难做到将loss随每个参数的变化可视化出来(因为最多只能可视化出三维的图像，也就只能可视化三维参数)，但是我们可以把update的次数作为唯一的一个参数，将loss随着update的增加而变化的趋势给可视化出来(上图右半部分) 所以做gradient descent时可以把不同的learning rate下，loss随update次数的变化曲线给可视化出来，它可以提醒你该如何调整当前的learning rate的大小，直到出现稳定下降的曲线 Adaptive Learning rates显然这样手动地去调整learning rates很麻烦，因此我们需要有一些自动调整learning rates的方法。最基本、最简单的大原则是：learning rate通常是随着参数的update越来越小的。可以这样理解：在起始点的时候，通常是离最低点是比较远的，这时候步伐就要跨大一点；而经过几次update以后，会比较靠近目标，这时候就应该减小learning rate，让它能够收敛在最低点的地方。如可以设置到了第t次update，$\\eta^t=\\eta/ \\sqrt{t+1}$。 这种方法使所有参数以同样的方式同样的learning rate进行update，而最好的状况是每个参数都给他不同的learning rate去update AdagradAdagrad就是将不同参数的learning rate分开考虑的一种算法(adagrad算法update到后面速度会越来越慢，当然这只是adaptive算法中最简单的一种)。 这里的$w$是function中的某个参数，$t$表示第$t$次update，$g^t$表示Loss对$w$的偏微分，而$\\sigma^t$是之前所有Loss对$w$偏微分的均方根 root mean square这个值对每一个参数来说都是不一样的。 \\begin{equation} \\begin{split} &Adagrad\\\\ &w^1=w^0-\\frac{\\eta^0}{\\sigma^0}\\cdot g^0 \\ \\ \\ \\sigma^0=\\sqrt{(g^0)^2} \\\\ &w^2=w^1-\\frac{\\eta^1}{\\sigma^1}\\cdot g^1 \\ \\ \\ \\sigma^1=\\sqrt{\\frac{1}{2}[(g^0)^2+(g^1)^2]} \\\\ &w^3=w^2-\\frac{\\eta2}{\\sigma^2}\\cdot g^2 \\ \\ \\ \\sigma^2=\\sqrt{\\frac{1}{3}[(g^0)^2+(g^1)^2+(g^2)^2]} \\\\ &... \\\\ &w^{t+1}=w^t-\\frac{\\eta^t}{\\sigma^t}\\cdot g^t \\ \\ \\ \\sigma^t=\\sqrt{\\frac{1}{1+t}\\sum\\limits_{i=0}^{t}(g^i)^2} \\end{split} \\end{equation}由于$\\eta^t$和$\\sigma^t$中都有一个$\\sqrt{\\frac{1}{1+t}}$的因子，两者相消，即可得到Adagrad的最终表达式： w^{t+1}=w^t-\\frac{\\eta}{\\sqrt{\\sum\\limits_{i=0}^t(g^i)^2}}\\cdot g^t对Adagrad中的contradiction的解释Adagrad的表达式$w^{t+1}=w^t-\\frac{\\eta}{\\sqrt{\\sum\\limits_{i=0}^t(g^i)^2}}\\cdot g^t$里面有一件很矛盾的事情：我们在做gradient descent的时候，希望的是当梯度值即微分值$g^t$越大的时候(此时斜率越大，还没有接近最低点)更新的步伐要更大一些，但是Adagrad的表达式中，分母表示梯度越大步伐越大，分子却表示梯度越大步伐越小，两者似乎相互矛盾。 在一些paper里是这样解释的：Adagrad要考虑的是，这个gradient有多surprise，即反差有多大，假设t=4的时候$g^4$与前面的gradient反差特别大，那么$g^t$与$\\sqrt{\\frac{1}{t+1}\\sum\\limits_{i=0}^t(g^i)^2}$之间的大小反差就会比较大，它们的商就会把这一反差效果体现出来。 而且需要注意的是：gradient越大，离最低点越远，这个有点直观的想法在有多个参数的情况下是不一定成立的。如下图所示，$w_1$和$w_2$分别是loss function的两个参数，loss的值投影到该平面中以颜色深度表示大小，分别在$w_2$和$w_1$处垂直切一刀(这样就只有另一个参数的gradient会变化)，对应的情况为右边的两条曲线，可以看出，比起a点，c点距离最低点更近，但是它的gradient却越大。 考虑一个简单的情况，对于一个二次函数$y=ax^2+bx+c$来说，最小值点的$x=-\\frac{b}{2a}$，而对于任意一点$x_0$，它迈出最好的步伐长度是$|x_0+\\frac{b}{2a}|=|\\frac{2ax_0+b}{2a}|$(这样就一步迈到最小值点了)，联系该函数的一阶和二阶导数$y’=2ax+b$、$y’’=2a$，可以发现the best step 就是 $|\\frac{y’}{y’’}|$，即|一阶导数|/|二阶导数|，可见best step不仅跟一阶导数(gradient)有关，还跟二阶导数有关，用这个best step可以来反映Loss Function上一点到最低点的距离。 再来回顾Adagrad的表达式： w^{t+1}=w^t-\\frac{\\eta}{\\sqrt{\\sum\\limits_{i=0}^t(g^i)^2}}\\cdot g^t其中，$g^t$是一次微分，而分母中的$\\sqrt{\\sum\\limits_{i=0}^t(g^i)^2}$则反映了二次微分的大小。Adagrad中用root mean square of the previous derivatives of parameter w来近似二次微分的大小，避免引入额外的计算量。 1.2.Tip 2: Stochastic Gradient Descent随机梯度下降 Stochastic Gradient Descent的方法可以让训练更快速，传统的gradient descent的思路是遍历所有的样本点之后再构建loss function，然后去update参数；而stochastic gradient descent的做法是，看到一个样本点就update一次，因此它的loss function不是所有样本点的error平方和，而是这个随机样本点的error平方。 Stochastic Gradient Descent与传统Gradient Descent的效果对比如下： 1.3.Tip 3: Feature Scaling特征缩放Feature Scaling，当多个特征的尺度很不一样时，最好将这些不同feature的范围缩放成一样的。 举个例子来解释为什么需要做feature scaling，$y=b+w_1x_1+w_2x_2$，假设$x_1$的值都是很小的，比如1,2…；$x_2$的值都是很大的，比如100,200…。此时去画出loss的error surface，如果对$w_1$和$w_2$都做一个同样的变动$\\Delta w$，那么$w_1$的变化对$y$的影响是比较小的，而$w_2$的变化对$y$的影响是比较大的。 左边的error surface表示，w1对y的影响比较小，所以w1对loss是有比较小的偏微分的，因此在w1的方向上图像是比较平滑的；w2对y的影响比较大，所以w2对loss的影响比较大，因此在w2的方向上图像是比较sharp的。如果x1和x2的的scale是接近的，那么w1和w2对loss就会有差不多的影响力，loss的图像接近于圆形，那这样做对gradient descent有什么好处呢？ 对Gradient Descent的帮助 之前我们做的demo已经表明了，对于这种长椭圆形的error surface，如果不使用Adagrad之类的方法，是很难搞定它的，因为在像w1和w2这样不同的参数方向上，会需要不同的learning rate，用相同的learning rate很难达到最低点。如果x1和x2有相同的scale的话，loss在参数w1、w2平面上的投影就是一个正圆形，update参数会比较容易。 而且gradient descent的每次update并不都是向着最低点走的，每次update的方向是顺着等高线的方向(梯度gradient下降的方向)，而不是径直走向最低点；但是当经过对input的scale使loss的投影是一个正圆的话，不管在这个区域的哪一个点，它都会向着圆心走。因此feature scaling对参数update的效率是有帮助的 如何做feature scaling 假设有R个example(上标i表示第i个样本点)，$x^1,x^2,x^3,…,x^r,…x^R$，每一个example，它里面都有一组feature(下标j表示该样本点的第j个特征) 对每一个feature (demension) i，都去算出它的平均值mean=$m_i$，以及标准差standard deviation=$\\sigma_i$ 对第r个example的第i个feature的值，减掉均值，除以标准差，即： x_i^r=\\frac{x_i^r-m_i}{\\sigma_i} 2.Gradient Descent Theory考虑当用梯度下降解决问题： \\theta^*=\\arg \\underset{\\theta}{\\min} L(\\theta)每次更新参数 $\\theta$，都得到一个新的$ \\theta$，它都使得损失函数更小。即： L(\\theta^0)>L(\\theta^1)>L(\\theta^2)>\\dots上述结论正确吗？其实是不正确的，我们并不能每次更新$\\theta$，都能使损失函数更小。那么如何更新$\\theta$才能使损失函数更小呢。 比如在$\\theta^0$ 处，可以在一个小范围的圆圈内找到使损失函数最小的$\\theta^1$，不断的这样去寻找。接着就考虑如何在这样一个小圆圈中寻找到$\\theta^1$。这里就需要引入泰勒展开式（高数里都有讲啦）。 回到刚才的问题上，我们将点(a,b)附件的小圆内的损失函数在点(a,b)处展开： L(\\theta)≈L(a,b)+\\frac{\\partial L(a,b)}{\\partial \\theta_1}(\\theta_1-a)+\\frac{\\partial L(a,b)}{\\partial \\theta_2}(\\theta_2-b)令 s=L(a,b),\\quad u=\\frac{\\partial L(a,b)}{\\partial \\theta_1},\\quad v=\\frac{\\partial L(a,b)}{\\partial \\theta_2}则 L(\\theta)≈s + u\\cdot (\\theta_1-a)+v\\cdot (\\theta_2-b)设红色圆圈的半径为d，则有限制条件： (\\theta_1-a)^2+(\\theta_2-b)^2≤d^2求$L(\\theta)_{min}$，这里有个小技巧，把$L(\\theta)$转化为两个向量的乘积： u\\cdot (\\theta_1-a)+v\\cdot (\\theta_2-b)=(u,v)\\cdot (\\theta_1-a,\\theta_2-b)=(u,v)\\cdot (\\Delta \\theta_1,\\Delta \\theta_2)显然，当向量$(\\theta_1-a,\\theta_2-b)$与向量$(u,v)$反向，且刚好到达red circle的边缘时(用$\\eta$去控制向量的长度)，$L(\\theta)$最小。 于是$L(\\theta)$局部最小值对应的参数为中心点减去gradient的加权： \\begin{bmatrix} \\Delta \\theta_1 \\\\ \\Delta \\theta_2 \\end{bmatrix}= -\\eta \\begin{bmatrix} u \\\\ v \\end{bmatrix}=> \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\end{bmatrix}= \\begin{bmatrix} a\\\\ b \\end{bmatrix}-\\eta \\begin{bmatrix} u\\\\ v \\end{bmatrix}= \\begin{bmatrix} a\\\\ b \\end{bmatrix}-\\eta \\begin{bmatrix} \\frac{\\partial L(a,b)}{\\partial \\theta_1}\\\\ \\frac{\\partial L(a,b)}{\\partial \\theta_2} \\end{bmatrix}这就是gradient descent在数学上的推导，注意它的重要前提是，设定的红色圈圈的范围要足够小，这样泰勒展开给我们的近似才会精确，而$\\eta$的值是与圆的半径成正比的，因此理论上learning rate要无穷小才能够保证每次gradient descent在update参数之后的loss会越来越小。于是当learning rate没有设置好，泰勒近似不成立，就有可能使gradient descent过程中的loss没有越来越小 当然泰勒展开可以使用二阶、三阶乃至更高阶的展开，但这样会使得运算量大大增加，反而降低了运行效率。 3.More Limitation of Gradient Descent之前已经讨论过，gradient descent有一个问题是它会停在local minima的地方就停止update了。事实上还有一个问题是，微分值是0的地方并不是只有local minima，settle point （鞍点）的微分值也是0。以上都是理论上的探讨，到了实践的时候，其实当gradient的值接近于0的时候，我们就已经把它停下来了，但是微分值很小，不见得就是很接近local minima，也有可能是在一个鞍点上。还有个限制就是在”坡度“小的区域update得很慢。总结一下就是下面三点： Very slow at the plateau Stuck at saddle point Stuck at local minima","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://nekomoon404.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://nekomoon404.github.io/tags/Machine-Learning/"},{"name":"Gradient Descent","slug":"Gradient-Descent","permalink":"http://nekomoon404.github.io/tags/Gradient-Descent/"}]},{"title":"单目视觉里程计","slug":"单目视觉里程计","date":"2020-06-16T02:15:05.000Z","updated":"2020-06-27T03:23:39.546Z","comments":true,"path":"2020/06/16/单目视觉里程计/","link":"","permalink":"http://nekomoon404.github.io/2020/06/16/%E5%8D%95%E7%9B%AE%E8%A7%86%E8%A7%89%E9%87%8C%E7%A8%8B%E8%AE%A1/","excerpt":"","text":"视觉里程计(Visual Odometry，VO)是一种利用连续的图像序列来估计相机或机器人移动距离的方法，也被称为Visual SLAM问题的前端，是移动机器人定位导航领域中的关键技术之一。作业要求基于OpenCV初步实现一个简单的视觉里程计，我主要参考了高翔的《视觉slam十四讲》来完成，并将项目上传到Github，地址为https://github.com/nekomoon404/slam-VO。下面整理了这次大作业的报告文档。 视觉里程效果截图 程序使用方法运行环境：Ubuntu16.04+OpenCV3.4.0 安装程序所需的依赖库：线性代数库Eigen3，基于图优化的库g2o。 使用的数据集：KITTI数据集 灰度序列 /00/image_0 中的前2000帧图片 运行前更改slam-VO.h文件中第137行对应的路径： 1ifstream myfile (\"/home/neko/slam-VO/poses/00.txt\"); 更改slam-VO.cpp文件中第23到25行，以及第99行对应的路径： 12345sprintf(filename1, \"/home/neko/slam-VO/00/image_0/%06d.png\", 0);sprintf(filename2, \"/home/neko/slam-VO/00/image_0/%06d.png\", 1);string pose_path = \"/home/neko/slam-VO/poses/00.txt\";sprintf(filename, \"/home/neko/slam-VO/00/image_0/%06d.png\", numFrame); 进入/build文件夹，在终端依次执行来运行程序： 123cmake ..make./slam-VO 算法详细介绍程序的整体流程如下： 单目视觉初始化： 读取第一、二帧图片，并提取第一帧图片的特征点； 通过光流跟踪法得到第二帧图片的特征点，删除跟踪失败的点； 计算本质矩阵，并恢复运动，并以该转换矩阵作为初始值。 主循环： 连续读取图片，当读取图片帧数小于设定的最大帧数MAX_FRAME时： 光流跟踪前一帧图片特征点，得到当前帧特征点，并删除跟踪失败的点; 计算本质矩阵，并恢复旋转运动$\\pmb{R}$和平移运动$\\pmb{t}$； 根据匹配到的两帧特征点，进行三角测量，计算两帧图片之间的距离尺度scale; 由前一帧图片的位置、当前图片旋转、平移矩阵、距离尺度，计算得到当前图片的位置，生成相机轨迹； 进行BA优化； 保存优化后的平移运动结果，画出轨迹； 判断跟踪后的特征点数目是否大于设定阈值MIN_NUM_FEAT，若小于则重新检测 1.图片提取根据网上的教程资料配置了Ubuntu16.04+OpenCV3.4.0的环境，具体步骤为： 下载OpenCV3.4.0 sources版本，解压 安装opencv的依赖库和cmake包，通过sudo apt-get install *命令进行安装 进入解压完的opencv文件夹，创建编译文件夹，mkdir build，依次执行cd build， cmake ..，make，make install。 配置OpenCV的编译环境，将OpenCV的库添加到路径，从而可以让系统找到，sudo gedit /etc/ld.so.conf.d/opencv.conf，在打开的文本中写入/usr/local/lib，执行sudo ldconfig 配置bash，执行sudo gedit /etc/bash.bashrc，在文本最末尾添加： 12PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfigexport PKG_CONFIG_PATH 保存，执行source /etc/bash.bashrc，使得配置生效：更新sudo updatedb 程序中使用sprintf()函数读入指定路径下的图片序列，用imread()函数读取当前图片，用imshow()函数显示当前图片。 12345sprintf(filename, /home/neko/slam-VO/00/image_0/%06d.png, numFrame);cout &lt;&lt; \"Current frame number:\"&lt;&lt;numFrame &lt;&lt; endl;currImage = imread(filename);imshow( \"CAMERA IMAGE\", currImage); 2.提取特征点视觉里程计的核心问题是如何根据图形估计相机运动，比较方便的做法是：首先从图像中选取比较有代表性的点，这些点在相机视角发生少量变化后会保持不变，于是能在各个图像中找到相同的点。然后在这些点的基础上讨论，讨论相机位姿估计问题，以及这些点的定位问题，这些点被称为图像特征。 在本次作业中我们提取FAST关键点，FAST 是一种角点，主要检测局部像素灰度变化明显的地方，以速度快著称。它的思想是：如果一个像素与它邻域的像素差别较大（过亮或过暗）, 那它更可能是角点。它的检测过程如下： 在图像中选取像素p，假设它的亮度为$I_p$。 设置一个阈值$T$ (比如$I_p$的20%)。 以像素$p$为中心, 选取半径为3的圆上的16个像素点。 假如选取的圆上，有连续的$N$个点的亮度大于$I_p+T$ 或小于$I_p-T$，那么像素$p$可以被认为是特征点。 循环以上四步，对每一个像素执行相同的操作。 ​ 图2-1 FAST特征点 OpenCV中默认采用Fast-9-16，即在周围取16个像素点，若超过连续9个点与中心点差值大于阈值即成为候选角点。为了更高效，可以进行预处理，检测圆周上第1.5.9.13个像素，当其中有三个及以上的符合阈值，才可以入围，若不符合，便可以直接排除。同时需要搜索局部极大值，抑制非极大值元素来避免角点集中的问题。 程序中通过featureDetection()函数来实现： 1234567void featureDetection(Mat img_1, vector&lt;Point2f&gt;&amp; points1) &#123; vector&lt;KeyPoint&gt; kps; int fast_threshold = 10; Ptr&lt;FastFeatureDetector&gt; detector=FastFeatureDetector::create(); detector-&gt;detect(img_1,kps); KeyPoint::convert(kps, points1, vector&lt;int&gt;());&#125; 3.特征跟踪与估计相机运动3.1.光流法特征跟踪使用光流（Optical Flow）来跟踪特征点的运动。这样可以回避计算和匹配描述子带来的时间。光流是一种描述像素随着时间，在图像之间运动的方法。随着时间的经过，同一个像素会在图像中运动。计算部分像素运动的称为稀疏光流，计算所有像素的称为稠密光流。稀疏光流以 Lucas-Kanade 光流为代表，并可以在 SLAM 中用于跟踪特征点位置。 ​ 图3-1 LK光流法示意图 在LK光流中，来自相机的图像是随时间变化的。图像可以看作时间的函数$I(t)$。那么，一个在$t$时刻，位于$(x,y)$处的像素，它的灰度可以写成$I(x,y,t)$。这种方式把图像看成了关于位置与时间的函数，它的值域就是图像中像素的灰度。 引入灰度不变假设：同一个空间点的像素灰度值，在各个图像中是固定不变的。对于t时刻位于$(x,y)$处的像素，设$t+dt$时刻，它运动到$(x + dx, y + dy)$处。由于灰度不变,有： I(x+dx,y+dy,t+dt)=I(x,y,t)通过对左边进行泰勒展开，保留一阶项，两边同时除$dt$，并写成矩阵形式： \\begin{bmatrix} I_x \\quad I_y \\end{bmatrix} \\begin{bmatrix} u \\\\v \\end{bmatrix}=-I_t引入额外的约束来计算$u, \\,\\,v$，假设某一个窗口内的像素具有相同的运动，如一个大小为$w \\times w$的窗口，它含有$w^2$数量的像素，因此共有$w^2$个方程： \\begin{bmatrix} I_x \\quad I_y \\end{bmatrix}_k \\begin{bmatrix} u \\\\v \\end{bmatrix}=-I_{tk}, \\quad k=1,\\dots,w^2这是一个关于$u,\\,\\, v$的超定线性方程，可以使用最小二乘法求解，这样就得到了像素在图像间的运动速度$u,\\,\\, v$。 如果相机运动较快，两张图像差异较明显，那么单层图像光流法容易达到一个局部极小值，这种情况可以通过引入图像金字塔来改善。图像金字塔是指对同一个图像进行缩放，得到不同分辨率下的图像。以原始的金字塔作为金字塔底层，每往上一层，就对下层图像进行一定倍率的缩放，就得到一个金字塔。然后在计算光流时，先从顶层的图像开始计算，然后把上一层的追踪结果，作为下一层光流的初始值。OpenCV中的calcOpticalFlowPyrLK()函数正是基于这一原理实现多层光流函数。 在程序中通过featureTracking()函数来实现特征跟踪： 1234567891011121314151617181920void featureTracking(Mat img_1, Mat img_2, vector&lt;Point2f&gt;&amp; points1, vector&lt;Point2f&gt;&amp; points2, vector&lt;uchar&gt;&amp; status) &#123; vector&lt;float&gt;err; calcOpticalFlowPyrLK(img_1, img_2, points1, points2, status, err); //调用光流法跟踪特征点 int indexCorrection = 0; for( int i=0; i&lt;status.size(); i++) //删除跟踪失败的点 &#123; Point2f pt = points2.at(i- indexCorrection); if ((status.at(i) == 0)||(pt.x&lt;0)||(pt.y&lt;0)) &#123; if((pt.x&lt;0)||(pt.y&lt;0)) &#123; status.at(i) = 0; &#125; points1.erase (points1.begin() + (i - indexCorrection)); points2.erase (points2.begin() + (i - indexCorrection)); indexCorrection++; &#125; &#125;&#125; 3.2.对极几何估计相机运动接下来根据匹配的点对估计相机的运动。当相机为单目时，只知道2D的像素坐标，因而问题是根据两组2D点估计运动，可以用对极几何来解决。 ​ 图3-2 对极几何约束 如下图所示，我们希望求取两帧图像$I_1$，$I_2$之间的运动，设第一帧到第二帧的运动为$R$，$t$。两个相机中心分别为$O_1$，$O_2$现在，考虑$I_1$ 中有一个特征点$p_1$，它在$I_2$中对应着特征点$p_2$。根据针孔相机模型，可得两个像素点的$p_1$，$p_2$的像素位置： s_1p_1=KP,\\quad s_2p_2=K(RP+t)其中$K$为相机内参，$R$，$t$为两个坐标系的相机运动。使用齐次坐标表示像素点，例如$s_1p_1$和$s_2p_2$成投影关系，它们在齐次坐标下的意义是相等的，称为尺度意义下相等。经推导得到对极约束的表达式： p_2^Tk^{-T}t^\\land RK^{-1}p_1=0把中间部分记作两个矩阵：基础矩阵（Fundamental Matrix）$F$ 和本质矩阵（Essential Matrix）$E$，可以进一步简化对极约束： E=t ^\\land R, \\quad F=K^{-T}EK^{-1},\\quad x_2^TEx_1=p_2^TFp_1=0对极约束简洁地给出了两个匹配点的空间位置关系，相机位姿估计问题变为以下两步： 根据配对点的像素位置，求出$E$ 或者$F$； 根据$E$或者$F$，求出$R$，$t$。 在程序中我们使用本质矩阵$E$求解，然后通过本质矩阵获取摄像机的相对旋转和平移量： 123//计算本质矩阵并恢复R,tE = findEssentialMat(prevFeatures, currFeatures, focal_length, principal_point, RANSAC, 0.999, 1.0, mask);recoverPose(E, prevFeatures,currFeatures, R, t, focal_length, principal_point, mask); 3.3.三角测量在单目SLAM 中，仅通过单张图像无法获得像素的深度信息，我们需要通过三角测量（Triangulation）（或三角化）的方法来估计地图点的深度。三角测量是指，通过在两处观察同一个点的夹角，确定该点的距离。在SLAM 中主要用三角化来估计像素点的距离。 ​ 图3-2 通过三角测量的方法获得地图点的深度 论上直线$O_1p_1$ 与 $O_2p_2$在场景中会相交于一点$P$，该点即是两个特征点所对应的地图点在三维场景中的位置。然而由于噪声的影响，这两条直线往往无法相交。因此，可以通过最二小乘去求解。设$x_1$，$x_2$为两个特征点的归一化坐标，那么它们满足： s_2x_2=s_1Rx_1+t如果要算$s_1$，那么先对上式两侧左乘一个$x_2^\\land$，得： s_2x_2^\\land x_2=0=s_1x_2 ^\\land R x_1 +x_2 ^\\land t该式左侧为零，右侧可以看成是$s_2$的一个方程，根据它可求出$s_2$，进而能求出$s_1$。这样就得到了两帧下的深度，确定了它们的空间坐标。由于噪声的存在，估得的$R,t$不一定精确使上式为零，所以常用最小二乘法求解。 在程序中通过triangulation()函数实现三角测量： 12345678910111213141516171819202122232425262728293031323334353637383940void triangulation ( const vector&lt; Point2f&gt;&amp; points_1, const vector&lt; Point2f&gt;&amp; points_2, const Mat&amp; R, const Mat&amp; t, vector&lt; Point3f &gt;&amp; points)&#123; Mat T1 = (Mat_&lt;float&gt; (3,4) &lt;&lt; 1,0,0,0, 0,1,0,0, 0,0,1,0); Mat T2 = (Mat_&lt;float&gt; (3,4) &lt;&lt; R.at&lt;double&gt;(0,0), R.at&lt;double&gt;(0,1), R.at&lt;double&gt;(0,2), t.at&lt;double&gt;(0,0), R.at&lt;double&gt;(1,0), R.at&lt;double&gt;(1,1), R.at&lt;double&gt;(1,2), t.at&lt;double&gt;(1,0), R.at&lt;double&gt;(2,0), R.at&lt;double&gt;(2,1), R.at&lt;double&gt;(2,2), t.at&lt;double&gt;(2,0) ); Mat K = ( Mat_&lt;double&gt; ( 3,3 ) &lt;&lt; 718.856, 0, 607.1928, 0, 718.856, 185.2157, 0, 0, 1 ); vector&lt;Point2f&gt; pts_1, pts_2; for ( int idex=0;idex&lt;points_1.size();idex++ ) &#123; // 将像素坐标转换至相机坐标 pts_1.push_back ( pixel2cam( points_1[idex], K) ); pts_2.push_back ( pixel2cam( points_2[idex], K) ); &#125; Mat pts_4d; cv::triangulatePoints( T1, T2, pts_1, pts_2, pts_4d ); // 转换成非齐次坐标 for ( int i=0; i&lt;pts_4d.cols; i++ ) &#123; Mat x = pts_4d.col(i); x /= x.at&lt;float&gt;(3,0); // 归一化 Point3f p ( x.at&lt;float&gt;(0,0), x.at&lt;float&gt;(1,0), x.at&lt;float&gt;(2,0) ); points.push_back( p ); &#125;&#125; 4.生成相机轨迹与Ground Truth比较Ground Truth轨迹绘制，通过读取00.txt中的数据绘制，slam-VO.h文件中定义了get_Pose()函数，slam-VO.cpp中函数调用程序如下： 123vector&lt;vector&lt;float&gt;&gt; poses = get_Pose(pose_path);Point2f trace1 = Point2f(int(poses[numFrame][3]) + 400, int(poses[numFrame][11]) + 150); //绘制Ground Truth circle(trace, trace1, 1, Scalar(0,0,0), 1); 初步未优化得到的相机运动轨迹绘制，在三角测量得到特征点三维信息后，通过判断两帧间是否有一定程度的位移决定该次三角测量精度是否准确，若可以，则绘制当前位置点，程序如下： 123456789101112131415161718192021//三角测量triangulation (prevFeatures,currFeatures,R,t,points);cout&lt;&lt;\"id&lt;points.size():\"&lt;&lt;points.size()&lt;&lt;endl; for ( int id=0; id&lt;points.size(); id++ )&#123; points1.push_back(prevFeatures[id]); points2.push_back(currFeatures[id]);&#125; scale = getAbsoluteScale(numFrame, 0, t.at&lt;double&gt;(2));//平移的距离 if ((scale&gt;0.1)&amp;&amp;(-t.at&lt;double&gt;(2) &gt; -t.at&lt;double&gt;(0)) &amp;&amp; (-t.at&lt;double&gt;(2) &gt; -t.at&lt;double&gt;(1)))&#123; //确保有一定程度的平移而不是纯旋转以保证三角测量的精度 t_f = t_f + scale*(R_f*(-t)); R_f = R.inv()*R_f; pre_t=t.clone(); int x = int(t_f.at&lt;double&gt;(0)) + 400; int y = int(t_f.at&lt;double&gt;(2)) + 150; Point2f trace2 = Point2f(x,y) ; circle(trace, trace2 ,1, Scalar(0,255,0), 1);//绘制初步估计的轨迹&#125;else&#123; cout &lt;&lt; \"scale below 0.1, or incorrect translation\" &lt;&lt; endl;&#125; 5.Bundle Adjustment优化PnP用于求解3D到2D运动的投影位置方法。可以通过线性方法先求解相机位姿再求解空间投影点，但存在的问题是线性解法鲁棒性不太好，并且需要建立线性方程求代数解较为困难。因此，当运动比较连续时，一般选择非线性优化方法来进行迭代求解最优解，通常采用基于最小二乘问题的Bundle Adjustment(BA, 捆集调整)方法。针对本文采用的单目相机数据集，必须先对其进行初始化，再使用BA算法。 非线性优化将相机位姿、空间特征点等多个参数看作优化变量。如上图所示，根据之前的特征匹配，已知$p_1$、$p_2$为同一空间点的两个投影点，利用最小化重投影误差对相机位姿进行优化，使计算得到的$p_2’$与实际$p_2$不断接近，直到达到精度范围或最大迭代次数。设相机位姿变换为$R$，$t$，对应李代数为$\\xi$。相机内参矩阵为$K$。任一三维空间点P的坐标$ \\pmb{P}_i=[X_i,Y_i,Z_i]^T$。对应的投影像素坐标为$\\pmb{u}_i=[u_i,v_i]^T$。则像素点与空间点存在如下关系： s_i\\begin{bmatrix}u_i\\\\v_i\\\\1 \\end{bmatrix}=\\pmb{K}(\\exp(\\pmb{\\xi}^\\land)\\begin{bmatrix}X_i\\\\Y_i\\\\Z_i\\\\1 \\end{bmatrix})_{1:3}即： s_i\\pmb{u}_i=\\pmb{K}(\\exp(\\pmb{\\xi}^\\land)\\pmb{P}_i)_{1:3}因此，以相机位姿李代数$\\xi$、特征点空间位置等为优化变量，以重投影误差为目标函数，计算将像素坐标观测值与按相机位姿计算得到的像素坐标计算值之间的误差，构建最小二乘问题，利用Gauss-Newton法或Levenburg-Marquadt优化算法寻找最优相机位姿和特征点空间坐标。优化模型如下所示： \\xi^*=\\arg \\min\\limits_{\\xi}\\frac{1}{2}\\sum^n_{i=1}\\Arrowvert u_i-\\frac{1}{s_i}K\\exp (\\xi^\\land)P_i\\Arrowvert_2^2在利用优化算法进行迭代的时候，最重要的是计算每次迭代的下降梯度方向，对目标函数误差求导可得： e(x+\\Delta x)\\approx e(x)+J\\Delta x其中，$J$为$2 \\times 6$的雅各比矩阵。通过扰动模型求解李代数导数，计算过程如下：由上文已知像素点与空间点的关系，设$\\pmb{P}’$为空间点$\\pmb{P}$经相机位姿变换后的空间坐标，即$\\pmb{P}’=(\\exp(\\pmb{\\xi}^\\land)\\pmb{P})_{1:3}=[X’,Y’,Z’]^T$。将相机内参$K$展开得： \\begin{bmatrix}su\\\\ sv \\\\s \\end{bmatrix}=\\begin{bmatrix}f_x &0 &c_x\\\\ 0 &f_y &c_y\\\\0 & 0 &1 \\end{bmatrix} \\begin{bmatrix} X' \\\\ Y' \\\\Z' \\end{bmatrix}消元可得： u=f_x\\frac{X'}{Z'}+c_x, \\quad v=f_y\\frac{Y'}{Z'}+c_y将相机位姿李代数左乘扰动量$\\delta \\pmb{\\xi}$，再通过误差变化对扰动量的求导可得： \\frac{\\partial e}{\\partial \\delta \\pmb{\\xi}}=\\lim \\limits_{\\delta \\pmb{\\xi}\\to0}\\frac{\\delta \\pmb{\\xi}\\bigoplus \\pmb{\\xi}}{\\delta \\pmb{\\xi}}=\\frac{\\partial e}{\\partial \\pmb{P}'}\\frac{\\partial \\pmb{P}'}{\\partial\\delta \\pmb{\\xi}}误差变化$e$对于$\\pmb{P}’$的求导可以根据之前消元得到的$u$、$v$表达式得到： \\frac{\\partial e}{\\partial \\pmb{P}'}= -\\begin{bmatrix} \\displaystyle\\frac{\\partial u}{\\partial {X}'} & \\displaystyle\\frac{\\partial u}{\\partial {Y}'} & \\displaystyle\\frac{\\partial u}{\\partial {Z}'} \\\\ \\displaystyle\\frac{\\partial v}{\\partial {X}'} & \\displaystyle\\frac{\\partial v}{\\partial {Y}'} & \\displaystyle\\frac{\\partial v}{\\partial {Z}'} \\end{bmatrix} =- \\begin{bmatrix} \\displaystyle\\frac{f_x}{Z'} & 0 & -\\displaystyle\\frac{f_x X'}{Z'^2} \\\\ 0 & \\displaystyle\\frac{f_y}{Z'} & -\\displaystyle\\frac{f_y Y'}{Z'^2} \\end{bmatrix}$\\pmb{P}’$对扰动量$\\delta \\pmb{\\xi}$求导为： \\frac{\\partial \\pmb{P}'}{\\partial\\delta \\pmb{\\xi}}=[\\pmb{I},\\,\\,-\\pmb{P}'^ \\land]于是最终得到相机位姿导数的雅各比矩阵： \\frac{\\partial e}{\\partial \\delta \\pmb{\\xi}}=-\\begin{bmatrix} \\displaystyle\\frac{f_x}{Z'} & 0 & -\\displaystyle\\frac{f_x X'}{Z'^2} & -\\displaystyle\\frac{f_x X' Y'}{Z'^2} &f_x+\\displaystyle\\frac{f_x X'}{Z'^2} &-\\displaystyle\\frac{f_x Y'}{Z'} \\\\ 0 & \\displaystyle\\frac{f_y}{Z'} & -\\displaystyle\\frac{f_y Y'}{Z'^2} &-f_y-\\displaystyle\\frac{f_y Y'}{Z'^2} & -\\displaystyle\\frac{f_y X' Y'}{Z'^2} &-\\displaystyle\\frac{f_y X'}{Z'} \\end{bmatrix}同理，对于特征点空间位置的优化，还需要将误差$e$对空间点$\\pmb{P}$进行求导，可以得到如下计算模型： \\frac{\\partial e}{\\partial \\pmb{P}}=\\frac{\\partial e}{\\partial \\pmb{P}'}\\frac{\\partial \\pmb{P}'}{\\partial\\delta \\pmb{P}} \\pmb{P}'=\\exp(\\pmb{\\xi}^\\land)\\pmb{P}=\\pmb{RP}+\\pmb{t} \\frac{\\partial e}{\\partial \\delta \\pmb{\\xi}}=-- \\begin{bmatrix} \\displaystyle\\frac{f_x}{Z'} & 0 & -\\displaystyle\\frac{f_x X'}{Z'^2} \\\\ 0 & \\displaystyle\\frac{f_y}{Z'} & -\\displaystyle\\frac{f_y Y'}{Z'^2} \\end{bmatrix}\\pmb{R}基于以上推导，采用g2o库实现相机位姿图优化，部分代码如下图所示。图优化时以下一个相机位姿和所有特征点空间坐标为节点，以下一个相机中的投影坐标为边。以RANSAC PnP结果为初值，调用g2o进行优化。 slam-VO.h文件中定义 Bundle Adjustment()函数程序如下：函数输入前一帧的三维信息和后一帧的两维信息，以及相机内参矩阵，则可得到优化后的相机变化位姿矩阵。函数定义中，首先初始化g2o，定义优化求解器，定义图优化的边和节点，定义误差函数等。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273void bundleAdjustment ( vector&lt; Point3f &gt; points_3d, vector&lt; Point2f &gt; points_2d, Mat K, Mat R, Mat t, Mat&amp; RR, Mat&amp; tt )&#123; // 初始化g2o typedef g2o::BlockSolver&lt; g2o::BlockSolverTraits&lt;6,3&gt; &gt; Block; // pose 维度为 6, landmark 维度为 3 Block::LinearSolverType* linearSolver = new g2o::LinearSolverCSparse&lt;Block::PoseMatrixType&gt;(); // 线性方程求解器 Block* solver_ptr = new Block (linearSolver); // 矩阵块求解器 g2o::OptimizationAlgorithmLevenberg* solver = new g2o::OptimizationAlgorithmLevenberg (solver_ptr); g2o::SparseOptimizer optimizer; optimizer.setAlgorithm ( solver ); // vertex g2o::VertexSE3Expmap* pose = new g2o::VertexSE3Expmap(); // camera pose Eigen::Matrix3d R_mat; R_mat &lt;&lt; R.at&lt;double&gt; ( 0,0 ), R.at&lt;double&gt; ( 0,1 ), R.at&lt;double&gt; ( 0,2 ), R.at&lt;double&gt; ( 1,0 ), R.at&lt;double&gt; ( 1,1 ), R.at&lt;double&gt; ( 1,2 ), R.at&lt;double&gt; ( 2,0 ), R.at&lt;double&gt; ( 2,1 ), R.at&lt;double&gt; ( 2,2 ); pose-&gt;setId ( 0 ); pose-&gt;setEstimate ( g2o::SE3Quat (R_mat,Eigen::Vector3d ( t.at&lt;double&gt; ( 0,0 ), t.at&lt;double&gt; ( 1,0 ), t.at&lt;double&gt; ( 2,0 ) ) ) ); optimizer.addVertex ( pose ); int index = 1; for ( const Point3f p:points_3d ) // landmarks &#123; g2o::VertexSBAPointXYZ* point = new g2o::VertexSBAPointXYZ(); point-&gt;setId ( index++ ); point-&gt;setEstimate ( Eigen::Vector3d ( p.x, p.y, p.z ) ); point-&gt;setMarginalized ( true ); optimizer.addVertex ( point ); &#125; // parameter: camera intrinsics g2o::CameraParameters* camera = new g2o::CameraParameters ( K.at&lt;double&gt; ( 0,0 ), Eigen::Vector2d ( K.at&lt;double&gt; ( 0,2 ), K.at&lt;double&gt; ( 1,2 ) ), 0 ); camera-&gt;setId ( 0 ); optimizer.addParameter ( camera ); // edges index = 1; for ( const Point2f p:points_2d ) &#123; g2o::EdgeProjectXYZ2UV* edge = new g2o::EdgeProjectXYZ2UV(); edge-&gt;setId ( index ); edge-&gt;setVertex ( 0, dynamic_cast&lt;g2o::VertexSBAPointXYZ*&gt; ( optimizer.vertex ( index ) ) ); edge-&gt;setVertex ( 1, pose ); edge-&gt;setMeasurement ( Eigen::Vector2d ( p.x, p.y ) ); edge-&gt;setParameterId ( 0,0 ); edge-&gt;setInformation ( Eigen::Matrix2d::Identity() ); edge-&gt;setRobustKernel( new g2o::RobustKernelHuber() ); optimizer.addEdge ( edge ); index++; &#125; optimizer.setVerbose ( false ); optimizer.initializeOptimization(); optimizer.optimize ( 5); Eigen::Isometry3d T = Eigen::Isometry3d ( pose-&gt;estimate() ); //RR、t为优化后的R和t RR=(Mat_&lt;double&gt;(3,3)&lt;&lt; T(0,0),T(0,1),T(0,2), T(1,0),T(1,1),T(1,2), T(2,0),T(2,1),T(2,2)); tt=(Mat_&lt;double&gt;(3,1)&lt;&lt; T(0,3),T(1,3),T(2,3));&#125; slam-VO.cpp文件中调用该函数程序如下：输入前后两帧的三维特征信息、相机内参、相机变换位姿，可以返回 优化后的位资矩阵。同样，需要判断两帧间平移是否到达一定程度，否则需要进行尺度修正。然后将每次循环得到的轨迹进行实时绘制。 123456789101112131415161718192021222324//BA优化bundleAdjustment ( points,points2, K,R,t, RR,tt );if ((scale&gt;0.1)&amp;&amp;(-tt.at&lt;double&gt;(2) &gt; -tt.at&lt;double&gt;(0)) &amp;&amp; (-tt.at&lt;double&gt;(2) &gt; -tt.at&lt;double&gt;(1)))&#123; if(abs(tt.at&lt;double&gt;(2)-t.at&lt;double&gt;(2))&lt;0.05) &#123; t_E = t_E + scale*(R_E*(-tt)); R_E = RR.inv()*R_E; pre_t=tt.clone(); &#125; else &#123; cout&lt;&lt;\"优化失败\"&lt;&lt;endl; t_E = t_E + scale*(R_E*(-t)); R_E = R.inv()*R_E; pre_t=t.clone(); &#125; &#125;else&#123; cout &lt;&lt; \"scale below 0.1, or incorrect translation\" &lt;&lt; endl;&#125;Point2f trace3 = Point2f(int(t_E.at&lt;double&gt;(0)) + 400, int(t_E.at&lt;double&gt;(2)) + 150);circle(trace, trace3, 1, Scalar(0,0,255), 1);//绘制BA优化后的轨迹 得到的BA优化轨迹如图中红色轨迹所示，右上角可以看出，累积误差较大的时候，BA优化的轨迹展示出了更好的性能。由于这里只考虑相邻两帧之间的优化，因此随着累积误差的增大，对极约束估计以及BA优化得到的轨迹效果都越来越差，需要通过局部地图以及后端的回环检测等方法来优化。","categories":[],"tags":[]},{"title":"数据结构与算法（20）串","slug":"数据结构与算法（20）串","date":"2020-04-04T01:24:46.000Z","updated":"2020-04-05T01:24:46.000Z","comments":true,"path":"2020/04/04/数据结构与算法（20）串/","link":"","permalink":"http://nekomoon404.github.io/2020/04/04/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8820%EF%BC%89%E4%B8%B2/","excerpt":"","text":"串或字符串属于线性结构，自然地可以直接利用向量或列表等序列结构加以实现。但字符串作为一种数据结构，特点极其鲜明，作为字符串基本组成元素的字符，种类通常不多，甚至可能极少。因此，为了高效地处理以字符串形式表示的海量数据，需要设计专门的处理方法。 本章直接利用C++所提供的字符数组，将重点几种在串匹配算法的设计、实现与优化。在此类应用中，更多地是以某一局部子串为单位，考查其对应的模式（pattern），因此也称作循模式访问（call-by-pattern）。 1.串及串匹配1.1.串字符串： 由来自字母表$\\sum$的字符所组成的有限序列，$S=”a_0\\,a_1\\,a_2\\dots\\,a_{n-1}”$，其中$a_i\\in \\sum, 0 \\le i &lt; n$。这里的$\\sum$是所有可用字符的集合，称作字符表（alphabet）。字符串S所含的字符的总数n，称作S的长度，记作$|S|=n$，长度为0的串称作空串（null string）。通常，字符的种类不多，而串长$=n&gt;&gt;|\\sum|$。 子串： 字符串中任一连续的片段，称作其子串（substring）。对于任意的$0\\le i \\le i+k &lt;n$，由字符串S中起始于位置i的连续k个字符组成的子串记作：S.substr(i, k) = S[i, i+k)。 起始于位置0，长度为k的子串称为前缀（prefix）：S.prefix(k) = S.substr(0,k) = S[0,k)。 终止于位置n-1、长度为k的子串称为后缀（suffix）：S.suffix(k) = S.substr(n-k,k) = S[n-k,n)。 空串是任何字符串的子串，也是任何字符串的前缀和后缀；任何字符串都是自己的子串，也是自己的前缀和后缀。此类子串。前缀和后缀分别称作平凡子串（trivial substring）、平凡前缀（trivial prefix）和平凡后缀（trivial suffix）。反之，字符串本身之外的所以非空子串。前缀和后缀，分别称作真子串（proper substring）、真前缀（proper prefix）和真后缀（proper suffix）。 判等： 字符串S[0,n)和T[0,m)称作相等，当且仅当二者长度相等（n=m），且对应字符分别相同。 ADT： 串结构主要的操作接口可归纳为下表： 操作接口 功能 length() 查询串的长度 charAt(i) 返回第i个字符 substr(i,k) 返回从第i个字符起，长度为k的子串 prefix(k) 返回长度为k的前缀 suffix(k) 返回长度为k的后缀 equal(T) 判断T是否与当前字符串相等 concat(T) 将T串接在当前字符串之后 indexOf(P) 若P是当前字符串的一个子串，则返回该子串的起始位置；否则返回-1 下面是一些实例： 1.2.串匹配在涉及字符串的众多实际应用中，模式匹配是最常使用的一项基本操作。如在生物信息处理领域需要在蛋白质序列中寻找特定的氨基酸模式，或在DNA序列中寻找特定的碱基模式。再如邮件过滤器也需要根据事先定义的特征串，通过扫描电子邮件的地址、标题及正文来识别垃圾邮件等等。 上述所有应用问题，本质上都可转化和描述为如下形式：如何在字符串数据中，检测和提取以字符串形式给出的某一局部特征。这类操作都属于串模式匹配（string pattern matching）范畴，简称串匹配。一般地，即： 对基于同一字符表的任何文本串T（|T| = n）和模式串P（|P| = m），判定T中是否存在某一子串与P相同，若存在（匹配），则报告该子串在T中的起始位置。 串的长度n和m本身通常都很多，但相对而言n更大，即满足$2 \\ll m \\ll n$。 根据具体应用的要求不同，串匹配问题有多种形式： 模式检测（pattern detection）问题：只关心是否存在匹配而不关心具体的匹配位置，如垃圾邮件的检测； 模式定位（pattern location）问题：若经判断的确存在匹配，则还需确定具体的匹配位置； 模式计数（pattern counting）问题：若有多处匹配，则统计出匹配子串的总数； 模式枚举（pattern enumeration）问题：在有多处匹配时，报告出所有匹配的具体位置。 鉴于串结构自身的特点，在设计和分析串模式匹配算法时也必须做特殊的考虑，一个重要的问题是：如何对任一串匹配算法的性能作出客观的测量和评估。不幸的是评估算法性能的常规口径和策略并不适用于这一问题，因为若假设文本串T和模式串P都是随机生成的，那么计算得到的匹配成功的概率往往极低。 实际上，有效涵盖成功匹配情况的一种简便策略是，随机选取文本串T，并从T中随机取出长度为m的子串作为模式串P，这即是文本将采用的评价标准。 2.蛮力算法蛮力串匹配是最直接的方法，不妨按自左向右的次序考查子串，逐个字符对比。如下图，模式串P的每一黑色方格对应于字符的一次匹配，每一灰色方格对应于一次失败，白色方格对应于未进行的一次对比。若经过检查，当前的m对字符均匹配，则意味着整体匹配成功，从而返回匹配子串的位置。 蛮力算法的正确性显而易见，既然只有在某一轮的m次比对全部成功之后才成功返回，故不致于误报；反过来，所有对齐位置都会逐一尝试，故亦不致漏报。 下面是蛮力算法的两个实现版本，二者原理相同、过程相仿，但分别便于引入后续的不同改进算法，故在此先做一比较。 版本一： 12345678910111213141516171819/************************************************************************************ * Text : 0 1 2 . . . i-j . . . . i . . n-1 * ------------------------|-------------------|------------ * Pattern : 0 . . . . j . . * |-------------------| ***********************************************************************************/int match ( char* P, char* T ) &#123; //串匹配算法（Brute-force-1） size_t n = strlen ( T ), i = 0; //文本串长度、当前接受比对字符的位置 size_t m = strlen ( P ), j = 0; //模式串长度、当前接受比对字符的位置 while ( j &lt; m &amp;&amp; i &lt; n ) //自左向右逐个比对字符 /*DSA*/&#123; /*DSA*/showProgress ( T, P, i - j, j ); getchar(); if ( T[i] == P[j] ) //若匹配 &#123; i ++; j ++; &#125; //则转到下一对字符 else //否则 &#123; i -= j - 1; j = 0; &#125; //文本串回退、模式串复位 /*DSA*/&#125; return i - j; //如何通过返回值，判断匹配结果？i-j&lt;=n-m就表示成功，反之失败&#125; 版本二： 123456789101112131415161718/************************************************************************************ * Text : 0 1 2 . . . i i+1 . . . i+j . . n-1 * ------------------------|-------------------|------------ * Pattern : 0 1 . . . j . . * |-------------------| ***********************************************************************************/int match ( char* P, char* T ) &#123; //串匹配算法（Brute-force-2） size_t n = strlen ( T ), i = 0; //文本串长度、与模式串首字符的对齐位置 size_t m = strlen ( P ), j; //模式串长度、当前接受比对字符的位置 for ( i = 0; i &lt; n - m + 1; i++ ) &#123; //文本串从第i个字符起，与 for ( j = 0; j &lt; m; j++ ) //模式串中对应的字符逐个比对 /*DSA*/&#123;showProgress ( T, P, i, j ); getchar(); if ( T[i + j] != P[j] ) break; //若失配，模式串整体右移一个字符，再做一轮比对 /*DSA*/&#125; if ( j &gt;= m ) break; //找到匹配子串 &#125; return i; //如何通过返回值，判断匹配结果？i&lt;=n-m就表示成功，反之失败&#125; 时间复杂度： 从理论上讲，蛮力算法至多迭代n-m+1轮，且各轮至多需要进行m次比对，故总共只需做不超过$(n-m+1)\\cdot m$次比对，其中成功的和失败的各有$(m-1)\\cdot (n-m+1)+1$和$n-m-2$次。因$m\\ll n$，渐进的时间复杂度应为$O(n\\cdot m)$。 蛮力算法的效率也并非总是如此低下，如上右图，若将模式串P左右颠倒，则每经一次比对都可排除文本串中的一个字符，故此类情况下的运行时间将为$O(n)$。实际上，此类最好（或接近最好）情况出现的概率并不很低，尤其是在字符表较大时。 3.KMP算法3.1.构思蛮力算法在最坏情况下所需时间，为文本串长度与模式长度的乘积，故无法应用于规模稍大的应用环境，很有必要改进。为此，不妨从最坏情况入手，最坏情况在于这里存在大量的局部匹配：每一轮的m次比对中，仅最后一次可能失配，而一旦发现失配，文本串、模式串的字符指针都要回退，并从头开始下一轮尝试。实际上，这列重复的字符比对操作没有必要。既然这些字符在前一轮迭代中已经接受过比对并且成功，我们也就掌握了它们的所有信息。接下来就要考虑如何利用这些信息，提高匹配算法的效率。 如下图，用T[i]和P[j]分别表示当前正在接受比对的一对字符。当本轮比对进行到最后一对字符并发现失配后，蛮力算法令两个字符指针同步回退（即令i = i - j + 1和j = 0），然后再从这一位置继续比对，然而事实上，指针i完全不必回退。 因为经过前一次的比对，我们已经知道子串T[i-j,i)完全由&#39;0&#39;组成，便可预测出：在回退之后紧接着的下一轮对比中，前j-1次比对必然都会成功。因此可直接令i保持不变，令j=j-1，然后继续比对。如此下一轮只需1次比对，共减少j-1次。这个操作可以理解为：令P相对于T右移一个单元，然后从前一失配位置继续比对。实际上这一技巧可以推广：利用以往的成功比对所提供的信息（记忆），不仅可避免文本串字符指针的回退，而且可使模式串尽可能大跨度地右移（经验）。下面是一个更一般的例子： 3.2.next表一般地，假设前一轮比对终止于T[i] $\\ne$ P[j]，按以上构思，指针i不必回退，而是将T[i]与P[t]对齐并开始下一轮对比。接下来考虑t的值应该取作多少。 如上图，经过此前一轮的对比，已经确定匹配的范围应为：P[o,j) = T[i-j,i)。 于是，若模式串P经适当右移之后，能够与T的某一（包含T[i]在内的）子串完全匹配，则一项必要条件就是：P[0,t) = T[i-t,i) = P[j-t,j)，亦即，在P[0,j)中长度为t的真前缀，应与长度为t的真后缀完全匹配，故t必来自集合： N(P,j)=\\{0\\le t \\le j \\,\\,\\,|\\,\\,\\,P[0,t)=P[j-t,j)\\}一般地，该集合可能包含多个这样的t，而需要注意的是，t的值仅取决于模式串P以及前一轮比对的首个失配位置P[j]，而与文本串T无关。若下一轮比对将从T[i]与P[t]的比对开始，这等效于将P右移j-t个单元，位移量与t成反比。因此为保证P与T的对齐位置（指针i）绝不倒退，同时又不致遗漏任何可能的匹配，应在集合N(P,j)中挑选最大的t，即当有多个右移方案时，应该保守地选择其中移动距离最短者。 于是令：next[j] = max( N(P,j) )，则一旦发现P[i]与T[i]失配，即可转而将P[ next[i] ]与T[i]彼此对准，并从这一位置开始继续下一轮比对。 既然集合N(P,j)仅取决于模式串P以及失配位置j，而与文本串无关，作为其中的最大元素，next[j]也必然具有这一性质。对于任一模式串P，不妨通过预处理提前计算出所有位置j所对应的next[j]值，并整理为表格以表此后反复查询。 3.3.KMP算法上述思路可整理为如下代码，即著名的KMP算法。对照此前的蛮力算法，只是在else分支对失配情况的处理手法有所不同，这也是KMP算法的精髓所在。 1234567891011121314151617int match ( char* P, char* T ) &#123; //KMP算法 int* next = buildNext ( P ); //构造next表 int n = ( int ) strlen ( T ), i = 0; //文本串指针 int m = ( int ) strlen ( P ), j = 0; //模式串指针 while ( j &lt; m &amp;&amp; i &lt; n ) //自左向右逐个比对字符 //*DSA*/ &#123; //*DSA*/ showProgress ( T, P, i - j, j ); //*DSA*/ printNext ( next, i - j, strlen ( P ) ); //*DSA*/ getchar(); printf ( \"\\n\" ); */ if ( 0 &gt; j || T[i] == P[j] ) //若匹配，或P已移出最左侧（两个判断的次序不可交换） &#123; i ++; j ++; &#125; //则转到下一字符 else //否则 j = next[j]; //模式串右移（注意：文本串不用回退） //*DSA*/ &#125; delete [] next; //释放next表 return i - j;&#125; next[0] = -1 只要j&gt;0，则必有$0\\in N(P,j)$。此时N(P,j)非空，从而可以保证“ 在其中取最大值 ”这一操作的确可行。但反过来，若j=0，则即便集合N(P,j)可以定义，也必是空集。反观串匹配的过程，若在某一轮比对中首对字符即失配，则应将P直接右移一个字符，然后启动下一轮比对。如下表，不妨假想地在P[0]的左侧“ 附加 ”一个P[-1]，且该字符与任何字符都是匹配的，就实际效果而言，这一处理方法完全等同于“ 令next[0] = -1 ”。 next[j+1] 若已知next[0,j]，如何才能递推地计算出next[j+1]？若next[j]=t，则意味着在P[0,j]中，自匹配的真前缀和真后缀的最大长度为t，故必有next[j=1] $\\le$ next[j]+1，当且仅当P[j]=P[t]时，取等号。如下图： 一般地，若P[j] $\\ne$ P[t]，由next表的功能定义，next[j+1]的下一候选者应该依次是： next[ next[j] ] + 1，next[ next[ next[j] ] ] + 1，…… 因此只需反复用next[t]替换t（即令t = next[t]），即可按优先次序遍历以上候选者；一旦发现P[j]与P[t]匹配（含与P[t=-1]的通配），即可令next[j+1] = next[t] + 1。 既然总有next[t] &lt; t，故在此过程中t必然严格递减；同时，即便t降低至0，亦必然会终止于通配的next[0] = -1，而不致下溢，如此该算法的正确性完全可以保证。 构造next表： 按照以上思路，可实现next表构造算法如下。next表的构造算法与KMP算法几乎完全一致，实际上按照以上分析，这一构造过程完全等效于串的自我匹配，因此两个算法在形式上的近似亦不足为怪。 1234567891011121314int* buildNext ( char* P ) &#123; //构造模式串P的next表 size_t m = strlen ( P ), j = 0; //“主”串指针 int* N = new int[m]; //next表 int t = N[0] = -1; //模式串指针 while ( j &lt; m - 1 ) if ( 0 &gt; t || P[j] == P[t] ) &#123; //匹配 j ++; t ++; N[j] = t; //此句可改进... &#125; else //失配 t = N[t]; //*DSA*/printString ( P ); printf ( \"\\n\" ); //*DSA*/printNext ( N, 0, m ); return N;&#125; 性能分析： 由上可见，KMP算法借助next表可避免大量不必要的字符比对操作，但这意味着渐渐意义上的时间复杂度会有实质改进嘛？ 为此的证明需要一些技巧，若令k = 2i - j，并考查k在KMP算法过程中的变化趋势，则不难发现，while循环每迭代一轮，k都会严格递增。对于while循环内部的if - else分支，无非两种情况：若转入if分支，则i和j同时加一，于是k = 2i -j必将增加；反之若转入else分支，则尽管i保持不变，但在赋值j = next[j] 之后j必然减少，于是k = 2i -j也必然增加。 纵观算法的整个过程：启动时有i =j =0，即k =0；算法结束时$i\\le n$或$j \\ge 0$，故有$k\\le 2n$。在此期间尽管整数k从0开始持续地严格递增，但累计增幅不超过$2n$，故while循环至多执行$2n$轮。另外，while循环体内部不含任何循环或调用，故只需$O(1)$时间，因此若不计构造next表所需的时间，KMP算法本身的运行时间不超过$O(n)$。也就是说，尽管可能有$\\Omega(n)$个对齐位置，但就分摊意义而言，在每一对齐位置仅需$O(1)$次对比。 既然next表构造算法的流程与KMP算法并无实质区别，故仿照上述分析可知，next表的构造仅需$O(m)$时间，综上，KMP算法的总体运行时间为$O(n+m)$。 3.4.改进尽管以上KMP算法已可保证线性的运行时间，但在某些情况下仍有进一步改进的余地。如下面的例子，按照此前定义的next表，仍会进行多次本不必要的字符比对操作。前一轮对比因T[i] = &#39;1&#39; $\\ne$ &#39;0&#39; = P[3]失配而中断，接下来KMP算法将依次将P[2]、P[1]和P[0]与T[i]对准并做比对。 不难发现原因出在 P[3] = P[2] =P[1] = &#39;0&#39;，而此前比对已发现T[i] $\\ne$ P[3]，那么继续将T[i]和那些与P[3]相同的字符做比对，就是徒劳无功的。 就算法策略而言，引入next表的实质作用在于帮助我们利用以往成功比对所提供的“ 经验 ”，而实际上，此前已进行过的比对还远远不止这些，确切地说还包括那些失败的比对——作为“ 教训 ”。为把这类“ 负面 ”信息引入next表，只需将集合$N(P,j)$的定义修改为： N(P,j)=\\{0\\le t < j\\,\\,\\,|\\,\\,\\,P[0,t)=P[j-t,j)且P[t] \\ne P[j]\\}也就是说，除“ 对应于自匹配长度 ”以外，t只要还同时满足“ 当前字符对不匹配 ” 的必要条件，方能归入集合$N(P,j)$并作为next表项的候选。 相应地，原next表构造算法也需稍作修改如下： 12345678910111213int* buildNext ( char* P ) &#123; //构造模式串P的next表（改进版本） size_t m = strlen ( P ), j = 0; //“主”串指针 int* N = new int[m]; //next表 int t = N[0] = -1; //模式串指针 while ( j &lt; m - 1 ) if ( 0 &gt; t || P[j] == P[t] ) &#123; //匹配 N[j] = ( P[++j] != P[++t] ? t : N[t] ); //注意此句与未改进之前的区别 &#125; else //失配 t = N[t]; /*DSA*/printString ( P ); printf ( \"\\n\" ); /*DSA*/printNext ( N, 0, strlen ( P ) ); return N;&#125; 改进后的算法与原算法的唯一区别在于，每次在P[0,j)中发现长度为t的真前缀和真后缀相互匹配之后，还需进一步检查P[j]是否等于P[t]。唯有在P[j] $\\ne $ P[t]时，才能将t赋予next[j]；否则需转而代之以next[t]，改进后next表的构造算法同样只需$O(m)$时间。 将改进后的KMP算法应用于前面的例子，在首轮比对因T[i] = &#39;1&#39; $\\ne$ &#39;0&#39; = P[3]失配而中断之后，将随机以P[ next[3] ] = P[-1]与T[i]对齐，并进行下一轮比对。这样就等同于聪明且安全地跳过了三个不必要的对齐位置。 4.BM算法4.1.构思KMP算法的思路可概括为：当前比对一旦失配，即利用此前的比对所提供的信息，尽可能长距离地移动模式串。其精妙之处在于，无需显式地反复保存或更新比对的历史，而是独立于具体的文本串，事先根据模式串预测出所有可能出现的失配情况，并将这些信息记录于next表中。 回顾之前的知识不难发现：串匹配 = x次失败的对齐 + 0/1次成功的对齐，这样与其说要加速匹配，不如说是要加速失败——尽快排除失败的对齐。就单个对齐的位置的排除而言：平均仅需常数次比对（只要$|\\sum|$不致太小，单次比对成功概率足够低），且具体的比对位置及次序无所谓。然后就排除更多后续对齐位置而言，不同的对比位置及次序，作用差异极大。通常是越靠前的位置，作用越小；越靠后的位置，作用越大。 BM算法与KMP算法类似，二者的区别仅在于预测和利用“ 历史 ”信息的具体策略与方法。BM算法中，模式串P与文本串T的对准位置依然“ 自左向右 ”推移，而在每一对准位置确实“ 自右向左 ”地逐一比对各字符。具体地，在每一轮自右向左的比对过程中，一旦发现失配，则将P右移一定距离并再次与T对准，然后重新一轮自右向左的扫描比对。 BM算法的主题框架，可实现为如下代码： 123456789101112131415161718192021222324//////////////////////////////////////////////////////////////////////////// Boyer-Moore算法//////////////////////////////////////////////////////////////////////////void ShowProgress ( String, String, int, int );#define CARD_CHAR_SET 256 //Cardinality of charactor setint* BuildBC ( String ); //构造Bad Charactor Shift表int* suffixes ( String );int* BuildGS ( String ); //构造Good Suffix Shift表int match ( char* P, char* T ) &#123; //Boyer-Morre算法（完全版，兼顾Bad Character与Good Suffix） int* bc = buildBC ( P ); int* gs = buildGS ( P ); //构造BC表和GS表 size_t i = 0; //模式串相对于文本串的起始位置（初始时与文本串左对齐） while ( strlen ( T ) &gt;= i + strlen ( P ) ) &#123; //不断右移（距离可能不止一个字符）模式串 int j = strlen ( P ) - 1; //从模式串最末尾的字符开始 while ( P[j] == T[i + j] ) //自右向左比对 if ( 0 &gt; --j ) break; /*DSA*/showProgress ( T, P, i, j ); printf ( \"\\n\" ); getchar(); if ( 0 &gt; j ) //若极大匹配后缀 == 整个模式串（说明已经完全匹配） break; //返回匹配位置 else //否则，适当地移动模式串 i += __max ( gs[j], j - bc[ T[i + j] ] ); //位移量根据BC表和GS表选择大者 &#125; delete [] gs; delete [] bc; //销毁GS表和BC表 return i;&#125; 这里采用了蛮力算法后一版本的方式，借助整数i和j指示文本串中当前的对齐位置T[i]和模式串中接受比对的字符P[j]。一旦局部失配，采用两种启发式策略确定最大的安全移动距离，为此需要经过预处理，根据模式串P整理出坏字符和好后缀两类信息。与KMP算法一样，算法过程中指针i始终单调递增；相应地，P相对于T的位置也绝不回退。 4.2.坏字符策略如下图中(a)和(b)，若模式串P当前在文本串T中的对齐位置为i，且在这一轮自右向左将P与substr(T,i,m)的比对过程中，在P[j]处首次发现失配：T[i+j] = &#39;X&#39; $\\ne$ &#39;Y&#39; = P[j]，则将&#39;X&#39;称作坏字符（bad character）。接下来考虑应该选择P中哪个字符对准T[i+j]。 若P与T的某一（包括T[i+j]在内的）子串匹配，则必然在T[i+j] = &#39;X&#39;处匹配；反之，若与T[i+j]对准的字符不是&#39;X&#39;，则必然失配。故如图(c)，只需找出P中的每一字符&#39;X&#39;，分别于T[i+j] = &#39;X&#39;对准，并执行一轮自右向左的扫描比对。 若P中包含多个&#39;X&#39;，其实并没有必要逐一尝试，逐一对比并无法确保文本串指针i永不回退。一种简便而高效的做法是：仅尝试P中最靠右的字符&#39;X&#39;（若存在）。如此便可在确保不致遗漏匹配的前提下，始终单向地滑动模式串，如上图(c)若P中最靠右的字符&#39;X&#39;为P[k] = &#39;X&#39;，则P的右移量即为j-k。 bc[ ]表： 对于任一给定的模式串P，k值只取决于字符T[i+j] = &#39;X&#39;，因此可将其视作从字符表到整数（P中字符的秩）的一个函数： bc(c)=\\begin{cases} k,\\quad &若P[k]=c，且对所有的i>k都有P[i] \\ne c\\\\ -1,\\quad &若P[\\,\\,\\,]中不含字符c \\end{cases}故如当前对齐位置为i，则一旦出现坏字符P[j] = &#39;Y&#39;，即重新对齐与：i += j - bc[ T[i+j] ]，并启动下一轮比对。为此可预先将函数bc()整理为一份查询表，称作BC表。 接下来考虑BC表中可能出现的两种特殊情况：若P根本就不含坏字符&#39;X&#39;，如上图(d)，则模式串整体移过失配位置T[i+j]，用P[0]对准T[i+j+1]，再启动下一轮比对，此类字符在BC表的对应项置为-1，其效果也等同于在模式串的最左端，增添一个通配符。若P中含有坏字符&#39;X&#39;，但其中最靠右者的位置也可能太靠右，以至于j-k &lt; 0，相当于左移模式串。显然这是不必要的——匹配算法能进行到此，则此前左侧的所有位置已被显式或隐式地否定排除了，因此只需将P串右移一个单位，然后启动下一轮自右向左的比对。 以由大写字母和空格组成的字符表为例，与模式串”DATA STRUCTURES”相对应的BC表如下所示： 按照以上思路，BC表的构造算法可实现如下： 1234567891011121314//***********************************************************************************// 0 bc['X'] m-1// | | |// ........................X***************************************// .|&lt;------------- 'X' free ------------&gt;|//***********************************************************************************int* buildBC ( char* P ) &#123; //构造Bad Charactor Shift表：O(m + 256) int* bc = new int[256]; //BC表，与字符表等长 for ( size_t j = 0; j &lt; 256; j ++ ) bc[j] = -1; //初始化：首先假设所有字符均未在P中出现 for ( size_t m = strlen ( P ), j = 0; j &lt; m; j ++ ) //自左向右扫描模式串P bc[ P[j] ] = j; //将字符P[j]的BC项更新为j（单调递增）——画家算法 /*DSA*/printBC ( bc ); return bc;&#125; 如上的算法在对BC初始化之后，对模式串P做一遍线性扫描，并不断用当前字符的秩更新BC表中的对应项。因为是按秩递增的次序从左到右扫描，故只要字符c在P中出现过，则最终的bc[c]必会记录下其中最靠右者的秩，这类算法也因此被称作“ 画家算法 ”（painter’s algorithm）。 算法的运行时间可以分为两部分，分别消耗于其中的两个循环：前者是对字符表$\\sum$中的每个字符分别做初始化，时间量不超过$O(|\\sum|)$；后一循环对模式串P做一轮扫描，其中每个字符消耗$O(1)$时间，需要$O(m)$时间。因此BC表构造消耗时间为$O(|\\sum|+m)$。 下面是一个运用BM_BC算法的实例： 整个过程总共做过6次成功的比对（黑色字符）和4次失败的比对（白色字符），累计10次，文本串的每个有效字符平均为10/11不足一次。 复杂度： BM算法本身进行串模式匹配所需的时间与具体的输入十分相关，将文本串和模式串的长度分别记为n和m，则在通常情况下的实际运行时间往往低于$O(n)$。在最好情况下，每经过常数次比对，就可以将模式串右移m个字符（即整体右移），如下图中的一类情况（对应蛮力算法中的最坏情况），只需$n/m$次比对算法即可终止，故最好情况下BM算法只需$O(n/m)$时间。 如将模式串P左右颠倒，则对应着最坏的一类情况，在每一轮比对中，P总要完整地扫描一遍才发现失配并向右移动一个字符，运行时间需要$O(n \\times m)$。 4.3.好后缀策略参照KMP算法的改进思路不难发现，坏字符策略仅利用了此前（最后一次）失败比对所提供的“ 教训 ”。而实际上在此之前，还做过一系列成功的比对，但这些“ 经验 ”却被忽略了。如上图，每当在P[0] =&#39;1&#39; $\\ne$ &#39;0&#39; 处失配，自然地想到应该将其替换成字符&#39;0&#39;。但既然本轮比对过程中已有大量字符&#39;0&#39;的成功匹配，则无论将P[0]对准其中的任何一个都注定会失配。故此时更应明智地将P整体移过这段区间，直接以P[0]对准T中尚未接受比对的首个字符，如此算法的运行时间将有望降回至$O(n)$。 好后缀： 每轮比对中的若干次（连续的）成功匹配，都对应于模式串P的一个后缀，称作“ 好后缀 ”（good suffix），若要改进BM算法，必须利用好后缀提供的“ 经验 ”。一般地，如下图的(a)和(b)所示，设本轮自右向左的扫描终止于失配位置：T[i+j] = &#39;X&#39; $\\ne$ &#39;Y&#39; = P[j]。若分别记： W = substr(T, i+j+1, m-j-1) = T[i+j+1, m+i) U = suffix(P, m-j-1) = P[j+1, m) 好后缀U长度为m-j-1，故只要j &lt;= m-2，则U必非空，且有U = W。接下来就应考虑：根据好后缀所提供的信息应如何确定，P中有哪个（哪些）字符值得与上一失配字符T[i+j]对齐，然后启动下一轮比对呢？ 如上图(c)设存在一整数k，使得在将P右移j - k个单元，并使P[k]与T[i+j]相互对齐之后，P能够与文本串T的某一（包含T[m+i-1]在内的）子串匹配，亦即：P = substr(T, i+j-k, m) = T[i+k-k, m+i+j-k)。 于是，若记：V(k) = substr(P, k+1, m-j-1) = P[k+1, m-j+k)，则必然有：V(k) = W = U，也就是说，若值得将P[k]与T[i+j]对齐并做新的一轮比对，则P的子串V(k)首先必须与P自己的后缀U相互匹配。 还有另一必要条件：P中的这两自匹配子串的前驱字符不得相等，即P[k] != P[j]，否则在对齐位置也注定不会出现与P的整体匹配。若模式串P中同时存在多个满足上述必要条件的子串V(k)，则不妨选取其中最靠右者（对应于最大的k，最小的右移距离j-k），这两点都与KMP算法的处理类似。 gs[ ]表： 如上图(c)若满足上述必要条件的子串V(k)起始于P[k+1]，则模式串对应的右移量就是j-k。而k本身仅取决于模式串P以及j值，因此又可仿照KMP算法通过预处理将模式串P转换为另一张查找表gs[0, m)，其中gs[j] = j-k分别记录对应的位移量。 若P中没有任何子串V(k)可与好后缀U完全匹配，则需要从P的所有前缀中，找出可与U的某一（真）后缀相匹配的最长者，作为V(k)，并取gs[j] = m - |V(k)|。如下例是模式串P = “ICED RICE PRICE”对应的GS表，gs[5] = 12 就意味着：一旦在P[5] = &#39;R&#39;处发生失配，则应将模式串P整体右移12个字符，然后继续启动下一轮比对，也可以等效地认为，以P[5-12] = P[-7]对准文本串中失配的字符，或以P[0]对准文本串中尚未对准过的最左侧字符。 下面是一个运行好后缀策略进行串匹配的实例： 整个过程中总共做10次成功的比对（黑色字符）和2次失败的比对（灰色字符），累计12次比对，文本串的每个字符，平均（12/13）不足一次。 复杂度： 同时结合BC表和GS表两种启发策略，加快模式串相对于文本串的右移速度，可以证明对于匹配失败的情况，总体比对的次数不致超过$O(n)$。若不排除完全匹配的可能，则该算法在最坏情况下的效率，有可能退化至与蛮力算法相当，所幸只要做些简单的改进，依然能够保证总体的比对次数不超过线性。综上在兼顾了坏字符与好后缀两种策略之后，BM算法的运行时间为$O(n+m)$。 4.4.gs[ ]表构造算法：蛮力算法： 一个很直接的方法就是：对于每个好后缀P(j, m)，按照自后向前（k从j-1递减至0）的次序，将其与P的每个子串P(k ,m+k-j)逐一对齐，并核对是否出现有匹配的子串，一旦发现，对应的位移量即是gs[j]的取值。然而这里共有$O(m)$个好后缀，各需与$O(m)$个子串对齐，每次对齐后在最坏情况下需要比对$O(m)$次，因此该“算法”可能需要$O(m^3)$的时间。 MS[ ]串与ss[ ]表： 实际上构造gs[ ]表仅需线性的时间，为此需要引入ss[ ]表。如上图，对于任一整数j $\\in$ [0, m)，在P[0, j)的所有后缀中，考查那些与P的某一后缀匹配者。若将其中的最长者记作MS[j]，则ss[j]就是该串的长度|MS[j]|。当MS[j]不存在时，取ss[j] = 0。综上可定义ss[j]为： ss[j]=\\max \\{ 0\\le s \\le j+1\\,\\,\\,|\\,\\,\\,P(j-s,j]=P[m-s,m) \\}特别地，当j =m-1时，必有s = m——此时，有P(-1, m-1] = P[0, m)。如下例是模式串P = &quot;ICED RICE PRICE&quot;所对应的ss[]表： 由ss[ ]表构造gs[ ]表： 如下图所示，任一字符P[j]所对应的ss[j]值，可分两种情况提供有效的信息。 第一种情况如图(a)，设该位置j满足：ss[j] = j+1，即MS[j]就是整个前缀P[0,j]。此时，对于P[m-j-1]左侧的每个字符P[i]而言，对应于4.3节中图11.12(d)所示的情况，m-j-1都应该是gs[i]取值的一个候选。 第二种情况如图(b)所示，设该位置j满足：ss[j] &lt;= j，即MS[j]只是P[0, j]的一个真后缀。同时既然MS[j]是极长的，故必有：P[ m - ss[j] -1 ] != P[ j - ss[j] ] 。这就意味着此时的字符P[m - ss[j] - 1]恰好对应于如4.3节中的图11.12(c)的情况，因此m-j-1也应是gs[m - ss[j] - 1]取值的一个候选。 反过来，根据此前的定义，每一位置i所对应的gs[i]值只可能来自与以上候选。进一步地，既然gs[i]的最终取值是上述候选中的最小（最安全者），故仿照构造bc[]表的画家算法，累计用时将不超过$O(m)$。 ss[ ]表的构造： ss[]表是构造gs[] 表的基础与关键，若采用蛮力策略，对每个字符P[j]都做一趟扫描比对，直到出现失配，如此累计需要$O(m^2)$时间。 为了提高效率，不妨自后想前地逆向扫描，并逐一计算出各字符P[j]对应的ss[j]值。如下图所示，此时必有P[j] = P[m-hi+j-1]，故可利用此前已计算出的ss[m-hi+j-1]，分两种情况快速地导出ss[j]。在此期间，只需动态地记录当前的极长匹配后缀：P(lo, hi] = P[m -hi +lo, m)。 第一种情况如图(a)，设：ss[m - hi +j -1] &lt;= j-lo，此时ss[m - hi +j -1]也是ss[j]可能的最大取值，于是便可直接得到：ss[j] = ss[m - hi +j -1] 第二种情况如图(b)，设：j-lo &lt; ss[m - hi +j -1]，此时至少仍有：P(lo, j] = P[m - hi + lo, m -hi +j)，故只需将P(j - ss[m - hi + j - 1], lo] 与 P[m - hi + j - ss[m -hi + j -1], m - hi + lo)做一比对，也可确定ss[j]。当然这种情况下极大匹配串的边界lo和hi也需要相应左移。 以上构思只要实现得当，也只需$O(m)$时间即可构造出ss[]表。 算法实现： 按照以上思路，GS表的构造算法可实现如下代码： 123456789101112131415161718192021222324252627282930313233int* buildSS ( char* P ) &#123; //构造最大匹配后缀长度表：O(m) int m = strlen ( P ); int* ss = new int[m]; //Suffix Size表 ss[m - 1] = m; //对最后一个字符而言，与之匹配的最长后缀就是整个P串// 以下，从倒数第二个字符起自右向左扫描P，依次计算出ss[]其余各项 for ( int lo = m - 1, hi = m - 1, j = lo - 1; j &gt;= 0; j -- ) if ( ( lo &lt; j ) &amp;&amp; ( ss[m - hi + j - 1] &lt; j - lo ) ) //情况一 ss[j] = ss[m - hi + j - 1]; //直接利用此前已计算出的ss[] else &#123; //情况二 hi = j; lo = __min ( lo, hi ); while ( ( 0 &lt;= lo ) &amp;&amp; ( P[lo] == P[m - hi + lo - 1] ) ) //二重循环？ lo--; //逐个对比处于(lo, hi]前端的字符 ss[j] = hi - lo; &#125; //*DSA*/ printf ( \"-- ss[] Table -------\\n\" ); //*DSA*/ for ( int i = 0; i &lt; m; i ++ ) printf ( \"%4d\", i ); printf ( \"\\n\" ); //*DSA*/ printString ( P ); printf ( \"\\n\" ); //*DSA*/ for ( int i = 0; i &lt; m; i ++ ) printf ( \"%4d\", ss[i] ); printf ( \"\\n\\n\" ); return ss;&#125;int* buildGS ( char* P ) &#123; //构造好后缀位移量表：O(m) int* ss = buildSS ( P ); //Suffix Size table size_t m = strlen ( P ); int* gs = new int[m]; //Good Suffix shift table for ( size_t j = 0; j &lt; m; j ++ ) gs[j] = m; //初始化 for ( size_t i = 0, j = m - 1; j &lt; UINT_MAX; j -- ) //逆向逐一扫描各字符P[j] if ( j + 1 == ss[j] ) //若P[0, j] = P[m - j - 1, m)，则 while ( i &lt; m - j - 1 ) //对于P[m - j - 1]左侧的每个字符P[i]而言（二重循环？） gs[i++] = m - j - 1; //m - j - 1都是gs[i]的一种选择 for ( size_t j = 0; j &lt; m - 1; j ++ ) //画家算法：正向扫描P[]各字符，gs[j]不断递减，直至最小 gs[m - ss[j] - 1] = m - j - 1; //m - j - 1必是其gs[m - ss[j] - 1]值的一种选择 //*DSA*/ printGS ( P, gs ); delete [] ss; return gs;&#125; 5.算法纵览本文针对串匹配问题，依次介绍了蛮力、KMP、基于BC表、综合BC表与GS表等四种典型算法，其渐进复杂度的跨度范围，可概括为： KMP算法相比于蛮力算法的优势在于无论何种情况，时间效率均稳定在$O(n+m)$，因此在蛮力算法效率接近或达到最坏的$O(nm)$时，KMP算法的优势才会十分明显。仅采用坏字符启发策略（BC）的BM算法，时间效率介于$O(nm)$至$O(n/m)$之间，其最好情况与最坏情况相差悬殊。结合了好后缀策略（BC+GS）后的BM算法，则介于$O(n+m)$与$O(n/m)$之间，其在改进最低效率的同时也保持了最高效率的优势。 单次比对成功概率： 有趣的是，单次比对成功的概率，是决定串匹配算法时间效率的一项关键因素。纵观以串匹配算法，在每一对齐位置所进行的一轮比对中，仅有最后一次可能失败：反之此前的所有比对（若的确进行过）必然都是成功的。而各种算法的最坏情况均可概括为：因启发策略不够精妙甚至不当，在每一对齐位置都需进行多达$\\Omega(n)$次成功的比对，另加最后一次失败的比对。 若将单次比对成功的概率记作Pr，以上串匹配算法的时间性能随Pr的变化趋势，大致如下图所示，其中纵坐标为运行时间，分为$O(n/m)$、$O(n+m)$、$O(n*m)$三挡。（图线只是示意大致的增长趋势）由于对于同一算法，消耗于每一对齐位置的平均时间成本随Pr的提高而增加，因此计算时间与Pr都具有单调正相关的关系。 字符表长度： 实际上在所有字符均等概率出现的情况下，Pr的取值将主要决定于字符表的长度$|\\sum|$，并与之成反比关系：字符越长，其中任何一对字符匹配的概率越低。在通常情况下，蛮力算法实际的运行效率并不算太低。而不同的串匹配算法也因此有各自适用的场合。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"串匹配","slug":"串匹配","permalink":"http://nekomoon404.github.io/tags/%E4%B8%B2%E5%8C%B9%E9%85%8D/"},{"name":"KMP算法","slug":"KMP算法","permalink":"http://nekomoon404.github.io/tags/KMP%E7%AE%97%E6%B3%95/"},{"name":"BM算法","slug":"BM算法","permalink":"http://nekomoon404.github.io/tags/BM%E7%AE%97%E6%B3%95/"}]},{"title":"数据结构与算法（19）优先级队列","slug":"数据结构与算法（19）优先级队列","date":"2020-03-28T08:42:05.000Z","updated":"2020-04-01T15:49:07.758Z","comments":true,"path":"2020/03/28/数据结构与算法（19）优先级队列/","link":"","permalink":"http://nekomoon404.github.io/2020/03/28/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8819%EF%BC%89%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97/","excerpt":"","text":"[TOC] 此前的搜索树结构和词典结构，都支持覆盖数据全集的访问和操作，其中存储的每一数据对象都可作为查找和访问目标，为此搜索树结构需要在所有元素之间定义并维护一个显式的全序关系。优先级队列，这类结构则将操作对象限定于当前的全局极值者。这种根据数据对象之间相对优先级对其进行访问的方式，称作循优先级访问（call-by-priority）。 “全局极值”隐含了“所有元素可相互比较”这一性质，但优先级队列并不会也不必动态维护这个全序，却转而维护一个偏序（partial order）关系，如此不仅足以高效地支持仅针对极值对象的接口操作，更可有效地控制整体计算成本。作为不失高效率的轻量数据结构，对于常规的查找、插入或删除操作，优先级队列的效率并不低于此前的结构；而对于数据集的批量构建及相互合并等操作，其性能却更胜一筹。 1.接口定义除了作为存放数据的容器，数据结构还应能够按某种约定的次序动态地组织数据，以支持高效的查找和修改操作，如遵循“先进先出”原则的队列，而有些实际情况则要按某种优先级原则，如医院抢救最危急的病人。 从数据结构的角度看，无论是待排序节点的数值、超字符的权重，还是时间的发生时间，数据项的某种属性只要可以相互比较大小，则这种大小关系即可称作优先级（priority）。而按照事先约定的优先级，可以始终高效查找并访问优先级最高数据项的数据结构，也统称作优先级队列（priority queue）。 仿照词典结构，也讲优先级队列中的数据项称作词条（entry），而与特定优先级相对应的数据属性，也称作关键码（key）。作为确定词条优先级的依据，关键码之间必须可以比较大小（词典结构仅要求关键码支持判等操作）。因此对于优先级队列，必须以比较器的形式兑现对应的优先级关系，为了简化起见，这里假定关键码或者可直接比较，或者已重载了对应的操作符。 优先级队列作为一类独特数据的意义恰恰在于，通过转而维护词条的一个偏序关系，不仅依然可以支持对最高优先级词条的动态访问，而且可将相应的计算成本控制在足以令人满意的范围之内。 优先级队列接口的定义如下表： 操作接口 功能描述 size() 报告优先级队列的规模，即其中词条的总数 insert() 将指定词条插入优先级队列 getMax() 返回优先级最大的词条（若优先级队列非空） delMax() 删除优先级最大的词条（若优先级队列非空） 这里以模板类PQ的形式给出优先级队列的操作接口定义，这一组基本的ADT接口可能有不同的实现方式，故这里均以虚函数形式统一描述这些接口，以便在不同的派生类中具体实现。 12345template &lt;typename T&gt; struct PQ &#123; //优先级队列PQ模板类 virtual void insert ( T ) = 0; //按照比较器确定的优先级次序插入词条 virtual T getMax() = 0; //取出优先级最高的词条 virtual T delMax() = 0; //删除优先级最高的词条&#125;; 2.实现2.1.尝试考虑用此前学过的数据结构来实现优先级队列，要兼顾efficiency和cost，以下考虑了向量Vector，有序向量Sorted Vector，列表List，有序列表Sorted List和平衡二叉搜索树BBST，但基于它们实现的优先级队列都不能实现高效率和低cost的兼顾。 Vector: Sorted Vector： List： Sorted List： BBST： 对于AVL、Splay、Red-black这三种数据结构，查找、插入和删除三个接口均只需$O(\\log n)$时间，但是BBST的功能却远远超出了PQ的需求，可以这么理解： PQ=1\\times insert()+0.5\\times search()+0.5\\times remove()=\\frac{2}{3}\\times BBST若只需查找极单元，则不必维护所有元素之间的全序关系，偏序足矣，因此有理由相信，存在某种更为简单、维护成本更低的实现方式，使得各功能接口时间复杂度依然为$O(\\log n)$，而且实际效率更高。 2.2.完全二叉堆有限偏序的极值必然存在，借助堆（heap）结构可以维护一个偏序，堆有多种实现形式，这里采用一种最基本的形式——完全二叉堆（complete binary heap），它具有结构性与堆序性。 结构性：其逻辑结构等同于完全二叉树（complete binary tree），即平衡因子处处非负（只能是0或1）的AVL树。因此由n个词条组成的完全二叉堆的高度$h=\\lfloor \\log_2 n \\rfloor=O(\\log n)$。 堆序性：堆顶以外的每个节点都不大于其父节点，称为大顶堆；还可定义为堆顶以外的每个节点都不小于其父节点，称为小顶堆。 尽管二叉树不属于线性结构，但作为特例的完全二叉树，却与向量有着紧密的对应关系。完全二叉堆的拓扑联接结构，完全由其规模n确定。按照层次遍历的次序，每个节点都对应于唯一的编号。故若将所有节点组织为一个向量，则堆中所有节点（编号）与向量各单元（秩）也将彼此一一对应。 这一实现方式的优势首先体现在，各节点在物理上连续排列，故总共仅需$O(n)$空间，而且更重要的是，利用各节点的编号（或秩），也可便捷地判别父子关系，对于任意节点v，必然满足： 若v有左孩子，则$i (lchild(v))=2*i(v)+1$; 若v有右孩子，则$i (rchild(v))=2*i(v)+2$; 若v有父节点，则$i(parent(v))=\\lfloor(i(v)-1)/2\\rfloor=\\lceil i(v)/2)\\rceil-1$ 最后，由于向量支持低分摊成本的扩容调整，故随着堆的 规模和内容不断地动态调整，除标准接口以外的操作所需的时间可以忽略不计。 2.3.实现宏：为简化后续算法的描述及实现，可如下代码所示预先设置一系列的宏定义。 123456789101112#define Parent(i) ( ( ( i ) - 1 ) &gt;&gt; 1 ) //PQ[i]的父节点（floor((i-1)/2)，i无论正负）#define LChild(i) ( 1 + ( ( i ) &lt;&lt; 1 ) ) //PQ[i]的左孩子#define RChild(i) ( ( 1 + ( i ) ) &lt;&lt; 1 ) //PQ[i]的右孩子#define InHeap(n, i) ( ( ( -1 ) &lt; ( i ) ) &amp;&amp; ( ( i ) &lt; ( n ) ) ) //判断PQ[i]是否合法#define LChildValid(n, i) InHeap( n, LChild( i ) ) //判断PQ[i]是否有一个（左）孩子#define RChildValid(n, i) InHeap( n, RChild( i ) ) //判断PQ[i]是否有两个孩子#define Bigger(PQ, i, j) ( lt( PQ[i], PQ[j] ) ? j : i ) //取大者（等时前者优先）#define ProperParent(PQ, n, i) /*父子（至多）三者中的大者*/ \\ ( RChildValid(n, i) ? Bigger( PQ, Bigger( PQ, i, LChild(i) ), RChild(i) ) : \\ ( LChildValid(n, i) ? Bigger( PQ, i, LChild(i) ) : i \\ ) \\ ) //相等时父节点优先，如此可避免不必要的 PQ_ComplHeap模板类：借助多重继承的机制，定义完全二叉堆模板类PQ_ComplHeap 12345678910111213#include \"Vector/Vector.h\" //借助多重继承机制，基于向量#include \"PQ/PQ.h\" //按照优先级队列ADT实现的template &lt;typename T&gt; struct PQ_ComplHeap : public PQ&lt;T&gt;, public Vector&lt;T&gt; &#123; //完全二叉堆 /*DSA*/friend class UniPrint; //演示输出使用，否则不必设置友类 PQ_ComplHeap() &#123; &#125; //默认构造 PQ_ComplHeap ( T* A, Rank n ) &#123; copyFrom ( A, 0, n ); heapify ( _elem, n ); &#125; //批量构造 void insert ( T ); //按照比较器确定的优先级次序，插入词条 T getMax(); //读取优先级最高的词条 T delMax(); //删除优先级最高的词条&#125;; //PQ_ComplHeaptemplate &lt;typename T&gt; void heapify ( T* A, Rank n ); //Floyd建堆算法template &lt;typename T&gt; Rank percolateDown ( T* A, Rank n, Rank i ); //下滤template &lt;typename T&gt; Rank percolateUp ( T* A, Rank i ); //上滤 getMax()：既然全局优先级最高的词条总是位于堆顶，因此只需返回向量的首单元，即可在$O(1)$时间内完成getMax()操作。 1template &lt;typename T&gt; T PQ_ComplHeap&lt;T&gt;::getMax() &#123; return _elem[0]; &#125; //取优先级最高的词条 2.4.插入算法插入算法首先要调用向量的标准插入接口，将新词条接至向量的末尾，得益于向量结构良好的封装性，这里无需关心这一步骤的具体细节，尤其是无需考虑溢出扩容等特殊情况。 新引入的词条并未破坏堆的结构性，但只要新词条e不是堆顶，就有可能与其父亲违反堆序性，而其他位置的堆序性依然满足，故以下将调用percolateUp()函数，对新接入的词条做适当调整，在保持结构性的前提下恢复整体的堆序性。 上滤： 根据e在向量中对应的秩，可以简便地确定词条p对应的秩，即$i(parent(v))=\\lfloor(i(v)-1)/2\\rfloor$。此时若经比较判定$e\\le p$，则堆序性在此局部以至全堆均已满足，插入操作因此即告完成。反之，若$e&gt;p$，则可在向量中令e和p互换位置。如此不仅全堆的结构性依然满足，而且e和p之间的堆序性也得以恢复。此后e与其新父亲，可能再次违背堆序性，只需继续套用以上方法。 每交换一次，新词条e都向上攀升一层，故这一过程也形象地称作上滤（percolate up），e至多上滤至堆顶，一旦上滤完成，全堆的堆序性必将恢复。下面是一个实例： 上滤调整过程中交换操作的累计次数，不致超过全堆的高度$\\lfloor \\log_2n \\rfloor$，而在向量中，每次交换操作只需常数时间，故上滤调整乃至整个词条插入算法整体的时间复杂度，均为$O(\\log n)$。 插入算法和上滤调整的代码实现如下： 1234567891011121314template &lt;typename T&gt; void PQ_ComplHeap&lt;T&gt;::insert ( T e ) &#123; //将词条插入完全二叉堆中 Vector&lt;T&gt;::insert ( e ); //首先将新词条接至向量末尾 percolateUp ( _elem, _size - 1 ); //再对该词条实施上滤调整&#125;//对向量中的第i个词条实施上滤操作，i &lt; _sizetemplate &lt;typename T&gt; Rank percolateUp ( T* A, Rank i ) &#123; while ( 0 &lt; i ) &#123; //在抵达堆顶之前，反复地 Rank j = Parent ( i ); //考查[i]之父亲[j] if ( lt ( A[i], A[j] ) ) break; //一旦父子顺序，上滤旋即完成；否则 swap ( A[i], A[j] ); i = j; //父子换位，并继续考查上一层 &#125; //while return i; //返回上滤最终抵达的位置&#125; 2.5.元素删除待删除词条r总是位于堆顶，故可直接将其取出并备份，堆的结构性将破坏。为修复这一缺陷，将最末尾的词条e转移至堆顶。而新的堆顶可能与其孩子们违背堆序性——尽管其他位置的堆序性依然满足，故下调用percolateDown()函数调整新堆顶，在保持结构性的前提下，恢复整体的堆序性。 下滤： 若新堆顶e不满足堆序性，将e与其（至多）两个孩子中的大者交换位置。此后，堆中可能的缺陷依然只能来自与词条e——它与新孩子可能再次违背堆序性，只需继续套用以上方法。因每进过一次交换，词条e都会下降一层，故这一调整过程也称作下滤（percolate down）。与上滤同理，这一过程也必然终止，全堆的堆序性必将恢复，下滤乃至整个删除算法的时间复杂度也为$O(\\log n)$。下面是一个实例： 删除算法和下滤调整的代码实现如下： 12345678910111213template &lt;typename T&gt; T PQ_ComplHeap&lt;T&gt;::delMax() &#123; //删除非空完全二叉堆中优先级最高的词条 T maxElem = _elem[0]; _elem[0] = _elem[ --_size ]; //摘除堆顶（首词条），代之以末词条 percolateDown ( _elem, _size, 0 ); //对新堆顶实施下滤 return maxElem; //返回此前备份的最大词条&#125;//对向量前n个词条中的第i个实施下滤，i &lt; ntemplate &lt;typename T&gt; Rank percolateDown ( T* A, Rank n, Rank i ) &#123; Rank j; //i及其（至多两个）孩子中，堪为父者 while ( i != ( j = ProperParent ( A, n, i ) ) ) //只要i非j，则 &#123; swap ( A[i], A[j] ); i = j; &#125; //二者换位，并继续考查下降后的i return i; //返回下滤抵达的位置（亦i亦j）&#125; 2.6.建堆很多算法中输入词条都是成批给出，故在初始化阶段往往需要解决一个共同问题：给定一组词条，高效地将它们组织成一个堆，这一过程也称作“建堆”（heapification）。 蛮力算法： 从空堆起反复调用标准insert()接口，即可将词条逐一插入其中，并最终完成建堆。尽管这一方法无疑正确，但其消耗的时间却过多。具体地，若共有n个词条，则共需迭代n次，第k轮迭代耗时$O(\\log k)$，故累计耗时时间量应为： O(\\log 1+\\log 2+\\dots +\\log n)=O(\\log n!)=O(n\\log n)如此多的时间本来足以将所有词条做全排序，而在这里花费同样多的时间所生成的堆却只能提供一个偏序，这也暗示了存在某种更快的建堆方法。 自上而下的上滤： 在将所有输入词条纳入长为n的向量之后，首单元处的词条本身即可视作一个规模为1的堆。接下来考查下一单元，只需调用percolateUp()对其上滤，此后前两单元将构成规模为2的堆，如此反复进行，直到最终得到规模为n的堆。 这一过程可归纳为：对任何一棵完全二叉树，只需自顶而下、自左向右地对其中每个节点实施一次上滤，即可使之成为完全二叉堆。在此过程中，为将每个节点纳入堆中，所需消耗的时间量将线性正比与该节点的深度。不妨考查高度为$h$、规模为$n=2^{h+1}-1$的满二叉树，其中高度为$i$的节点共有$2^i$个，因此整个算法的总体时间复杂度为： \\sum^h_{i=0}(i\\cdot 2^i )=(d-1)\\times 2^{d+1}+2=(\\log_2(n+1)-2)\\cdot (n+1)+2=O(n\\log n)Floyd算法（自下而上的下滤）： 考虑一个相对简单的问题：任给堆$H_0$和$H_1$，以及另一独立节点p，然后以p为中介将堆$H_0$和$H_1$合并，故称作堆合并操作。首先为满足结构性，可将这两个堆当作p的左、右子树，联接成一棵完整的二叉树。此时等效于在delMax()操作中摘除堆顶，再将末位词条（p）转移至堆顶，以下只需对p实施下滤操作，即可将全树转换为堆。 如果将以上过程作为实现堆合并的一个通用算法，则在将所有词条组织为一棵完全二叉树后，只需自底而上地反复套用这一算法，即可不断地将处于下层的堆逐对地合并成更高一层的堆，并最终得到一个完整的堆。按照这构思，即可实现Floyd建堆算法。下面是一个实例： 在前几节知识的基础上，Floyd算法的实现十分简洁：只需自下而上、由深而浅地遍历所有内部节点，并对每个内部节点分别调用一次下滤算法percolateDown()。 1234template &lt;typename T&gt; void heapify ( T* A, const Rank n ) &#123; //Floyd建堆算法，O(n)时间 for ( int i = n/2 - 1; 0 &lt;= i; i-- ) //自底而上，依次 percolateDown ( A, n, i ); //下滤各内部节点&#125; 复杂度：Floyd算法依然需要做n步迭代，以对所有节点各做一次下滤。这里每个节点的下滤所需的时间正比与其高度，故总体运行时间取决于各节点的高度总和。以高度为$h$，规模为$n=2^{h+1}-1$的满二叉树为例，其运行时间为： \\sum^h_{i=0}((d-i)\\cdot 2^i)=2^{d+1}-(d+2)=n-\\log_2(n+1)=O(n)由于在遍历所有词条之前，绝不可能确定堆的结果，故以上已是建堆操作的最优算法。由此反观，蛮力算法低效率的根源，恰在于其“自上而下的下滤”策略。如此，各节点所消耗的时间线性正比于其深度——而在完全二叉树中，深度小的节点，远远少于高度小的节点。 2.7.就地堆排序完全二叉堆有另一具体应用：对于向量中的n个词条，如何借助堆的相关算法，实现高效的排序，相应地这类算法也称作堆排序（heapsort）算法。既然此前归并排序算法的渐进复杂度已达到理论上最优的$O(n\\log n)$，故这里将更关注与如何降低复杂度常系数。同时也希望空间复杂度能够有所降低，最好是除输入本身以外只需$O(1)$辅助空间。 原理： 算法的总体思路和策略与选择排序算法基本相同：将所有词条分成未排序和已排序两类，不断从前一类中取出最大者，顺序加至后一类中。这里不妨将其划分为前缀H和与之互补的后缀S，分别对应于上述未排序和已排序部分。新算法的不同之处在于：整个排序过程中，无论H包含多少词条，始终都组织为一个堆。另外，整个算法过程始终满足不变性：H中的最大词条不会大于S中的最小词条——除非二者之一为空，比如算法的初始和终止时刻。 首先如图(a)，取出首单元词条M，将其与末单元词条X交换。M即是当前堆中的最大者，同时根据不变性也不大于S中的任何词条，故如此交换之后M必处于正确的排序位置。故如图(b)，此时可等效地认为S向前扩大了一个单元，H相应地缩小了一个单元，注意如此重新分界之后，H和S依然满足以上不变性。最后仿照此前的词条删除算法，只需对X实施一次下滤调整，即可使H整体的堆序性重新恢复，结果如图(c)。 复杂度： 在每一步迭代中，交换M和X只需常数时间，对X的下滤调整不超过$O(\\log n)$时间，因此全部n步迭代累计耗时不超过$O(n\\log n)$，再加上建堆所用时间，整个算法的运行时间也不超过$O(n\\log n)$。纵览算法的整个过程，除了用于支持词条交换的一个辅助单元，几乎不需要更多的辅助空间，故的确属于就地算法。 得益于向量结构的简洁性，几乎所有以上操作都可便捷地实现，因此该算法不仅可简明地编码，其实际运行效率也因此往往要高于其他$O(n \\log n)$的算法，这离不开堆这一精巧的数据结构。 实例： 以向量$\\{4,2,5,1,3\\}$的堆排序过程为例，首先采用Floyd算法将该向量整理为一个完全二叉堆，其中虚线示意下滤过程中的词条交换操作。 然后按照堆排序算法只需5步迭代，完成排序。 实现： 12345template &lt;typename T&gt; void Vector&lt;T&gt;::heapSort ( Rank lo, Rank hi ) &#123; //0 &lt;= lo &lt; hi &lt;= size PQ_ComplHeap&lt;T&gt; H( _elem + lo, hi - lo ); //将待排序区间建成一个完全二叉堆，O(n) while ( !H.empty() ) //反复地摘除最大元并归入已排序的后缀，直至堆空 _elem[--hi] = H.delMax(); //等效于堆顶与末元素对换后下滤&#125; 3.左式堆3.1.堆合并 除了标准的插入和删除操作，堆结构在实际应用中的另一常见操作即为合并，即任给堆A和堆B，如何将二者所含的词条组织为一个堆。 可以想到两种简单的方法：一是反复取出堆B的最大词条并插入堆A中，将两堆的规模分别记为n和m，且$n\\ge m$，每一步迭代均需要做一次删除操作和一次插入操作，分别耗时$O(\\log m)$和$O(\\log(n+m))$，共需m步迭代，故总体运行时间应为： m\\times [O(\\log m)+O(\\log(n+m))]=O(m\\log(n+m))=O(m\\log n)另一种是将两个堆中的词条视作彼此独立的对象，从而可以借助Floyd算法，将它们组织为一个新的堆H，运行时间为： O(n+m)=O(n)尽管其性能梢优于前一个，但仍无法令人满意。实际上，既然所有词条已分两组各自成堆，则意味着它们已经具有一定的偏序性，这样构建一个更大的偏序集，理应比由一组相互独立的词条构建偏序集更为容易。以上尝试均未奏效的原因在于，不能保证合并操作所涉及的节点足够少。为此不妨设想，堆是否也必须与二叉搜索树一样，尽可能地保持平衡？而对于堆来说，为控制合并操作所涉及的节点数，反而需要保持某种意义上的“不平衡”。 3.2.单侧倾斜左式堆（leftist heap）是优先级队列的另一实现方式，可高效地支持堆合并操作。其基本思路是：在保持堆序性的前提下附加新的条件，使得在堆的合并过程中，只需调整很少量的节点。具体地，需参与调整的节点不超过$O(\\log n)$个，故可达到极高的效率。 左式堆的整体结构呈单侧倾斜状，其中节点的分布均偏向左侧，左式堆将不再如完全二叉堆那样满足结构性。这也不难理解，毕竟堆序性才是堆结构的关键条件，而结构性只不过是堆的一项附加条件。在将平衡性替换为左倾性之后，左式堆结构的merge()操作乃至insert()和delMax()操作均可高效地实现。 按照以上思路，可借助多重继承的机制，定义左式堆模板类PQ_LeftHeap： 1234567891011121314#include \"PQ/PQ.h\" //引入优先级队列ADT#include \"BinTree/BinTree.h\" //引入二叉树节点模板类template &lt;typename T&gt;class PQ_LeftHeap : public PQ&lt;T&gt;, public BinTree&lt;T&gt; &#123; //基于二叉树，以左式堆形式实现的PQ /*DSA*/friend class UniPrint; //演示输出使用，否则不必设置友类public: PQ_LeftHeap() &#123; &#125; //默认构造 PQ_LeftHeap ( T* E, int n ) //批量构造：可改进为Floyd建堆算法 &#123; for ( int i = 0; i &lt; n; i++ ) insert ( E[i] ); &#125; void insert ( T ); //按照比较器确定的优先级次序插入元素 T getMax(); //取出优先级最高的元素 T delMax(); //删除优先级最高的元素&#125;; //PQ_LeftHeap PQ_LeftHeap模板类借助多重继承机制，由PQ和BinTree结构共同派生而得。既然左式堆的逻辑结构不再等价于完全二叉树，若沿用此前基于向量的实现方法，必将难以控制空间复杂度。因此改用紧凑型稍差，灵活性更强的二叉树结构，将更具针对性。 3.3.空节点路径长度为控制左式堆的倾斜度，可借鉴AVL树和红黑树的技巧，先引入外部节点，将结构转化为真二叉树，然后为各节点引入“ 空节点路径长度 ”指标，并依此确定相关算法的执行方向。节点x的空节点路径长度（null path length），即做npl(x)。若x为外部节点，则约定npl(x) = npl(null) = 0。反之若x为内部节点，则npl(x)可递归地定义为：npl(x) = 1 + min( npl( lc(x) ), npl( rc(x) ) )，即节点x的npl值取决于其左、右孩子npl值中的小者。 不难验证：npl(x)既等于x到外部节点的最近距离，同时也等于以x为根的最大满子树的高度。 左式堆是处处满足“ 左倾性 ”的二叉堆，即任一内部节点x都满足：npl( lc(x) ) $\\ge$ npl( rc(x) )，即就npl指标而言，任一内部节点的左孩子都不小于其右孩子。 由npl及左倾性的定义可以发现，左式堆中任一内节点x都应满足：npl(x) = 1 + npl( rc(x) )，即左式堆中每个节点的npl值，仅取决于其右孩子。要注意的是，“ 左孩子的npl值不小于右孩子 ”并不意味着 “ 左孩子的高度比不小于右孩子 ”。 3.4.最右侧通路从x出发沿右侧分支一直前行直至空节点，经过的通路称作最右侧通路（rightmost path），记作rPath(x)。在左式堆中，尽管右孩子高度可能大于左孩子，但由“ 各节点npl值均决定于其右孩子 ”这一事实不难发现，每个节点的npl值，应恰好等于其最右侧通路的长度。 根节点r的最右侧通路，在此扮演的角色极其重要，rPath(r)的终点必为全堆中深度最小的外部节点，若记：npl(r) = |rPath(r)| = d，则该堆应包含一棵以r为根，高度为d的满二叉树（黑色部分），且该满二叉树至少应包含$2^{d+1}-1$个节点、$2^d-1$个内部节点——这也是堆的规模下限；反之在包含n个节点的左式堆中，最右侧通路必然不会长于：$\\lfloor \\log_2(n+1) \\rfloor-1=O(\\log n)$。 3.5.合并算法假设待合并的左式堆分别以a和b为堆顶，且不失一般性地令$a\\ge b$。于是可递归地将a的右子堆$a_R$与堆b合并，然后作为节点a的右孩子替换原先的$a_R$。为保证依然满足左倾性条件，最后还需要比较a左、右孩子的npl值——如有必要还需将二者交换，以保证左孩子的npl值不低于右孩子。 下面是一个实例： 按照以上思路，左式堆合并算法可实现为如下的代码： 1234567891011template &lt;typename T&gt; //根据相对优先级确定适宜的方式，合并以a和b为根节点的两个左式堆static BinNodePosi(T) merge ( BinNodePosi(T) a, BinNodePosi(T) b ) &#123; if ( ! a ) return b; //退化情况 if ( ! b ) return a; //退化情况 if ( lt ( a-&gt;data, b-&gt;data ) ) swap ( a, b ); //一般情况：首先确保b不大 ( a-&gt;rc = merge ( a-&gt;rc, b ) )-&gt;parent = a; //将a的右子堆，与b合并 if ( !a-&gt;lc || a-&gt;lc-&gt;npl &lt; a-&gt;rc-&gt;npl ) //若有必要 swap ( a-&gt;lc, a-&gt;rc ); //交换a的左、右子堆，以确保右子堆的npl不大 a-&gt;npl = a-&gt;rc ? a-&gt;rc-&gt;npl + 1 : 1; //更新a的npl return a; //返回合并后的堆顶&#125; //本算法只实现结构上的合并，堆的规模须由上层调用者负责更新 复杂度： 以上的合并算法中，所有递归实例可排成一个线性序列，因此该算法实质上属于线性递归，其运行时间应线性正比于递归深度。进一步地，递归只可能发生于两个待合并堆的最右侧通路上，若待合并堆的规模分别为n和m，则其两条最右侧通路的长度分别不会超过$O(\\log n)$和$O(\\log m)$，因此合并算法总体运行时间应不超过： O(\\log n)+O(\\log m)=O(\\log n+\\log m)=O(\\log(\\max(n,m)))若将以上递归版本改写为迭代版本，还可以从常系数的意义上进一步提高效率。 3.6.基于合并的插入和删除若将merge()操作当作一项更为基本的操作，则可以反过来实现优先级队列标准的插入和删除等操作。事实上，得益于merge()操作自身的高效率，如此实现的插入和删除操作，在时间效率方面不逊色与常规的实现方式，而其突出的简洁性也使得这一方式在实际中应用广泛。 3.6.1.delMax() 基于merge()操作实现delMax()算法，如图设堆顶x及其子堆$H_L$和$H_R$。在摘除x之后，$H_L$和$H_R$即可被视作为两个彼此独立的待合并的堆，只要通过merge()操作将它们合并其来，则效果完全等同于一次常规的delMax()删除操作。算法的时间复杂度依然不超过$O(\\log n)$。 12345678template &lt;typename T&gt; T PQ_LeftHeap&lt;T&gt;::delMax() &#123; BinNodePosi(T) lHeap = _root-&gt;lc; if (lHeap) lHeap-&gt;parent = NULL; //左子堆 BinNodePosi(T) rHeap = _root-&gt;rc; if (rHeap) rHeap-&gt;parent = NULL; //右子堆 T e = _root-&gt;data; delete _root; _size--; //删除根节点 _root = merge ( lHeap, rHeap ); //合并原左、右子堆 if( _root ) _root-&gt;parent = NULL; //若堆非空，还需相应设置父子连接 return e; //返回原根节点的数据项&#125; 3.6.1.insert() 基于merge()操作实现insert()算法，假设拟将词条x插入堆H中。实际上，只要将x也视作（仅含单个节点的）堆，则通过调用merge()操作将该堆与堆H合并之后，其效果即完全等同完成了一次词条插入操作，时间复杂度依然不超过$O(\\log n)$。 1234567template &lt;typename T&gt; void PQ_LeftHeap&lt;T&gt;::insert ( T e ) &#123; BinNodePosi(T) v = new BinNode&lt;T&gt; (e); //为e创建一个二叉树节点 _root = merge( _root, v); //通过合并完成新节点的插入 _root-&gt;parent = NULL; //既然此时堆非空，还需相应设置父子连接 //_root = merge( _root, new BinNode&lt;T&gt;( e, NULL ) ); //将e封装为左式堆，与当前左式堆合并 _size++; //更新规模&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"数据结构与算法（18）词典","slug":"数据结构与算法（18）词典","date":"2020-03-24T07:06:49.000Z","updated":"2020-10-08T08:24:00.652Z","comments":true,"path":"2020/03/24/数据结构与算法（18）词典/","link":"","permalink":"http://nekomoon404.github.io/2020/03/24/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8818%EF%BC%89%E8%AF%8D%E5%85%B8/","excerpt":"","text":"[TOC] 1.概述1.1.词典借助数据结构来表示和组织的数字信息，可将所有数据视作一个整体统筹处理，进而提高信息访问的规范性和其处理的效率。例如，借助关键码直接查找和访问数据元素的形式，已为越来越多的数据结构所采用，这也成为现代数据结构的一个重要特征。词典（dictionary）即是其中最典型的例子。 逻辑上的词典是由一组数据构成的集合，其中各元素都是由关键码和数据项合成的词条（entry）。映射（map）结构与词典结构一样，也是词条的集合。二者的差别仅仅在于，映射要求不同词条的关键码互异，而词典则允许多个词条拥有相同的关键码。除了静态查找，映射和词典都支持动态更新，二者统称作符号表（symbol table）。本章将不再过分强调二者的差异，而是笼统地称作词典。 尽管此处词典和映射中的数据元素，仍表示和实现为词条形式，但这一做法并非必须。与搜索树相比，符号表并不要求词条之间能够根据关键码比较大小，甚至也不需要按照大小次序来组织数据项——即便各数据项之间的确定义有某种次序。 实际上，以散列表为代表的符号表结构，将转而依据数据项的数值，直接做逻辑查找和物理定位。对于此类结构，在作为基本数据单位的词条内部，关键码（key）与数值（value）的地位等同，二者不必加以区分。此类结构所支持的这种新的数据访问方式，即所谓的循值访问（call-by-value）。 为支持循值访问的方式，在符号表的内部，仍然必须强制地在数据对象的数值与其物理地址之间建立某种关联。而散列，正是在兼顾空间与时间效率的前提下，讨论和研究赖以设计并实现这种关联的一般性原则、技巧与方法。 1.2.词典ADT1.2.1.操作接口除通用的接口之外，词典结构主要的操作接口可归纳为下表：get(key) 操作接口 功能描述 get(key) 若词典中存在以key为关键码的词条，则返回该词条的数据对象；否则，返回NULL put(key,value) 插入词条(key, value)，并报告是否成功 remove(key) 若词典中存在以key为关键码的词条，则删除之并返回true；否则，返回false 1.2.2.接口定义首先以如下代码所示模板类的形式定义词典的操作接口。 123456template &lt;typename K, typename V&gt; struct Dictionary &#123; //词典Dictionary模板类 virtual int size() const = 0; //当前词条总数 virtual bool put ( K, V ) = 0; //插入词条（禁止雷同词条时可能失败） virtual V* get ( K k ) = 0; //读取词条 virtual bool remove ( K k ) = 0; //删除词条&#125;; 其中，所有的操作接口均以虚函数形式给出，留待在派生类中予以具体实现。另外，尽管词条关键码类型可能支持大小比较，但这并非词典结构的必要条件，Dictionary模板类中的Entry类只需支持判断等操作。 2.散列散列以最基本的向量作为底层支撑结构，通过适当的散列函数在词条的关键码与向量单元的秩之间建立起映射关系。只要散列表、散列函数以及冲突排解策略设计得当，散列技术可在期望的常数时间内实现词典的所有接口操作。即就平均时间复杂度的意义而言，可以使这些操作所需的运行时间与词典的规模基本无关。尤为重要的是，散列技术完全摒弃了“关键码有序”的先决条件，故就实现词典结构而言，散列所特有的通用型和灵活性是其他方式无法比拟的。 2.1.散列表散列表（hashtable）是散列方法的底层基础，逻辑上由一系列可存放词条（或其引用）的单元组成，故这些单元也称作桶（bucket）或桶单元，各桶单元也应按其逻辑次序在物理上连续排列。因此这种线性的底层结构用向量来实现再自然不过；为简化实现并进一步提高效率，往往直接使用数组，此时的散列表亦称作桶数组（bucket array）。若桶数组的容量为R，则其中合法秩的区间[0, R)也称作地址空间（address space）。 一组词条在散列表内部的具体分布，取决于所谓的散列（hashing）方案——事先在词条与桶地址之间约定的某种映射关系，可描述为从关键码空间到桶数组地址空间的函数： hash():key \\to hash(key)这里的hash( )称作散列函数（hash function），反过来，hash(key)也称作key的散列地址（hashing address），亦即与关键码key相对应的桶在散列表中的秩。 完美散列（perfect hashing）：在时间和空间性能方面均达到最优的散列，可在$O(1)$时间内确定散列地址，并完成一次查找、插入或删除；空间性能方面，每个桶恰好存放一个词条，即无空余亦无重复。而完美散列实际上并不常见，在更多的应用环境中，为兼顾空间和时间的效率，无论散列表或散列函数都需要经过更为精心的设计。 在实际应用中往往会遇到一类问题，其共同的特点可归纳为：尽管词典中实际需要保存的词条数N（比如25000）远远小于可能出现的词条数R（如10^8），但R个词条中的任何一个都有可能出现在词典中。仿照对向量空间利用率的度量方法，可以将散列表中非空桶的数目与桶单元总数的比值称作装填因子（load factor）：$\\lambda=N/M=$ 存放的词条 / |桶数组| 2.2.散列函数不妨先假定关键码均为[0, R)范围内的整数，将词典中的词条数记作N，散列表长度记作M，于是通常有：$R&gt;&gt;M&gt;N$，如下图所示，散列函数hash( )的作用可理解为：将关键码空间[0, R)压缩为散列地址空间[0, M)。 好的散列函数hash( )应具备以下的条件： 确定性，无论所含的数据项如何，词条E在散列表中的映射地址hash(E.key)必须完全取决于其关键码E.key； 映射过程自身不能过于复杂，唯此方能保证散列地址的计算可快速完成，从而保证查询或修改操作整体的$O(1)$期望执行时间； 所有关键码经映射后应尽量覆盖整个地址空间[0, M)，唯此方可充分利用有限的散列表空间，即函数hash()最好是满射。 因定义域规模R远大于取值域规模M，hash()不可能是单射。这就意味着关键码不同的词条被映射到同一散列地址的情况——散列冲突（collision），难以彻底避免。 最为重要的一条原则就是，关键码映射到各桶的概率应尽量接近于1/M——若关键码均匀且独立地随机分布，这也是任意一对关键码相互冲突的概率。就整体而言，这等效于将关键码空间“均匀地”映射到散列地址空间，从而避免导致极端低效的情况。 总之，随机越强、规律性越弱的散列函数越好，当然完全符合上述条件的散列函数并不存在，我们只能通过先验地消除可能导致关键码分布不均匀的因素，最大限度地模拟理想的随机函数，尽最大可能降低发生冲突的概率。 2.2.1.除余法符合上述要求的一种最简单的映射方法，就是将散列表长度M取作素数，并将关键码key映射到key关于M整除的余数： hash(key)=key \\,\\,\\,mod\\,\\,\\,M以校园电话簿为例，若取M = 90001，如下图所示。 需要注意的是，采用除余法时，M必须选为素数，否则关键码被映射至[0, M)范围内的均匀度将大幅降低，发生冲突的概率将随M所含素因子的增多而迅速加大。 在实际应用中，对统一词典内词条的访问往往具有某种周期性，若其周期与M具有公共的素因子，则冲突的概率将急剧增加。考虑一个例子：某散列表从全空的初始状态开始，插入的前10个词条对应的关键码是等价数列$\\{1000,1015,1030,\\dots,1135\\}$。 如图(a)，若散列表长度取作M = 20，则其中每一关键码，都与另外一个或两个关键码相冲突；而反过来，散列表中80%的桶，此时却处于空闲状态。词条集中到散列表内少数若干桶中（或附近）的现象，称作词条的聚集（clustering）。显然好的散列函数应尽可能避免此类现象，而采用素数表长则是降低聚集发生概率的捷径。 一般地，散列表的长度M与词条关键码间隔T之间的最大公约数越大，发生冲突的可能性也讲越大。因此，若M取素数，则简便对于严格或大致等间隔的关键码序列，也不致出现冲突激增的情况，同时提高空间效率。如图(b)改用表长M = 19，则没有任何冲突，且空间利用率提高至50%以上；再如图(c)，取表长M = 11，则同样不致发生任何冲突，且仅有一个桶空闲。 若T本身足够大而且恰好可被M整除，则所有被访问词条都将相互冲突。如图(d)将表长取为素数M = 5，且只考虑插入序列中的前5个关键码，则所有关键码都将聚集于一个桶内，但实际中发生这种情况的概率极低。 除余法的缺陷： 以素数为表长的除余法尽管可在一定程度上保证词条的均匀分布，但从关键码空间到散列地址空间映射的角度看，依然残留有某种连续性。比如，相邻关键码所对应的散列地址，总是彼此相邻，称为零阶均匀；极小的关键码，通常都被集中映射到散列表的起始区段——其中，特别地，0值居然是一个“不动点”，其散列表地址总是0，而与散列表长度无关。 2.2.2.MAD法为了弥补除余法的不足，可采用MAD法（multiply-add-divide method），它将关键码key映射为： (a \\times key +b)\\,\\,\\,mod\\,\\,\\,M其中，M仍为素数，$a&gt;0,b&gt;0$，且$a\\,\\,\\,mod\\,\\,\\,M \\ne 0$ 尽管运算量略有增加，但只要常数a和b选取得当，MAD法可以很好地克服除余法原有的连续性缺陷。 如上图，若采用除余法，将关键码$\\{2011,2012,2013,2014,2015,2016\\}$插入长度为M = 17的空散列表后，这组词条将存放至地址连续的6个桶中，尽管没有任何关键码的冲突，却可以改善为“更高阶”的均匀性。若采用MAD法，当选取a = 31和b = 2时，各关键码的均匀性相对于图(a)有了很大改善。 2.2.3.更多的散列函数除了上面介绍的两种散列函数，还有很多种形式的散列函数。 2.3.Hashtable模板类按照词典的标准接口，可以模板类的形式，定义Hashtable类，实现如下。 12345678910111213141516171819202122232425#include \"Dictionary/Dictionary.h\" //引入词典ADT#include \"Bitmap/Bitmap.h\" //引入位图template &lt;typename K, typename V&gt; //key、valueclass Hashtable : public Dictionary&lt;K, V&gt; &#123; //符合Dictionary接口的Hashtable模板类 /*DSA*/friend class UniPrint;private: Entry&lt;K, V&gt;** ht; //桶数组，存放词条指针 int M; //桶数组容量 int N; //词条数量 Bitmap* lazyRemoval; //懒惰删除标记#define lazilyRemoved(x) (lazyRemoval-&gt;test(x))#define markAsRemoved(x) (lazyRemoval-&gt;set(x))protected: int probe4Hit ( const K&amp; k ); //沿关键码k对应的查找链，找到词条匹配的桶 int probe4Free ( const K&amp; k ); //沿关键码k对应的查找链，找到首个可用空桶 void rehash(); //重散列算法：扩充桶数组，保证装填因子在警戒线以下public: Hashtable ( int c = 5 ); //创建一个容量不小于c的散列表（为测试暂时选用较小的默认值） ~Hashtable(); //释放桶数组及其中各（非空）元素所指向的词条 int size() const &#123; return N; &#125; // 当前的词条数目 bool put ( K, V ); //插入（禁止雷同词条，故可能失败） V* get ( K k ); //读取 bool remove ( K k ); //删除&#125;; 散列表构造123456789101112131415template &lt;typename K, typename V&gt; Hashtable&lt;K, V&gt;::Hashtable ( int c ) &#123; //创建散列表，容量为 M = primeNLT ( c, 1048576, \"../../_input/prime-1048576-bitmap.txt\" ); //不小于c的素数M N = 0; ht = new Entry&lt;K, V&gt;*[M]; //开辟桶数组（还需核对申请成功），初始装填因子为N/M = 0% memset ( ht, 0, sizeof ( Entry&lt;K, V&gt;* ) *M ); //初始化各桶 lazyRemoval = new Bitmap ( M ); //懒惰删除标记比特图 //*DSA*/printf(\"A bucket array has been created with capacity = %d\\n\\n\", M);&#125;int primeNLT(int c, int n, char* file)&#123; //根据file文件中的记录，在[c, n)内取最小的素数 Bitmap B( file n); //file 已经按位图格式，记录了n以内的所有素数，因此只要 while( c &lt; n) //从c开始，逐位地 if( B.test( c ) ) c++; //测试，即可 else return c; //返回首个发现的素数 return c; //若没有这样的素数，返回n(实用中不能如此简化处理)&#125; 为了加速素数的选取，事先计算出不超过1049576的所有素数，并存放于文件中备查。于是在创建散列表（或重散列）时，对于在此范围内任意给定的长度下限c，都可通过调用primeNLT()，迅速地从该查询表中找到不小于c的最小素数M作为散列表长度，并依次为新的散列表申请相应数量的空调；同时创建一个同样长度的位图结构，作为懒惰删除标志表。 primeNLT()从长度下限c开始，逐个测试对应的标志位，直到第一个足够大的素数。 散列表析构123456template &lt;typename K, typename V&gt; Hashtable&lt;K, V&gt;::~Hashtable() &#123; //析构前释放桶数组及非空词条 for ( int i = 0; i &lt; M; i++ ) //逐一检查各桶 if ( ht[i] ) release ( ht[i] ); //释放非空的桶 release ( ht ); //释放桶数组 release ( lazyRemoval ); //释放懒惰删除标记&#125; //release()负责释放复杂结构，与算法无直接关系，具体实现详见代码包 在销毁散列表之前，需在逐一释放各桶中的词条（如果存在）之后，释放整个散列表ht[]以及对应的懒惰删除表lazyRemoval[]。 3.排解冲突散列表的基本构思，可以概括为： 开辟物理地址连续的桶数组ht[]，借助散列函数hash()，将词条关键码key映射； 为桶地址hash(key)，从而快速地确定待操作词条的物理位置 然而遗憾的是，无论散列函数设计得如何巧妙，也不可能保证不同的关键码之间互不冲突。比如课堂中的学生，发生生日（birthday）相同的概率，将全年各天视作365个桶，将学生视作词条，只要学生人数$n\\ge23$，则至少发生一次冲突的概率$P_{365}(n)\\ge 50\\%$，而此时的装填因子仅为$\\lambda=23/365=6.3\\%$。对于更长的散列表，只需更低的装填因子，即友50%概率会发生一次冲突。因此冲突具有普遍性。 3.1.开放散列3.1.1.多槽位法（multiple slots）最直接了当的一种对策是，将彼此冲突的每一组词条组织为一个小规模的子词典，分别存放于它们共同对应的桶单元中。比如一种简便的方法是，统一将各桶细分为更小的称作槽位（slot）的若干单元，每一组槽位可组织为向量或列表。 如对于下图的冲突散列，可将各桶细分为四个槽位，只要相互冲突的各组关键码不超过4个，即可分别保存与对应桶单元内的不同槽位。 按照这一思路，针对关键码key的任一操作都将转化为对一组槽位的操作。比如put(key,value)操作，将首先通过hash(key)定位对应的桶单元，并在其内部的一组槽位内，进一步查找key。若失败，则创建新词条(key, value)，并将其插至该桶单元内的空闲槽位中。get(key)和remove(key)操作的过程与此类似。 多槽位的缺陷也显而易见： 绝大多数的槽位通常都处于空闲状态，若每个桶被细分为k个槽位，则当散列表总共存有N个词条时，装填因子：$\\lambda’=N/(kM)=\\lambda/k$ 将降低至原先的$1/k$。 很难在事先确定槽位应细分到何种程度，方可保证在任何情况下都够用。比如在极端情况下，有可能所有（或接近所有）的词条都冲突与单个桶单元，此时尽管几乎其余所有的桶都处于空闲状态，该桶却会因冲突过多而溢出。 3.1.2.独立链法（separate chaining）独立链法与多槽位法类似，也令相互冲突的每组词条构成小规模的子词典，只不过采用列表来实现各子词典，令各桶内相互冲突的词条串接成一个列表。 既然好的散列函数已能保证通常不致发生极端的冲突，故个子词典的规模往往都不是很大，大多数往往只含单个词条或者甚至是空的，因此采用此前学过的基本列表结构足矣。 优点： 相对于多槽位法，独立链法可更为灵活地动态调整各子词典的容量和规模，从而有效地降低空间消耗。 缺陷： 指针需要额外空间，节点需要动态申请。 在查找过程中一旦发生冲突，则需要遍历整个序列，导致查找成本的增加。 3.1.3.公共溢出区法（overflow area）公共溢出区法是在原散列表之外另设一个词典结构$D_{overflow}$，一旦在插入词条时发生冲突就将该词条转存至$D_{overflow}$。就效果而言，$D_{overflow}$相当于一个存放冲突词条的公共缓冲池，就整体而言，此时的散列表也可理解为是一种递归形式的散列表。 尽管就逻辑结构而言，独立链等策略便捷而紧凑，但也有缺点。比如因需要引如次级关联结构，实现相关算法的代码自身的复杂程度和出错概率都将大大增加。反过来，因不能保证物理上的关联性，对于稍大规模的词条集，查找过程中将需做更多的I/O操作。 3.2.封闭散列实际上，仅仅靠基本的散列结构，且就地排解冲突，反而是更好的选择。即若新词条与已有词条冲突，则只允许在散列表内部为其寻找另一空桶。如此，各桶并非注定只能存放特定的一组词条，从理论上讲每个桶单元都有可能存放任一词条。因为散列地址空间对所有词条开放，故这一新的策略亦称作开放定址（open addressing）；同时因可用的散列地址仅限于散列表所覆盖的范围之内，故亦称闭散列（closed hashing）。因不得使用附件空间，故装填因子需要适当降低，通常都取$\\lambda \\le 0.5$。 3.2.1.线性试探法（linear probing）开放定址策略最基本的一种形式是：在插入关键码key时，若发现桶单元ht[hash(key)]已被占用，则转而试探桶单元ht[hash(key)+1]；若ht[hash(key)+1]也被占用，则继续试探ht[hash(key)+2]，如此不断直到发现一个可用空桶。为确保桶地址的合法，最后还需统一对M取模，第i次试验的桶单元应为： ht[(hash(key)+i)\\,\\,\\,mod\\,\\,\\,M],\\,\\,\\,i=1,2,3,\\dots如此被试探的桶单元在物理空间上一次连贯，其地址构成等差数列 查找链采用开放地址策略时，散列表中每一组相互冲突的词条都将被视作一个有序序列，对其中任何一员的查找都需要借助这一序列，该序列被称作查找链（probing chain）。对应的查找过程，可能终止于三种情况： 在当前桶单元命中目标关键码，则成功返回； 当前桶单元非空，则其中关键码与目标关键码不等，则须转入下一桶单元继续试探； 当前桶单元为空，则查找以失败返回。 如下图，长度为M = 17的散列表，设采用除余法定址，采用线性试探排解冲突。如图(a)，从开表开始依次插入5个相互冲突的关键码$\\{2011,2028.2045.2062.2079\\}$。此后针对其中任一关键码的查找都将从ht[hash(key)] = ht[5]出发，试探各相邻的桶单元，与这组关键码对应的桶单元ht[5,10)构成一个有序序列，对其中任一关键码的查找都将沿该序列顺序进行。 对应长度为n的查找链，失败查找长度就是n+1，在等概率假设下，平均成功查找长度为$\\lceil n/2 \\rceil$。需要注意的是，尽管相互冲突的关键码必属于同一查找链，但反过来，同一查找链中的关键码却未必相互冲突。如将上图中的(a)和(b)合并，并按从大到小的次序逐一插入空散列表如图(c)，对于2079或2082关键码而言，查找链中的关键码未必与它们冲突。原因在于，多组各自冲突的关键码所对应的查找链，有可能相互交织和重叠，此时各组关键码的查找长度将会进一步增加。 由上可见，线性试探法中组成各查找链的词条，在物理上保持一定的连贯性，具有良好的数据局部性，故系统缓存的作用可以充分发挥，查找过程中几乎无需I/O操作。尽管闭散列策略同时也会在一定程度上增加冲突发生的可能，但只要散列表的规模不是很小，装填因子不是很大，则相对于于I/O负担的降低而言，这些问题都将微不足道。因此，相对于独立链等开散列策略，闭散列策略的实际应用更为广泛。 3.2.2.懒惰删除查找链中任何一环的缺失，都会导致后续词条因无法抵达而丢失，表现为有时无法找到实际已存在的词条。因此若采用开放定址策略，则在执行删除操作时，需同时做特别的调整。 如下图，若为删除词条ht[9]=2031，按常规方法将其清空，则该桶的缺失将导致对应的查找链“断裂”，从而致使五个后续词条“丢失”——尽管它们在词典中的确存在，但查找却会失败。 一种简明而有效的方法是：为每个桶另设一个标志位，指示该桶尽管目前为空，但此前确曾存放过词条。在Hashtable模板类中，名为lazyRemoval的Bitmap对象正是起到了这一作用。删除词条时，只需将对应的桶ht[r]标志位lazilyRemoved(r)，如此该桶虽不存放任何实质的词条，却依然是查找链上的一环。 带有删除标记的桶所扮演的角色，因具体的操作类型而异： 查找词条时，被视作“ 必不匹配的非空桶 “，查找链在此得以延续； 插入词条时，被视作“ 必然匹配的空闲桶 ”，可以用来存放新词条。 因此采用“ 懒惰删除 ”策略之后，get()，put()和remove() 等操作中的查找算法，都需要做相应的调整。 词条查找接口get()的实现： 12345678910111213141516171819202122232425262728template &lt;typename K, typename V&gt; V* Hashtable&lt;K, V&gt;::get ( K k ) //散列表词条查找算法&#123; int r = probe4Hit ( k ); return ht[r] ? &amp; ( ht[r]-&gt;value ) : NULL; &#125; //禁止词条的key值雷同/**************************************************************************** * 沿关键码k对应的查找链，找到与之匹配的桶（供查找和删除词条时调用） * 试探策略多种多样，可灵活选取；这里仅以线性试探策略为例 ***************************************************************************/template &lt;typename K, typename V&gt; int Hashtable&lt;K, V&gt;::probe4Hit ( const K&amp; k ) &#123; int r = hashCode ( k ) % M; //从起始桶（按除余法确定）出发 //*DSA*/printf(\" -&gt;%d\", r); while ( ( ht[r] &amp;&amp; ( k != ht[r]-&gt;key ) ) || ( !ht[r] &amp;&amp; lazilyRemoved ( r ) ) ) r = ( r + 1 ) % M; //沿查找链线性试探：跳过所有冲突的桶，以及带懒惰删除标记的桶 //*DSA*/printf(\" -&gt;%d\", r); //*DSA*/printf(\"\\n\"); return r; //调用者根据ht[r]是否为空，即可判断查找是否成功&#125;/**************************************************************************** * 沿关键码k对应的查找链，找到首个可用空桶（仅供插入词条时调用） * 试探策略多种多样，可灵活选取；这里仅以线性试探策略为例 ***************************************************************************/template &lt;typename K, typename V&gt; int Hashtable&lt;K, V&gt;::probe4Free ( const K&amp; k ) &#123; int r = hashCode ( k ) % M; //从起始桶（按除余法确定）出发 //*DSA*/printf(\" -&gt;%d\", r); //首个试探的桶单元地址 while ( ht[r] ) r = ( r + 1 ) % M; //沿查找链逐桶试探，直到首个空桶（无论是否带有懒惰删除标记）//*DSA*/ while (ht[r]) &#123; r = (r+1) % M; printf(\" -&gt;%d\", r); &#125; printf(\"\\n\"); return r; //为保证空桶总能找到，装填因子及散列表长需要合理设置&#125; 首先采用除余法确定首个试探的桶单元，然后按线性试探法沿查找链逐桶试探。这里共有两只试探终止的可能：在一个非空的桶内找到目标关键码（成功），或者遇到一个不带懒惰删除标记的空桶（失败）。否则，无论是当前桶中词条的关键码与目标码不等，还是当前桶为空但带有懒惰删除标记，都意味着有必要沿着查找链前进一步继续查找，该算法同一返回最后被试探桶的秩，上层调用者只需核对该桶是否为空，即可判断查找是否失败。 词条删除接口remove()的实现如下： 12345template &lt;typename K, typename V&gt; bool Hashtable&lt;K, V&gt;::remove ( K k ) &#123; //散列表词条删除算法 int r = probe4Hit ( k ); if ( !ht[r] ) return false; //对应词条不存在时，无法删除 release ( ht[r] ); ht[r] = NULL; markAsRemoved ( r ); N--; return true; //否则释放桶中词条，设置懒惰删除标记，并更新词条总数&#125; 这里首先调用probe4Hit(k)算法，沿关键码k对应的查找链顺序查找。若在某桶单元命中，则释放其中的词条，为该桶单元设置懒惰删除标记，并更新词典的规模。 词条插入接口put()的实现如下： 1234567template &lt;typename K, typename V&gt; bool Hashtable&lt;K, V&gt;::put ( K k, V v ) &#123; //散列表词条插入 if ( ht[probe4Hit ( k ) ] ) return false; //雷同元素不必重复插入 int r = probe4Free ( k ); //为新词条找个空桶（只要装填因子控制得当，必然成功） ht[r] = new Entry&lt;K, V&gt; ( k, v ); ++N; //插入（注意：懒惰删除标记无需复位） if ( N * 2 &gt; M ) rehash(); //装填因子高于50%后重散列 return true;&#125; 调用一下probe4Free(k)算法，若沿关键码k所属查找链能找到一个空桶，则在其中创建对应的词条，并更新词典的规模。 3.2.3.重散列（rehashing）就对散列表性能及效率的影响而言，装填因子$\\lambda=N/M$是最为重要的一个因素，随着$\\lambda$的上升，词条在散列表中聚集的程度亦将迅速加剧。若同时还采用基本的懒惰删除法，则不带懒惰删除标记的桶单元必将持续减少，这也势必加剧查找成本的进一步增多。理论分析和实验统计均表明，只要能将装填因子$\\lambda$控制在适当范围内，闭散列策略的平均效率，通常都可保持在较为理想的水平，比如一般的建议是保持$\\lambda&lt;0.5$。 重散列是常用的一种将装填因子控制在一定范围以内的方法，其实现如下： 123456789101112131415/**************************************************************************** * 重散列算法：装填因子过大时，采取“逐一取出再插入”的朴素策略，对桶数组扩容 * 因散列函数的定址与表长M直接相关，既然M已改变，就不可简单地批量复制原桶数组 ***************************************************************************/template &lt;typename K, typename V&gt; void Hashtable&lt;K, V&gt;::rehash() &#123; int old_capacity = M; Entry&lt;K, V&gt;** old_ht = ht; M = primeNLT ( 2 * M, 1048576, \"../../_input/prime-1048576-bitmap.txt\" ); //容量至少加倍 N = 0; ht = new Entry&lt;K, V&gt;*[M]; memset ( ht, 0, sizeof ( Entry&lt;K, V&gt;* ) * M ); //新桶数组 release ( lazyRemoval ); lazyRemoval = new Bitmap ( M ); //新开懒惰删除标记比特图 //*DSA*/printf(\"A bucket array has been created with capacity = %d\\n\\n\", M); for ( int i = 0; i &lt; old_capacity; i++ ) //扫描原桶数组 if ( old_ht[i] ) //将非空桶中的词条逐一 put ( old_ht[i]-&gt;key, old_ht[i]-&gt;value ); //插入至新的桶数组 release ( old_ht ); //释放原桶数组——由于其中原先存放的词条均已转移，故只需释放桶数组本身&#125; 重散列的效果，只不过是将原词条集，整体“搬迁”至容量至少加倍的新散列表中。与可扩充向量同理，这一策略也可使重散列所耗费的时间，在分摊至各次操作后可以忽略不计。 3.2.4.平方试探法（quadratic probing）线性试探法虽然简明紧凑，但各查找链均由物理地址连续的桶单元组成，因而会加剧关键码的聚集趋势。如下图，采用除余法将7个关键码$\\{2011,2012,2013,2014,2015,2016,2017\\}$依次插入长度M = 17的散列表，则会形成聚集区段ht[5, 12)。接下来若想插入关键码3456和4000，由hash(3456) = hash(4000) = hash(2011) = 5，二者的试探都将起始于桶单元ht[5]，分别经过8次和9次试探，它们将被插入聚集区段右侧的位置，形成更长的聚集区段。如果再考虑到聚集区段的生长还会加剧不同聚集区段之间的相互交叠，查找操作平均的下降程度将会更加严重。 采用2.2.2节的MAD法，可在一定程度上减缓上述聚集现象。而平方试探法则是一种更为有效的方法。具体地，在试探过程中若连续发生冲突，则按如下规则确定第i次试探的桶地址： (hash(key)+j^2)\\,\\,\\,mod\\,\\,\\,M,\\,\\,\\,j=0,1,2,\\dots各次试探的位置起始位置的距离，以平方速率增长。如上图(c)，为插入3456，将依次试探秩为5、6、9、14的桶单元，插入ht[14]；为插入4000，将依次试探秩为5、6、9、14、21 $\\equiv$ 4的桶单元，并最终将其插入ht[4]。 可见，聚集区段并未扩大，同时针对这两个关键码的后续查找，也分别只需3次和4次试探，速度得以提高至两倍以上。平试探之所以能够有效地缓解聚集现象，是因为充分利用了平方函数的特点——顺着查找链，试探位置的间距将以线性（而不再是常数1的）速度增长。于是一旦发生冲突，即可“ 尽快逃离 ”关键码聚集的区段。 要注意的是：装填因子须足够小。只要散列表长度M为素数且装填因子$\\lambda\\le50\\%$，则平方试探迟早必将终止于某个空桶。 双向平方试探法 正向和逆向的子查找链，各包含$\\lceil M/2 \\rceil$个互异的桶。表长取作素数 $M=4\\times k+3$，必然可以保证查找链的前M项均互异。 再散列法（double hashing）再散列也是延缓词条聚集趋势的一种有效方法，需选取一个适宜的二级散列函数： 若取$hash_2(key)=1$即是线性试探。 3.3.散列码转换为扩大散列技术的适用范围，散列函数hash()必须能够将任意类型的关键码key映射为地址空间[0, M)内的一个整数hash(key)，以便确定key所对应的散列地址。由关键码到散列地址的映射，可分解为两步： 利用某一种散列码转换函数hashCode()，将关键码key统一转换为一个整数——称作散列码（hash code） 再利用散列函数将散列码映射为散列地址。 散列码转换函数hashCode()应具备以下的条件： 取值范围应覆盖系统所支持的最大整数范围； 各关键码经hashCode()映射后得到的散列码，相互之间的冲突也应可能减少； hashCode()也应与判等器保持一致，即被判等器判定为相等的词条，对应的散列吗应该相等。 强制转换为整数对于byte、short、int和char等本身即可表示为不超过32位整数的数据类型，可通过类型强制将它们转化为32位的整数。 对成员对象求和long long和double之类长度超过32位的基本类型，不宜强制转换为整数。可以将高32位和低32位分别看作两个32位整数，将二者之和作为散列码。这一方法可推广至由任意多个整数构成的组合对象，如可将其成员对象各自对应的整数累加起来，再截取低32位作为散列码。 多项式散列码与一般的组合对象不同，字符串内各字符之间的次序具有特定含义，故在做散列码转换时，务必考虑它们之间的次序。为计入各字符的出现次序，可取常数$a\\ge2$，并将字符串$x_0x_1\\dots x_{n-1}$的散列码取作： x_0a^{n-1}+x_1a^{n-2}+\\dots x_{n-2}a^{1}+x_{n-1}这一转换等效于，依次将字符串中的各个字符，视作一个多项式的各项系数，故亦称作多项式散列码（polynomial hash code）。其中的常数a非常关键，为尽可能多地保留原字符串的信息以减少冲突，其低比特位不得全为0。另外，针对不同类型的字符串，应通过实验确定a的最佳取值，如对于英语单词之类的字符串，a = 33、37、39或41都是不错的选择。 利用重载机制，实现散列码的统一转换方法hashCode()。 123456789static size_t hashCode ( char c ) &#123; return ( size_t ) c; &#125; //字符static size_t hashCode ( int k ) &#123; return ( size_t ) k; &#125; //整数以及长长整数static size_t hashCode ( long long i ) &#123; return ( size_t ) ( ( i &gt;&gt; 32 ) + ( int ) i ); &#125;static size_t hashCode ( char s[] ) &#123; //生成字符串的循环移位散列码（cyclic shift hash code） unsigned int h = 0; //散列码 for ( size_t n = strlen ( s ), i = 0; i &lt; n; i++ ) //自左向右，逐个处理每一字符 &#123; h = ( h &lt;&lt; 5 ) | ( h &gt;&gt; 27 ); h += ( int ) s[i]; &#125; //散列码循环左移5位，再累加当前字符 return ( size_t ) h; //如此所得的散列码，实际上可理解为近似的“多项式散列码”&#125; //对于英语单词，\"循环左移5位\"是实验统计得出的最佳值 4.散列应用4.1.桶/计数排序给定[0, M)内的n个互异整数（$n\\le M$），如何高效地对其排序？自然，此前学过的向量排序器或列表排序器中的任一排算法，均可完成这一任务，但CBA式排序算法注定在最坏情况下需要$\\Omega(n \\log n)$时间。实际上，针对数值类型和取值范围特定的这一具体问题，完全可在更短时间内完成排序。 简单情况为此，引入长度为M的散列表，如下图为M = 10和n = 5的一个实例。可使用最简单的散列函数hash(key) = key，将这些整数视作关键码并逐一插入散列表中。然后顺序遍历一趟该散列表，依次输出非空桶中存放的关键码，即可得到原整数集合的排序结果。该算法借助一组桶单元实现对一组关键码的分拣，故称作桶排序（bucketsort）。 该算法所用散列表共占$O(M)$空间，散列表的创建和初始化耗时$O(M)$，将所有关键码插入散列表耗时$O(n)$，依次读出非空桶中的关键码耗时$O(M)$，故总体运行时间为$O(n+M)$。 一般情况若将上述问题进一步推广：若允许输入整数重复，又改如何高效地实现排序？ 依然可以沿用以上构思，只不过这次需要处理散列冲突。如上图用过独立链法排解冲突，在将所有整数作为关键码插入散列表之后，只需一趟顺序遍历将各非空桶中的独立链依次串接起来即可得到完整的排序结果。而且只要在串联时留意链表方向，甚至可以确保排序结果的稳定，故如此实现的桶排序算法属于稳定算法。 推广之后的桶排序算法，总体运行时间依然为$O(n+M)$。其实这一问题十分常见，它涵盖了众多实际应用中的具体需求，这类问题还具有一个特点：$n\\gg M$。 在$n\\gg M$的场合，桶排序算法的运行时间将是：$O(n+M)=O(\\max(n,M))=O(n)$，线性正比与于待排序元素的数目，突破了$\\Omega(n \\log n)$的下界。这是因为，以上基于散列表的桶排序算法，采用的是循秩访问的方式，摒弃了以往基于关键码大小比较式的设计思路，故不在受到CBA式算法固有的下界约束。正因为此，桶排序在算法设计方面也占有其独特的地位。 桶排序例子：最大间隙问题：任意n个互异点均将实轴分为n-1段有界区间，其中哪一段最长？ 平凡算法： 对所有点排序，最坏情况下$\\Omega(n \\log n)$； 依次计算各相邻点对的间距，保留最大者，$\\Theta(n)$。 利用散列： 通过一趟顺序扫描找到最靠左和最靠右的点，将其坐标分别记作lo和hi； 建立一个长度为n的散列表，并使用散列函数 $hash(x)=\\lfloor (n-1)*(x-lo)/(hi-lo)\\rfloor$，讲各点分别插入对应的桶单元，其中x为各点的坐标值，hash(x)为对应的桶编号； 对散列表做一趟遍历，在每个非空桶（黑色）内部确定最靠左和最靠右的点，并删除所有的空桶（白色）； 再顺序扫描一趟散列表，即可确定相邻非空桶之间的间隙，记录并报告其中的最大者，即为全局的最大间隙。 正确性： MaxGap至少与相邻的两个桶相交，等价地，定义MaxGap的点不可能属于同一个桶，既有maxGap $\\ge$ w = ( hi - lo ) /( n - 1)。这就意味着MaxGap的两个端点绝不可能落在同一个桶单元内，进一步地，它们必然来自两个不同的非空桶，且左端点在前一非空桶的应该最靠右，右端点在后一非空桶中应该最靠左——故在散列过程中只需记录各桶中的最左、最右点。 复杂度： 空间上，除了输入本身这里只需维护一个散列表，共占用$O(n)$的辅助空间。 无论是生成散列表、找出各桶最左和最右点，还是计算相邻非空桶之间的间距，并找出其中的最大者，该算法的每一步均耗时$O(n)$。故即便在最坏情况下，累计运行时间也不超过$O(n)$。 4.2.基数排序实际应用环境中词条的关键码，未必都是整数。比如，一种常见的情形是，关键码由多个域（字段）组合而成，并采用所谓的字典序（lexicographical order）确定大小次序：任意两个关键码之间的大小关系，取决于它们第一个互异的域。 如日期型关键码，可分解为year、month和day三个整数字段，并按常规惯例，以“ 年 - 月 - 日 ”的优先级定义字典序。有时同一关键码内各字段的类型也未必一致，如扑克牌所对应的关键码，可以分解为枚举型的suite（花色）和整型的number（点数），若按桥牌的约定，以“ 花色 - 点数 ”为字典序，则每副牌都可按大小排列为： 不妨假定个字段类型所对应的比较器均已就绪。设关键码由t个字段$\\{k_t,k_{t-1},\\dots,k_1\\}$，优先级由高到低。于是以其中任一字段$k_i$为关键码，均可调用以上桶排序算法做一趟排序。只需按优先级递增的次序（从$k_1$到$k_t$）针对每一字段各做一趟桶排序，即可实现按整个关键码字典序的排序。 这一算法称作基数排序（radixsort），它采用了低位字段优先（least significant digit first）的策略，其中所作桶排序的趟数，取决于组成关键码的字段数。 正确性： 复杂度： 由以上基数排序的流程，总体运行时间应等于其中各趟桶排序所需时间的总和。设各字段取值范围为$[0,M_i)，1\\le i \\le t$，若记$M=\\max\\{M_1,M_2,\\dots,M_t\\}$。 则总体运行时间不超过： O(n+M_1)+O(n+M_2)+\\dots O(n+M_t)=O(t*(n+M))","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"数据结构与算法（17）红黑树","slug":"数据结构与算法（17）红黑树","date":"2020-03-18T08:24:28.000Z","updated":"2020-03-23T14:01:07.310Z","comments":true,"path":"2020/03/18/数据结构与算法（17）红黑树/","link":"","permalink":"http://nekomoon404.github.io/2020/03/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8817%EF%BC%89%E7%BA%A2%E9%BB%91%E6%A0%91/","excerpt":"","text":"1.概述1.1.动机平衡二叉搜索树的形式多样，如伸展树实现简便，无需修改节点结构、分摊复杂度低，但最坏情况下的单次操作需要$\\Omega(n)$时间；AVL树尽管可以保证最坏情况下的单次操作速度，但需在节点中嵌入平衡因子等标识，且删除之后的重平衡可能需做多达$\\Omega(\\log n)$次旋转，从而频繁地导致全树整体拓扑结构的大幅度变化。 红黑树即是针对后一不足的改进，通过为节点指定颜色，并巧妙地动态调整，红黑树可保证：在每次插入或删除操作之后的重平衡过程中，全树拓扑结构的更新仅涉及常数个节点。尽管最怀情况下需对多达$\\Omega(\\log n)$个节点重染色，但就分摊意义为言仅为$O(1)$个。 为此需要在AVL树“适度平衡”标准的基础上，进一步放宽条件，红黑树所采用的“适度平度”标准可大致表述为：任一节点左、右子树的高度，相差不得超过两倍。 不论是线性的向量、列表、栈或队列，也无论半线性的树结构 以及非线性的图结构，它们大多呈现这么样一种特征：每当经过一次动态的操作 使得其中的逻辑结构发生变化之后，它都会随即完全的转入新的状态 同时将此前的状态完全的遗忘掉，这类结构也因此称作ephemeral data structure，也就是说它们都是随时变化的，每一个状态只会存在于某一个瞬间。 然而在实际应用中往往可能会有更高的要求，比如若将数据结构比作是人，那么它的每一个瞬间状态都相当于人一生中某一时刻的快照，我们或许会对他的历史档案感兴趣，并希望能够任意调阅甚至修改某个历史时刻的档案。因此无论静态还是动态操作，除了指定目标关键码，还需要同时指定一个版本号，用以说明我们是在这个数据结构的哪一份历史档案中去查找特定对象。如果一个数据结构能够支持这种类型的需求，就称作是一致性结构，或持久性结构（persistent structures）。 乍看起来任意数据结构的持久化都不是那么困难的一件事，比如一种直接了当的方法就是为数据结构的每一个所需的版本都独立的保存一份快照，同时将所有版本的入口组成一个搜索结构。这里我们针对一棵真实的BBST记录了它整个生命期内的5个版本。这样 一旦指定了版本号，我们就可以转入对应的快照，并按照常规搜索方法在其中定位需要操作的元素。 从单次访问的效率而言这个结构还可以接受，如果将整个历史快照的数目记作h，那么每次我们只需$O(\\log h)$的时间便可确定版本档案的入口，接下来再花费$O(\\log n)$的时间，在这份档案中进行定位和操作，即单次操作需$O(\\log h+\\log n)$时间。然而就空间而言这种方法是断乎不可接受的，在这样的一组历史快照中每一个元素都可能会被保存多份，渐近的来说n个元素中的每一个都有可能在这组档案中被保存多达h份，这就意味着空间复杂度将伴随着h成线性的速度增长。空间复杂度自然也构成了时间复杂度的一个下限，因此在整个历史过程中为了生成和记录所有的快照，累计所花费的时间也会多达$O(h*n)$这样一个规模，分摊下来为了生成每一组快照都大致需要花费线性的时间来创建一个完整的副本。 那么我们可否就此做一改进呢，比如除了所有元素各自所占用的空间，是否能够将每一个版本所消耗的均摊空间量控制在$O(\\ log n)$的范围内呢，即将复杂度控制在$O(n+h\\log n)$内呢？答案是可以，为此我们需要*利用同一数据结构相邻版本之间的关联性。 对于每一组相邻的历史快照而言，后者都是在前者的基础上做过相对少量的更新而得的，即绝大部分的数据对象在二者之中都是相同的，二者的差异只是非常非常小的一部分。因此可以改用这样一种策略：大量的元素都作共享，只有发生变化的少量的数据元素才需要进行更新。 实际上只要实现方法得当，完全可以将相邻版本之间的差异量控制在$O(\\log n)$的范围。比如对于BBST而言 下图就是一种可行的实现方法：每一条红线都对应于一个共享，蓝色的虚线所指示的是在相邻版本之间的更新量，也是我们不得不花费空间来进行存储的量，而这个量可以控制在极低的水平。在《计算几何》课程中会介绍这类高级结构。 进一步地，我们能否将BBST前后版本的空间差异控制在$O(1)$的范围呢？这样整体的空间复杂度将进一步优化至$O(n+h)$ 而不是$O(h*\\log n)$。答案也是可以的。 为此所应具备的一项必要条件是非常好理解的，即就BBST的树形拓扑结构而言，相邻版本之间的差异本身不能超过常数。然而很遗憾绝大多数的BBST，包括AVL树都不能保证这一点。所谓的拓扑结构差异无非是来自自调整过程中的旋转操作，每一次局部的旋转都意味着在结构上引入常数的差异。因此反过来如果需要保证前后版本在拓扑结构上的差异不超过常数，也就意味着在从前一版本转入后一版本的过程中所执行的旋转操作不得超过常数次。 反观AVL树的两个动态操作，插入操作是满足这一条的，每次插入之后一旦经过一次旋转，局部乃至全树的高度都会复原；然而删除操作却不满足，从AVL树中删除一个节点之后有可能自底而上逐层引发多达$O(\\log n)$次的旋转，从而导致树形拓扑结构的剧烈变化。 因此为了使得上述的构思能够兑现就需要这样一种BBST，它的任何动态操作，无论插入还是删除对树形拓扑结构的影响都能控制在常数的范围之内，而红黑树（red-black tree）正是具有这一特性的一个变种。 1.2.结构为便于对红黑树的理解、实现与分析，不妨按照此前介绍的B-树的做法，统一地引入n+1个外部节点NULL，以保证原树中每一节点的左、右孩子均非空，如此可将二叉树扩展为真二叉树。 由红、黑两色节点组成的二叉搜索树若满足一下条件，即为红黑树（red-black tree）： （1）树根始终为黑色； （2）外部节点均为黑色； （3）其余节点若为红色，则其孩子节点必为黑色； （4）从任一外部节点到根节点的沿途，黑节点的数目相等。 其中，条件（1）和（2）意味着红节点均为内部节点，且其父亲点及左、右孩子必然存在。条件（3）意味着红节点之父必为黑色，因此树中任一通路都不含相邻的红节点。可见在从根节点通往任一节点的沿途，黑节点都不少于红节点，除去根节点本身，沿途所经黑节点的总数称作节点的黑深度（black depth）——根节点的黑深度为0，故条件（4）可理解为“所有外部节点的黑深度统一”。 由条件（4）可进一步推知，在从任一节点通往其任一后代外部节点的沿途，黑节点的总数亦必相等。除去（黑色）外部节点，沿途所经黑节点的总数称作该节点的黑高度（black height）——所有外部节点的黑高度均为0。特别地，根节点的黑高度亦称作全树的黑高度，在数值上与外部节点的黑深度相等。 (2,4)树 == 红黑树红黑树与B-树之间，存在极其密切的联系，经适当转换之后，二者相互等价。具体地，自顶而下逐层考查红黑树各节点，每遇到一个红节点，都将对应的子树整体提升一层，从而与其父节点（必黑）水平对齐，二者之间的联边则相应地调整为横向。如此转换之后，沿水平方向相邻的边至多两条，涉及的节点至多三个，此时若将原红黑树的节点视作关键码，沿水平方向相邻的每一组（父亲至多三个）节点恰好构成4阶B-树的一个节点。 下图针对所有可能的情况，分别给出了具体的转换过程。按照上述对应关系，每棵红黑树都等价于一棵(2,4)-树：前者的每一节点都对应于后者的一个关键码。通往黑节点的边，对黑高度有贡献，并在(2,4)-树中得以保留；通往红节点的边对黑高度没有贡献，在(2,4)-树中对应于节点内部一对相邻的关键码。 对照红黑树的条件，(2,4)-树中的每个节点应包含且仅包含一个黑关键码，同时红关键码不得超过两个，而且若某个节点果真包含两个红关键码，则黑关键码的位置必然居中。 平衡性与所有二叉搜索树一样，红黑树的性能首先取决于其平衡性。包含n个内部节点的红黑树T的高度h也不致超过$O(\\log n)$，更严格地有：$\\log_2(n+1)\\le h\\le 2 \\cdot \\log_2(n+1)$。 若将T的黑高度记作H，则H也是T多对应(2,4)-树，故由关于B-树高度与所含关键码总数关系的结论，有： H\\le 1+\\log_{\\lceil \\frac{4}{2} \\rceil} \\lfloor \\frac{n+1}{2} \\rceil \\le 1+ \\log_2 \\lfloor \\frac{n+1}{2} \\rfloor \\le \\log _2(n+1)既然任一通路都不含相邻的红节点，故必有： h \\le 2H \\le 2\\cdot \\log_2(n+1)=O(\\log n)尽管红黑树不能如完全树那样可做到理想平衡，也不如AVL树那样可做到较严格的适度平衡，但其高度仍控制在最小高度的两倍以内，从渐进的角度看仍是$O(\\log n)$，依然保证了适度平衡——这正是红黑树可高效率支持各种操作的基础。 1.3.红黑树接口定义基于BST模板类，可派生出RedBlack模板类。 1234567891011#include \"BST/BST.h\" //基于BST实现RedBlacktemplate &lt;typename T&gt; class RedBlack : public BST&lt;T&gt; &#123; //RedBlack树模板类protected: void solveDoubleRed ( BinNodePosi(T) x ); //双红修正 void solveDoubleBlack ( BinNodePosi(T) x ); //双黑修正 int updateHeight ( BinNodePosi(T) x ); //更新节点x的高度public: BinNodePosi(T) insert ( const T&amp; e ); //插入（重写） bool remove ( const T&amp; e ); //删除（重写）// BST::search()等其余接口可直接沿用&#125;; 这里直接沿用了BST的查找算法search()，并根据红黑树的重平衡规则与算法，重写了insert()和remove()接口；新加的两个内部功能接口solveDoubleRed()和solveDoubleBlack()，分别用于在节点插入或删除之后恢复全树平衡。 另外还需使用此前二叉树节点模板类BinNode中预留的两个成员变量height和color，可借助辅助宏来检查节点的颜色以及判定是否需要更新（黑）高度记录。这里的确并未真正地实现外部节点，而是将它们统一地直接判定为黑“节点”——尽管它们实际上只不过是NULL，其余节点则一概视作节点。下面是用以简化红黑树算法描述的宏： 123456#define IsBlack(p) ( ! (p) || ( RB_BLACK == (p)-&gt;color ) )#define IsRed(p) ( ! IsBlack(p) )#define BlackHeightUpdated(x) ( /* RedBlack高度更新条件 */ \\ ( stature( (x).lc ) == stature( (x).rc ) ) &amp;&amp; \\ ( (x).height == ( IsRed( &amp;x ) ? stature( (x).lc ) : stature( (x).lc) + 1 ) ) \\) 下面是红黑树节点的黑高度更新的算法实现，此处的height已不再是指常规的树高，而是红黑树的黑高度，节点黑高度需要更新的情况共分三种：左、右孩子的黑高度不等；或者作为红节点，黑高度与其孩子不相等；或者作为黑节点，黑高度不等于孩子的黑高度加一。 1234567template &lt;typename T&gt; int RedBlack&lt;T&gt;::updateHeight ( BinNodePosi(T) x ) &#123; //更新节点高度 x-&gt;height = __max ( stature ( x-&gt;lc ), stature ( x-&gt;rc ) ); //孩子一般黑高度相等，除非出现双黑 /*DSA*/// 红黑树中各节点左、右孩子的黑高度通常相等 /*DSA*/// 这里之所以取更大值，是便于在删除节点后的平衡调整过程中，正确更新被删除节点父亲的黑高度 /*DSA*/// 否则，rotateAt()会根据被删除节点的替代者（高度小一）设置父节点的黑高度 return IsBlack ( x ) ? x-&gt;height++ : x-&gt;height; //若当前节点为黑，则计入黑深度&#125; //因统一定义stature(NULL) = -1，故height比黑高度少一，好在不致影响到各种算法中的比较判断 2.节点插入算法2.1.节点插入与双红现象假定经调用接口search(e)做查找之后，确认目标节点尚不存在。于是，在查找终止的位置x处创建节点，并随机将其染成红色（除非次时全树仅含一个节点），对照红黑树的四项条件，唯有（3）未必满足——即此时x的父亲可能是红色。 12345template &lt;typename T&gt; BinNodePosi(T) RedBlack&lt;T&gt;::insert ( const T&amp; e ) &#123; //将e插入红黑树 BinNodePosi(T) &amp; x = search ( e ); if ( x ) return x; //确认目标不存在（留意对_hot的设置） x = new BinNode&lt;T&gt; ( e, _hot, NULL, NULL, -1 ); _size++; //创建红节点x：以_hot为父，黑高度-1 BinNodePosi(T) xOld = x; solveDoubleRed ( x ); return xOld; //经双红修正后，即可返回&#125; //无论e是否存在于原树中，返回时总有x-&gt;data == e 因新节点的引入，而导致父子节点同时为红色的此类情况，称作“双红”（double red），为修正双红缺陷，可调用solveDoubleRed(x)接口。每引入一个关键码，该接口都可能迭代地调用多次。在此过程红，当前节点x的兄弟及两个孩子（初始时都是外部节点），始终均为黑色。 将x的父亲与祖父分别记作p和g，既然此前的红黑树合法，故作为红节点p的父亲，g必然存在且为黑色。g作为内部节点，其另一孩子（即p的兄弟、x的叔父）也必然存在，将其记作u。以下，视节点u的颜色不同，分两类情况分别处置。 2.2.双红修正2.2.1. RR-1首先，考查u为黑色的情况，此时x的兄弟、两个孩子的黑高度，均与u相等。下图中的(a)和(b)即为此类情况的两种可能（另有两种对称情况）。 此时红黑树条件（3）的违反，从B-树的角度等效地看，即同一节点不应包含紧邻的红色关键码。故只需令黑色关键码与紧邻的红色关键码互换颜色，这等效于按中序遍历次序，对节点x、p和g及其四棵子树，做一次局部“3+4”重构。 如此调整之后，局部子树的黑高度将复原，这意味着全树的平衡也必然得以恢复，同时新子树的根节点b为黑色，也不致引发新的双红现象，至此整个插入操作遂告完成。 2.2.2. RR-2再考查节点u为红色的情况，此时u的左、右孩子非空且均为黑色，其黑高度必与x的兄弟以及两个孩子相等。 此时红黑树条件（3）的违反，从B-树角度等效地看，即该节点因超过4度而发生上溢。从图(c)红黑树的角度看，只需将红节点p和u转为黑色，黑节点g转为红色，x保持红色；从图(c’)的角度看，等效于上溢节点的一次分裂。 如此调整之后局部子树的黑高度复原，然而子树根节点g转为红色之后，有可能在更高层再次引发双红现象，对应于在关键码g被移出并归入上层节点之后，进而导致上层节点的上溢——即上溢的向上传播。若发生这种传播，可将g视作新插入的节点，同样地分以上两类情况如法处置，每经过一次这样的迭代，节点g都将在B-树中（作为关键码）上升一层，而在红黑树中存在双红缺陷的位置也将相应地上升两层，故累计至多迭代$O(\\log n)$次。 特别地，若最后一步迭代之后导致原树根的分裂，并由g独立地构成新的树根节点，则应遵照红黑树条件（1）的要求，将其转换为黑色，如此，全树的黑高度随机增加一层。 2.2.3. 复杂度以上情况的处理流程可归纳为下图，其中的重构、染色等局部操作均只需常数时间、故只需统计这些操作在修正过程中被调用的总次数。可知节点插入之后的双红修正，累计耗时不会超过$O(\\log n)$，即便计入此前的关键码查找预计节点接入等操作，红黑树的每次节点插入操作，都可在$O(\\log n)$时间内完成。 需要指出的是，只有在RR-1修复时才需要1~2次旋转，而且一旦旋转后，修复过程必然随即完成，故就全树拓扑结构而言，每次插入后仅涉及常数次调整；下小节将要介绍的红黑树的节点删除操作也是如此，而回顾此前学过的AVL树，却只能保证前一点。 2.2.4. 实现1234567891011121314151617181920212223242526272829/******************************************************************************* * RedBlack双红调整算法：解决节点x与其父均为红色的问题。分为两大类情况： * RR-1：2次颜色翻转，2次黑高度更新，1~2次旋转，不再递归 * RR-2：3次颜色翻转，3次黑高度更新，0次旋转，需要递归 ******************************************************************************/template &lt;typename T&gt; void RedBlack&lt;T&gt;::solveDoubleRed ( BinNodePosi(T) x ) &#123; //x当前必为红 if ( IsRoot ( *x ) ) //若已（递归）转至树根，则将其转黑，整树黑高度也随之递增 &#123; _root-&gt;color = RB_BLACK; _root-&gt;height++; return; &#125; //否则，x的父亲p必存在 BinNodePosi(T) p = x-&gt;parent; if ( IsBlack ( p ) ) return; //若p为黑，则可终止调整。否则 BinNodePosi(T) g = p-&gt;parent; //既然p为红，则x的祖父必存在，且必为黑色 BinNodePosi(T) u = uncle ( x ); //以下，视x叔父u的颜色分别处理 if ( IsBlack ( u ) ) &#123; //u为黑色（含NULL）时 //*DSA*/printf(\" case RR-1:\\n\"); if ( IsLChild ( *x ) == IsLChild ( *p ) ) //若x与p同侧（即zIg-zIg或zAg-zAg），则 p-&gt;color = RB_BLACK; //p由红转黑，x保持红 else //若x与p异侧（即zIg-zAg或zAg-zIg），则 x-&gt;color = RB_BLACK; //x由红转黑，p保持红 g-&gt;color = RB_RED; //g必定由黑转红///// 以上虽保证总共两次染色，但因增加了判断而得不偿失///// 在旋转后将根置黑、孩子置红，虽需三次染色但效率更高 BinNodePosi(T) gg = g-&gt;parent; //曾祖父（great-grand parent） BinNodePosi(T) r = FromParentTo ( *g ) = rotateAt ( x ); //调整后的子树根节点 r-&gt;parent = gg; //与原曾祖父联接 &#125; else &#123; //若u为红色 //*DSA*/printf(\" case RR-2:\\n\"); p-&gt;color = RB_BLACK; p-&gt;height++; //p由红转黑 u-&gt;color = RB_BLACK; u-&gt;height++; //u由红转黑 if ( !IsRoot ( *g ) ) g-&gt;color = RB_RED; //g若非根，则转红 solveDoubleRed ( g ); //继续调整g（类似于尾递归，可优化为迭代形式） &#125;&#125; 3.节点删除算法3.1. 节点删除与双黑现象为删除关键码e，首先调用标准接口BST::search(e)进行查找，若查找成功，则调用内部接口removeAt(x)实施删除，按照对该接口所约定的语义，x为实际被摘除者，其父亲为p = _hot，其接替者为r，而r的兄弟为外部节点w = NULL。 1234567891011121314template &lt;typename T&gt; bool RedBlack&lt;T&gt;::remove ( const T&amp; e ) &#123; //从红黑树中删除关键码e BinNodePosi(T) &amp; x = search ( e ); if ( !x ) return false; //确认目标存在（留意_hot的设置） BinNodePosi(T) r = removeAt ( x, _hot ); if ( ! ( --_size ) ) return true; //实施删除// assert: _hot某一孩子刚被删除，且被r所指节点（可能是NULL）接替。以下检查是否失衡，并做必要调整 if ( ! _hot ) //若刚被删除的是根节点，则将其置黑，并更新黑高度 &#123; _root-&gt;color = RB_BLACK; updateHeight ( _root ); return true; &#125;// assert: 以下，原x（现r）必非根，_hot必非空 if ( BlackHeightUpdated ( *_hot ) ) return true; //若所有祖先的黑深度依然平衡，则无需调整 if ( IsRed ( r ) ) //否则，若r为红，则只需令其转黑 &#123; r-&gt;color = RB_BLACK; r-&gt;height++; return true; &#125;// assert: 以下，原x（现r）均为黑色 //*DSA*/printBinTree(_hot, 0, 0); solveDoubleBlack ( r ); return true; //经双黑调整后返回&#125; //若目标节点存在且被删除，返回true；否则返回false 因随后的复衡调整位置可能逐层上升，故不妨等效地理解为：w系与r黑高度相等的子红黑树，且随其父亲x一并被摘除，如此可将x统一视作上分支节点，从而更为通用地描述以下算法。不难验证此时红黑树的前两个条件继续满足，但后两个条件却未必。 当然若x原为树根，则无论r颜色如何，只需将其置为黑色并更新黑高度即可，因此不妨假定x的父亲p存在。可分为下面三种情况：第一种只需进行普通的删除操作，即摘除子树w，并将x替换为r；第二种只需在删除操作之后将r转为黑色；第三种，x与r均为黑色，则在删除操作之后，局部子树的黑高度将会降低一个单位。 被删除节点x以其替代者r同为黑色的此类情况，称作“双黑”（double black），此时需调用solveDoubleBlack(r)算法予以修正。为此需考查原黑节点x的兄弟s（必然存在，但可能是外部节点），并视s和p颜色的不同组合，按四种情况分别处置。 3.2. 双黑修正3.2.1. BB-1: s为黑，且至少有一个红孩子t 既然节点x的另一个孩子w = NULL，故从B-树角度看节点x被删除之后的情况，可等效理解为关键码x原所属的节点发生下溢。此时t和s必然属于B-树的同一节点，且该节点就是下溢节点的兄弟。故可参照B-树，下溢节点从父亲点借出一个关键码p，然后从父亲节点从下溢节点的兄弟节点借出一个关键s。 上述调整过程等效于，对节点t、s和p实施“3 + 4 重构”。若这三个节点按中序遍历次序重命名为a，b和c，则还需将a和c染成黑色，b则继承p此前的颜色。对上图的例子而言，就是将t和p染成黑色，s继承p此前的颜色，整个过程中节点r保持黑色不变。 经过以上处理之后，红黑树的所有条件，都在这一局部以及全局得到恢复，故删除操作遂高完成。 3.2.2. BB-2-R：s为黑，且两个孩子均为黑；p为红 与BB-1类似，在对应的B-树中，关键码x的删除导致其所属的节点下溢，但因此时关键码s所在节点只有两个分支，故下溢节点无法从父节点借出关键码p。同样按照B-树平衡算法，将关键码p取出并下降一层，再将原左、右孩子合并为一个节点，如图（b’）。从红黑树角度看，这一过程等效为将s和p颜色互换，如图（b）。 经过以上处理，红黑树的所有条件都在此局部得以恢复。另外，由于关键码p原为红色，故在关键码p所属节点中，其左或右必然还有一个黑色关键码（不可能左右都有），这意味着在关键码p从其中取出之后，不致引发新的下溢。至此，红黑树条件亦必在全局得以恢复，删除操作即告完成。 3.2.3. BB-2-B：s为黑，且两个孩子均为黑；p为黑 此时，与BB-2-R类似，在对应的B-树中，因关键码x的删除，导致其所属节点发生下溢。如图（b’）可将下溢节点与其兄弟合并，从红黑树的角度来看，这一过程可等效为将节点s转换为红色，如图（b）。 然后既然s和x在此之前均为黑色，故p原所属的B-树节点必然仅含p这一个关键码，于是在p被借出之后，该节点必将继而发生下溢，从而有待于后续的进一步修正。从红黑树的角度来看，此时的状态则可等效地理解为：节点p的（黑色）父亲刚被删除。 实际上这也是双黑修正过程中，需要再次迭代的唯一可能。而幸运的是，尽管此类情况可能持续发生，但下溢的位置必然会不断上升，故至多迭代$O(\\log n)$次后必然终止。 3.2.4. BB-3：s为红（其孩子均为黑） 此时作为红节点s的父亲，节点p必为黑色，同时s的两个孩子也应均为黑色。从B-树的角度看，如图（b’）令关键码s与p互换颜色，即可得到一棵与之完全等价的B-树，而从红黑树的角度来看，如图（b），这一转换对应于以节点p为轴做一次旋转，并交换节点s与p的颜色。 虽然经过如此处理之后，双黑缺陷依然存在（子树r的黑高度并未复原），而且缺陷位置的高度也并未上升。然而实际上情况已经发生微妙而本质的变换，观察图（b）可以看到，转换之后的红黑树中，被删除节点x有了一个新的兄弟s&#39;；另外现在的节点p，也已经由黑色转为红色。这就意味着接下来的修正调整只会转入前两种情况：BB-1或者BB-2-R，即接下来至多再做一次迭代调整，整个双黑修正的任务即可完成。 3.2.5. 复杂度以上各情况的处理流程，可归纳为下图： 其中涉及的重构、染色等局部操作，均可在常数时间内完成，故为了估计整个双黑修正过程的时间复杂度，也只需统计这些操作各自的累计执行次数，具体归纳为下表： 可见，前两种情况各自只需做一轮修正，最后一种情况亦不过两轮。情况BB-2-B虽可能需要反复修正，但由于待修正位置的高度严格单调上升，累计也不致过$O(\\log n)$轮，故双黑修正过程总共耗时不超过$O(\\log n)$。即便计入此前的关键码查找和节点删除操作，红黑树的节点删除操作总是可在$O(\\log n)$时间内完成。 从上面的分析可知，一旦在某步迭代中做过节点的旋转调整，整个修复过程便会随机完成。因此与双红修正一样，双黑修正的整个过程，也仅涉及常数次的拓扑结构调整操作。 这一特性也意味着，在每次删除操作之后，拓扑联接关系有所变化的节点绝不会超过常数个——这一点与AVL树的删除操作完成不同，也是二者之间最本质的一项差异。 3.2.6. 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546/****************************************************************************** * RedBlack双黑调整算法：解决节点x与被其替代的节点均为黑色的问题 * 分为三大类共四种情况： * BB-1 ：2次颜色翻转，2次黑高度更新，1~2次旋转，不再递归 * BB-2R：2次颜色翻转，2次黑高度更新，0次旋转，不再递归 * BB-2B：1次颜色翻转，1次黑高度更新，0次旋转，需要递归 * BB-3 ：2次颜色翻转，2次黑高度更新，1次旋转，转为BB-1或BB2R *****************************************************************************/template &lt;typename T&gt; void RedBlack&lt;T&gt;::solveDoubleBlack ( BinNodePosi(T) r ) &#123; BinNodePosi(T) p = r ? r-&gt;parent : _hot; if ( !p ) return; //r的父亲 BinNodePosi(T) s = ( r == p-&gt;lc ) ? p-&gt;rc : p-&gt;lc; //r的兄弟 if ( IsBlack ( s ) ) &#123; //兄弟s为黑 BinNodePosi(T) t = NULL; //s的红孩子（若左、右孩子皆红，左者优先；皆黑时为NULL） if ( IsRed ( s-&gt;rc ) ) t = s-&gt;rc; //右子 if ( IsRed ( s-&gt;lc ) ) t = s-&gt;lc; //左子 if ( t ) &#123; //黑s有红孩子：BB-1 //*DSA*/printf(\" case BB-1: Child (\"); print(s-&gt;lc); printf(\") of BLACK sibling (\"); print(s); printf(\") is RED\\n\"); RBColor oldColor = p-&gt;color; //备份原子树根节点p颜色，并对t及其父亲、祖父 // 以下，通过旋转重平衡，并将新子树的左、右孩子染黑 BinNodePosi(T) b = FromParentTo ( *p ) = rotateAt ( t ); //旋转 if ( HasLChild ( *b ) ) &#123; b-&gt;lc-&gt;color = RB_BLACK; updateHeight ( b-&gt;lc ); &#125; //左子 if ( HasRChild ( *b ) ) &#123; b-&gt;rc-&gt;color = RB_BLACK; updateHeight ( b-&gt;rc ); &#125; //右子 b-&gt;color = oldColor; updateHeight ( b ); //新子树根节点继承原根节点的颜色 //*DSA*/printBinTree(b, 0, 0); &#125; else &#123; //黑s无红孩子 s-&gt;color = RB_RED; s-&gt;height--; //s转红 if ( IsRed ( p ) ) &#123; //BB-2R //*DSA*/printf(\" case BB-2R: Both children (\"); print(s-&gt;lc); printf(\") and (\"); print(s-&gt;rc); printf(\") of BLACK sibling (\"); print(s); printf(\") are BLACK, and parent (\"); print(p); printf(\") is RED\\n\"); //s孩子均黑，p红 p-&gt;color = RB_BLACK; //p转黑，但黑高度不变 //*DSA*/printBinTree(p, 0, 0); &#125; else &#123; //BB-2B //*DSA*/printf(\" case BB-2R: Both children (\"); print(s-&gt;lc); printf(\") and (\"); print(s-&gt;rc); printf(\") of BLACK sibling (\"); print(s); printf(\") are BLACK, and parent (\"); print(p); printf(\") is BLACK\\n\"); //s孩子均黑，p黑 p-&gt;height--; //p保持黑，但黑高度下降 //*DSA*/printBinTree(p, 0, 0); solveDoubleBlack ( p ); //递归上溯 &#125; &#125; &#125; else &#123; //兄弟s为红：BB-3 //*DSA*/printf(\" case BB-3: sibling (\"); print(s); printf(\" is RED\\n\"); //s红（双子俱黑） s-&gt;color = RB_BLACK; p-&gt;color = RB_RED; //s转黑，p转红 BinNodePosi(T) t = IsLChild ( *s ) ? s-&gt;lc : s-&gt;rc; //取t与其父s同侧 _hot = p; FromParentTo ( *p ) = rotateAt ( t ); //对t及其父亲、祖父做平衡调整 //*DSA*/printBinTree&lt;T&gt;(s, 0, 0); solveDoubleBlack ( r ); //继续修正r处双黑——此时的p已转红，故后续只能是BB-1或BB-2R &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"数据结构与算法（16）B-树","slug":"数据结构与算法（16）B-树","date":"2020-03-13T02:04:15.000Z","updated":"2020-03-13T07:04:15.000Z","comments":true,"path":"2020/03/13/数据结构与算法（16）B-树/","link":"","permalink":"http://nekomoon404.github.io/2020/03/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8816%EF%BC%89B-%E6%A0%91/","excerpt":"","text":"1.构思1.1.分级存储从实际应用的需求来看，问题规模的膨胀远远快于存储能力的增长。在同等成本下，存储器的容量越大则访问速度越慢，因此一味地提高存储器容量，亦非解决这一矛盾的良策。实践证明，分级存储才是行之有效的方法。在由内存与外存（磁盘）组成的二阶存储系统中，数据全集往往存放于外存中，计算过程中则可将内存作为外存的高速缓存，存放最常用数据项的复本。借助高效的调度算法，如此便可将内存的“高速度”与外存的“大容量”结合起来。 两个相邻存储级别之间的数据传输，统称I/O操作。各级存储器的访问速度相差悬殊，故应尽可能地减少I/O操作。仍以内存与磁盘为例，其单次访问延迟大致分别在纳秒（ns）和毫秒（ms）级别，相差5至6个数量级。因此在衡量相关算法的性能时，基本可以忽略对内存的访问，转而更多地关注对外存的访问次数。 1.2.多路搜索树当数据规模大道内存已不足以容纳时，常规平衡二叉搜索树的效率将大打折扣，其原因在于，查找过程对外存的访问次数过多。为此，需要充分利用磁盘之类外部存储器的另一特性：就时间成本而言，读取物理地址连续的一千个字节，与读取单个字节几乎没有区别。因此不妨通过时间成本相对较低的多次内存操作，来代替时间成本相对极高的单次外存操作。相应地需要将通常的二叉搜索树，改造为多路搜索树——在中序遍历的意义下，这也是一种等价变换。 如上图，以两层为间隔，将各节点与其左右孩子合并为“大节点”，每个大节点有四个分支，故称作四路搜索树。一般地，以k层为间隔如此重组，可将二叉搜索树转化为等价的2^k路搜索树，统称为多路搜索树（multi - way search tree）。 多路搜索树在逻辑上与BBST等价，同样支持查找等操作，且效果与原二叉搜索树完全等同；然而重要的是，其对外存的访问方式已发生本质变化。实际上，在此时的搜索等下降一层，都以“大节点”为单位从外存读取一组（而不再是单个）关键码，这组关键码在逻辑上与物理上都彼此相邻，故可以批量方式从外存一次性读出，且所需时间与读取单个关键码几乎一样。而每组关键码的最佳数目，取决于不同外存的批量访问特性。 1.3.多路平衡搜索树所谓m阶B-树（B-tree），即m路平衡搜索树（m$\\ge$2）。其中所有外部节点均深度相等。同时，每个内部节点都存有不超过m-1个关键码，以及用以指示对应分支的不超过m个引用。 具体地，存有$n\\le m-1$个关键码：$K_1&lt;K_2&lt;K_3&lt;K_4&lt;\\dots&lt;K_n$的内部节点，同时还配有$n+1&lt;m$个引用：$A_0&lt;A_1&lt;A_2&lt;A_3&lt;A_4&lt;\\dots&lt;A_n$。 反过来，各内部节点的分支数也不能太少，除根以外的所有内部节点，都应满足：$n+1\\ge \\lceil m/2 \\rceil$，而在非空的B-树中，根节点应满足：$n+1\\ge2$。 由于个节点的分支数介于$\\lceil m/2 \\rceil$至$m$之间，故m阶B-树也称作$(\\lceil m/2 \\rceil),m)$-树，如(2,3)-树、(3,6)-树或(7,13)-树等。 B-树的外部节点（external node）实际上未必意味着查找失败，而可能表示目标关键码存在与更低层的某一次外部存储系统中，顺着该节点的指示，即可深入至下一级存储系统并继续查找。正因如此，在计算B-树高度时，还需要计入其最底层的外部节点。 作为与二叉搜索树等价的“扁平化”版本，B-树的宽度（亦即最底层外部节点的数目）往往远大于其高度。既然外部节点均同处于最底层，且深度完全一致，故用图表示时在将它们省略，可由下图(a)进一步精简为下图(c)的紧凑形式： 2.实现2.1.ADT接口按照以上定义，可以模板类的形式描述并实现B-树节点以及B-树结构。B-树节点BTNode类，可实现如以下的代码，这里，同一个节点的所有孩子组织为一个向量，各相邻孩子之间的关键码也组织为一个向量。当然按照B-树的定义，孩子向量的实际长度总是比关键码向量多一。 1234567891011121314151617#include \"vector/vector.h\"#define BTNodePosi(T) BTNode&lt;T&gt;* //B-树节点位置template &lt;typename T&gt; struct BTNode &#123; //B-树节点模板类// 成员（为简化描述起见统一开放，读者可根据需要进一步封装） BTNodePosi(T) parent; //父节点 Vector&lt;T&gt; key; //关键码向量 Vector&lt;BTNodePosi(T)&gt; child; //孩子向量（其长度总比key多一）// 构造函数（注意：BTNode只能作为根节点创建，而且初始时有0个关键码和1个空孩子指针） BTNode() &#123; parent = NULL; child.insert ( 0, NULL ); &#125; BTNode ( T e, BTNodePosi(T) lc = NULL, BTNodePosi(T) rc = NULL ) &#123; parent = NULL; //作为根节点，而且初始时 key.insert ( 0, e ); //只有一个关键码，以及 child.insert ( 0, lc ); child.insert ( 1, rc ); //两个孩子 if ( lc ) lc-&gt;parent = this; if ( rc ) rc-&gt;parent = this; &#125;&#125;; B-树模板类可实现为如下代码： 12345678910111213141516171819202122#include \"BTNode.h\" //引入B-树节点类template &lt;typename T&gt; class BTree &#123; //B-树模板类protected: int _size; //存放的关键码总数 int _order; //B-树的阶次，至少为3——创建时指定，一般不能修改 BTNodePosi(T) _root; //根节点 BTNodePosi(T) _hot; //BTree::search()最后访问的非空（除非树空）的节点位置 void solveOverflow ( BTNodePosi(T) ); //因插入而上溢之后的分裂处理 void solveUnderflow ( BTNodePosi(T) ); //因删除而下溢之后的合并处理public: BTree ( int order = 3 ) : _order ( order ), _size ( 0 ) //构造函数：默认为最低的3阶 &#123; _root = new BTNode&lt;T&gt;(); &#125; ~BTree() &#123; if ( _root ) release ( _root ); &#125; //析构函数：释放所有节点 int const order() &#123; return _order; &#125; //阶次 int const size() &#123; return _size; &#125; //规模 BTNodePosi(T) &amp; root() &#123; return _root; &#125; //树根 bool empty() const &#123; return !_root; &#125; //判空 BTNodePosi(T) search ( const T&amp; e ); //查找 bool insert ( const T&amp; e ); //插入 bool remove ( const T&amp; e ); //删除&#125;; //BTree 后面会看到，B-树的关键码插入操作和删除操作，可能会引发节点的上溢和下溢，因此这里设有内部接口solveOverflow()和solveUnderflow()，分别用于修正此类问题。 2.2.关键码查找B-树结构非常适宜于在相对更小的内存中，实现对大规模数据的高效操作。可以将大数据集组织为B-树并存放与外存，对于活跃的B-树，其根节点会常驻于内存，此外任何时刻通常只有另一节点（称作当前节点）留住于内存。 B-树的查找过程，与二次搜索树的查找过程基本类似：首先以根节点作为当前节点，然后再逐层深入。若在当前节点（所包含的一组关键码）中能够找到目标关键码，则成功返回。否则必在当前节点中确定某一个引用（“失败”位置），并通过它转至逻辑上处于下一层的另一节点。若该节点不是外部节点，则将其载入内存，并更新为当前节点，然后继续重复上述过程。 整个过程如下图所示，从根节点开始，通过关键码的比较不断深入至下一层，直到某一关键码命中（查找成功），或者到达某一外部节点（查找失败）。 与BST的不同之处在于，因此时各节点内通常都包含多个关键码，故有可能需要经过（在内存中的）多次比较，才能确定应该转向下一层的哪个节点并继续查找。 只有在切换和更新当前节点时才会发生I/O操作，而在同一节点内部的查找则完全在内存中进行。因内存的访问速度远远高于外存，再考虑到各节点所含关键码数量通常在128~512之间，故可直接使用顺序查找策略，而不必采用二分查找之类的复杂策略。 B-树的关键码查找算法实现如下： 123456789template &lt;typename T&gt; BTNodePosi(T) BTree&lt;T&gt;::search ( const T&amp; e ) &#123; //在B-树中查找关键码e BTNodePosi(T) v = _root; _hot = NULL; //从根节点出发 while ( v ) &#123; //逐层查找 Rank r = v-&gt;key.search ( e ); //在当前节点中，找到不大于e的最大关键码 if ( ( 0 &lt;= r ) &amp;&amp; ( e == v-&gt;key[r] ) ) return v; //成功：在当前节点中命中目标关键码 _hot = v; v = v-&gt;child[r + 1]; //否则，转入对应子树（_hot指向其父）——需做I/O，最费时间 &#125; //这里在向量内是二分查找，但对通常的_order可直接顺序查找 return NULL; //失败：最终抵达外部节点&#125; 与BST的查找实现类似，这里也约定查找结果由返回的节点位置指代：成功时返回目标关键码所在的节点，上层调用过程可在该节点内进一步查找以确定准确的命中位置；失败时返回对外部节点，其父亲节点则由变量_hot指代。 性能分析：由上可见，B-树查找操作所需的时间不外乎消耗于两个方面：将某一节点载入内存，以及在内存中对当前节点进行查找。鉴于内存、外存在访问速度上的句法差异，相对于前一类时间消耗，后一类时间消耗可以忽略不计，即B-树查找操作的效率主要取决于查找过程中的外存访问次数。 与BST类似，B-树的每一次查找过程中，在每一高度上至多访问一个节点，即对于高度为h的B-树，外存访问不超过$O(h-1)$。B-树节点的分支数并不固定，故其高度h并不完全取决于树中关键码的总数n，对于包含N个关键码的m阶B-树，高度h的取值范围为： \\log_m(N+1)\\le h \\le \\log_{\\lceil m/2 \\rceil} \\lfloor(N+1)/2 \\rfloor+1也就是说，存有N个关键码的m阶B-树的高度$h=\\Theta(\\log_m N)$。因此每次查找过程共需访问$\\Theta(\\log_m N)$个节点，相应地需要做$\\Theta(\\log_m N)$次外存读取操作，因此对存有N个关键码的m阶B-树的每次查找操作，耗时不超过$\\Theta(\\log_m N)$。 尽管没有渐进意义上的改进，但相对而言极其耗时的I/O操作的次数，却已大致缩减为原来的$1/ \\log_2 m$。鉴于m通常取值在256至1024之间，较之此前大致降低一个数量级，故使用B-树后，实际的访问效率将十分可观的提高。 2.3.关键码插入为在B -树中插入一个新的关键码e，首先调用search(e)在树中查找该关键码。若查找成功，则按照“禁止重复关键码”的约定不予插入，操作完成并返回false。 否则查找过程必然终止于某一外部节点v，且其父节点由变量_hot指示，此时的 _hot必然指向某一叶节点。接下来在该节点中再次查找目标关键码e，这次查找是失败的但是可以确定e在其中的正确插入位置r，最后只需将e插至这一位置。 B-树的关键码插入算法实现如下： 123456789template &lt;typename T&gt; bool BTree&lt;T&gt;::insert ( const T&amp; e ) &#123; //将关键码e插入B树中 BTNodePosi(T) v = search ( e ); if ( v ) return false; //确认目标节点不存在 Rank r = _hot-&gt;key.search ( e ); //在节点_hot的有序关键码向量中查找合适的插入位置 _hot-&gt;key.insert ( r + 1, e ); //将新关键码插至对应的位置 _hot-&gt;child.insert ( r + 2, NULL ); //创建一个空子树指针 _size++; //更新全树规模 solveOverflow ( _hot ); //如有必要，需做分裂 return true; //插入成功&#125; 至此，_hot所指的节点中增加了一个关键码。若该节点关键码的总数依然合法（$\\le m-1$），则插入操作随机完成；否则称该节点发生了一次上溢（overflow），此时需要适当的处理，使该节点以及整数重新满足B-树的条件。 上溢与分裂：一般地，刚发生上溢的节点，应恰好含有m个关键码，若取$s=\\lfloor m/2 \\rfloor$，则它们依次为： \\{k_0,\\dots,k_{s-1};\\,k_s;\\,k_{s+1},\\dots,k_{m-1}\\}以$k_s$为界，可将该节点分为前、后两个字节点，且二者大致等长。可令关键码k上升一层，归入其父节点（若存在）中的适当位置，并分别以这两个子节点作为其左、右孩子，这一过程称作节点的分裂（split）。可以验证如此分裂所得的两个孩子节点，均符合m阶B-树关于节点分支数的条件。 可能的情况： 被提升的关键码，可能有三种进一步的处置方式。 首先如图(a1)，设原上溢节点的父亲节点存在，且足以接纳一个关键码。此种情况下，只需将被提升的关键码(37)按次序插入父节点中，修复即告完成，修复后的局部如图(a2)。 其次如图(b1)，尽管上溢节点的父节点存在，但也已处于饱和状态。此时如图(b2)，将被提升的关键码插入父节点之后，会导致父节点也发生上溢，这种现象称作上溢的向上传递。每经过一次修复，上溢节点的高度必然上升一层，最远不至超过树根。 最后如图(c1)，若上溢传递至树根节点，则可令被提升的关键码(37)自成一个节点，并作为新的树根，如图(c2)，至此上溢修复完毕，全树增高一层。这样新创建的树根仅含关键码，这也正是就B-树节点分支数的下限要求而言，树根节点作为例外的原因。 可见整个过程中所作分裂操作的次数，必不超过全树的高度，即$O(\\log_m N)$。 针对上溢的处理算法实现如下： 1234567891011121314151617181920template &lt;typename T&gt; //关键码插入后若节点上溢，则做节点分裂处理void BTree&lt;T&gt;::solveOverflow ( BTNodePosi(T) v ) &#123; if ( _order &gt;= v-&gt;child.size() ) return; //递归基：当前节点并未上溢 Rank s = _order / 2; //轴点（此时应有_order = key.size() = child.size() - 1） BTNodePosi(T) u = new BTNode&lt;T&gt;(); //注意：新节点已有一个空孩子 for ( Rank j = 0; j &lt; _order - s - 1; j++ ) &#123; //v右侧_order-s-1个孩子及关键码分裂为右侧节点u u-&gt;child.insert ( j, v-&gt;child.remove ( s + 1 ) ); //逐个移动效率低 u-&gt;key.insert ( j, v-&gt;key.remove ( s + 1 ) ); //此策略可改进 &#125; u-&gt;child[_order - s - 1] = v-&gt;child.remove ( s + 1 ); //移动v最靠右的孩子 if ( u-&gt;child[0] ) //若u的孩子们非空，则 for ( Rank j = 0; j &lt; _order - s; j++ ) //令它们的父节点统一 u-&gt;child[j]-&gt;parent = u; //指向u BTNodePosi(T) p = v-&gt;parent; //v当前的父节点p if ( !p ) &#123; _root = p = new BTNode&lt;T&gt;(); p-&gt;child[0] = v; v-&gt;parent = p; &#125; //若p空则创建之 Rank r = 1 + p-&gt;key.search ( v-&gt;key[0] ); //p中指向u的指针的秩 p-&gt;key.insert ( r, v-&gt;key.remove ( s ) ); //轴点关键码上升 p-&gt;child.insert ( r + 1, u ); u-&gt;parent = p; //新节点u与父节点p互联 solveOverflow ( p ); //上升一层，如有必要则继续分裂——至多递归O(logn)层&#125; 复杂度：若将B-树的阶次m视作常数，则关键码的移动和复制操作所需的时间都可以忽略。而solveOverflow()算法其中的每一递归实例均只需常数时间，递归层数不超过B-树高度。由此可知，对于存有N个关键码的m阶B-树，每次插入操作都可在$O(\\log_m N)$时间内完成。 实际上，因插入操作而导致$\\Omega(\\log_m N)$次分裂的情况极为罕见，单次插入操作平均引发的分裂次数，远远低于这一估计，故时间通常主要消耗于对目标关键码的查找。 2.4.关键码删除为从B-树中删除关键码e，也首先需要调用search(e)查找e所属的节点。若查找失败，则说明关键码e尚不存在，删除操作即告完成；否则按照代码的出口约定，目标关键码所在的节点必由返回的位置v指示。此时，通过顺序查找，即可进一步确定e在节点 v中的秩r。 不妨假定v是叶节点，否则，e的直接后继（前驱）在其右（左）子树中必然存在，而且可在$O(height(v))$时间内确定它们的位置，其中$height(v)$为节点v的高度。此处不妨选用直接后继，于是e的直接后继关键码所属的节点u必为叶节点，且该关键码就是其中的最小者u[0]，然后令e与u[0]互换位置，即可确保删除的关键码e所属的节点是叶节点。 接下来可直接将e从v中删除，节点v中所含的关键码以及（空）分支将分别减少一半。此时，若该节点所含关键码的总数依然合法（$\\ge \\lceil m/2 \\rceil-1$），则删除操作随机完成。否则，称该节点发生了下溢（underflow），并需要通过适当的处置，使该节点以及整数重新满足B-树的条件。 B-树的关键码删除算法的实现如下： 123456789101112template &lt;typename T&gt; bool BTree&lt;T&gt;::remove ( const T&amp; e ) &#123; //从BTree树中删除关键码e BTNodePosi(T) v = search ( e ); if ( !v ) return false; //确认目标关键码存在 Rank r = v-&gt;key.search ( e ); //确定目标关键码在节点v中的秩（由上，肯定合法） if ( v-&gt;child[0] ) &#123; //若v非叶子，则e的后继必属于某叶节点 BTNodePosi(T) u = v-&gt;child[r+1]; //在右子树中一直向左，即可 while ( u-&gt;child[0] ) u = u-&gt;child[0]; //找出e的后继 v-&gt;key[r] = u-&gt;key[0]; v = u; r = 0; //并与之交换位置 &#125; //至此，v必然位于最底层，且其中第r个关键码就是待删除者 v-&gt;key.remove ( r ); v-&gt;child.remove ( r + 1 ); _size--; //删除e，以及其下两个外部节点之一 solveUnderflow ( v ); //如有必要，需做旋转或合并 return true;&#125; 下溢与合并：由上，在m阶B-树中，刚发生下溢的节点V必恰好包含$\\lceil m/2 \\rceil-2$个关键码和$\\lceil m/2 \\rceil-1$个分支。一下将根据其左、右兄弟所含关键码的数目，分三种情况做相应的处置。 v的左兄弟L存在，且至少包含$\\lceil m/2 \\rceil$个关键码： 如图，不妨设L和V分别是其父节点P中关键码v的左、右孩子，L中最大关键码为x(x $\\le$ y)。再将y从节点P转移至节点V中（作为最小关键码），再将x从L转移至P中（取代原关键码y）。至此，局部乃至整数都重新满足B-树条件，下溢修复完毕。 v的左兄弟R存在，且至少包含$\\lceil m/2 \\rceil$个关键码： 与第一种情况对称。 V的左、右兄弟L和R或者不存在，或者其包含的关键码均不足$\\lceil m/2 \\rceil$个： 实际上，此时的L和R不可能同时存在。如图，不失一般性地设做兄弟节点L存在。当然，此时节点L恰好包含$\\lceil m/2 \\rceil-1$个关键码。 于是为修复节点V的下溢缺陷，如图(b)从父亲节点P中抽出介于L和V之间的关键码y，并通过该关键码将节点L和V“粘接”成一个节点——这一过程称作节点的合并（merge）。要注意的是，在经如此合并而得新节点中，关键码总数应为： (\\lceil m/2 -1 \\rceil)+1+(\\lceil m/2-2\\rceil)=2\\times \\lceil m/2 \\rceil-2\\le m-1故原节点V的下溢缺陷得以修复，而且不致于引发上溢。 接下来，还须检查父节点P——关键码y的删除可能致使该节点出现下溢，这种现象称作下溢的传递，可以通过套用上述三种方法来解决。特别地，当下溢传递至根节点且其中不再含有任何关键码时，即可将其删除并代之以其唯一的孩子节点，全树高度也随之下降一层。 整个下溢修复的过程至多需做$O(\\log_m N)$次节点合并操作。 针对下溢的处理算法实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576template &lt;typename T&gt; //关键码删除后若节点下溢，则做节点旋转或合并处理void BTree&lt;T&gt;::solveUnderflow ( BTNodePosi(T) v ) &#123; if ( ( _order + 1 ) / 2 &lt;= v-&gt;child.size() ) return; //递归基：当前节点并未下溢 BTNodePosi(T) p = v-&gt;parent; if ( !p ) &#123; //递归基：已到根节点，没有孩子的下限 if ( !v-&gt;key.size() &amp;&amp; v-&gt;child[0] ) &#123; //但倘若作为树根的v已不含关键码，却有（唯一的）非空孩子，则 /*DSA*/printf ( \"collapse\\n\" ); _root = v-&gt;child[0]; _root-&gt;parent = NULL; //这个节点可被跳过 v-&gt;child[0] = NULL; release ( v ); //并因不再有用而被销毁 &#125; //整树高度降低一层 return; &#125; Rank r = 0; while ( p-&gt;child[r] != v ) r++; //确定v是p的第r个孩子——此时v可能不含关键码，故不能通过关键码查找 //另外，在实现了孩子指针的判等器之后，也可直接调用Vector::find()定位 /*DSA*/printf ( \"\\nrank = %d\", r );// 情况1：向左兄弟借关键码 if ( 0 &lt; r ) &#123; //若v不是p的第一个孩子，则 BTNodePosi(T) ls = p-&gt;child[r - 1]; //左兄弟必存在 if ( ( _order + 1 ) / 2 &lt; ls-&gt;child.size() ) &#123; //若该兄弟足够“胖”，则 /*DSA*/printf ( \" ... case 1\\n\" ); v-&gt;key.insert ( 0, p-&gt;key[r - 1] ); //p借出一个关键码给v（作为最小关键码） p-&gt;key[r - 1] = ls-&gt;key.remove ( ls-&gt;key.size() - 1 ); //ls的最大关键码转入p v-&gt;child.insert ( 0, ls-&gt;child.remove ( ls-&gt;child.size() - 1 ) ); //同时ls的最右侧孩子过继给v if ( v-&gt;child[0] ) v-&gt;child[0]-&gt;parent = v; //作为v的最左侧孩子 return; //至此，通过右旋已完成当前层（以及所有层）的下溢处理 &#125; &#125; //至此，左兄弟要么为空，要么太“瘦”// 情况2：向右兄弟借关键码 if ( p-&gt;child.size() - 1 &gt; r ) &#123; //若v不是p的最后一个孩子，则 BTNodePosi(T) rs = p-&gt;child[r + 1]; //右兄弟必存在 if ( ( _order + 1 ) / 2 &lt; rs-&gt;child.size() ) &#123; //若该兄弟足够“胖”，则 /*DSA*/printf ( \" ... case 2\\n\" ); v-&gt;key.insert ( v-&gt;key.size(), p-&gt;key[r] ); //p借出一个关键码给v（作为最大关键码） p-&gt;key[r] = rs-&gt;key.remove ( 0 ); //ls的最小关键码转入p v-&gt;child.insert ( v-&gt;child.size(), rs-&gt;child.remove ( 0 ) ); //同时rs的最左侧孩子过继给v if ( v-&gt;child[v-&gt;child.size() - 1] ) //作为v的最右侧孩子 v-&gt;child[v-&gt;child.size() - 1]-&gt;parent = v; return; //至此，通过左旋已完成当前层（以及所有层）的下溢处理 &#125; &#125; //至此，右兄弟要么为空，要么太“瘦”// 情况3：左、右兄弟要么为空（但不可能同时），要么都太“瘦”——合并 if ( 0 &lt; r ) &#123; //与左兄弟合并 /*DSA*/printf ( \" ... case 3L\\n\" ); BTNodePosi(T) ls = p-&gt;child[r - 1]; //左兄弟必存在 ls-&gt;key.insert ( ls-&gt;key.size(), p-&gt;key.remove ( r - 1 ) ); p-&gt;child.remove ( r ); //p的第r - 1个关键码转入ls，v不再是p的第r个孩子 ls-&gt;child.insert ( ls-&gt;child.size(), v-&gt;child.remove ( 0 ) ); if ( ls-&gt;child[ls-&gt;child.size() - 1] ) //v的最左侧孩子过继给ls做最右侧孩子 ls-&gt;child[ls-&gt;child.size() - 1]-&gt;parent = ls; while ( !v-&gt;key.empty() ) &#123; //v剩余的关键码和孩子，依次转入ls ls-&gt;key.insert ( ls-&gt;key.size(), v-&gt;key.remove ( 0 ) ); ls-&gt;child.insert ( ls-&gt;child.size(), v-&gt;child.remove ( 0 ) ); if ( ls-&gt;child[ls-&gt;child.size() - 1] ) ls-&gt;child[ls-&gt;child.size() - 1]-&gt;parent = ls; &#125; release ( v ); //释放v &#125; else &#123; //与右兄弟合并 /*DSA*/printf ( \" ... case 3R\\n\" ); BTNodePosi(T) rs = p-&gt;child[r + 1]; //右兄弟必存在 rs-&gt;key.insert ( 0, p-&gt;key.remove ( r ) ); p-&gt;child.remove ( r ); //p的第r个关键码转入rs，v不再是p的第r个孩子 rs-&gt;child.insert ( 0, v-&gt;child.remove ( v-&gt;child.size() - 1 ) ); if ( rs-&gt;child[0] ) rs-&gt;child[0]-&gt;parent = rs; //v的最左侧孩子过继给ls做最右侧孩子 while ( !v-&gt;key.empty() ) &#123; //v剩余的关键码和孩子，依次转入rs rs-&gt;key.insert ( 0, v-&gt;key.remove ( v-&gt;key.size() - 1 ) ); rs-&gt;child.insert ( 0, v-&gt;child.remove ( v-&gt;child.size() - 1 ) ); if ( rs-&gt;child[0] ) rs-&gt;child[0]-&gt;parent = rs; &#125; release ( v ); //释放v &#125; solveUnderflow ( p ); //上升一层，如有必要则继续分裂——至多递归O(logn)层 return;&#125; 复杂度：与插入操作同理，在存有N个关键码的m阶B-树中的每次关键码删除操作，都可以在$O(\\log_m N)$时间内完成。另外同样地，因某一关键码的删除而导致$\\Omega(\\log_m N)$次合并操作的情况也极为罕见，单词删除操作过程中平均只需做常数次节点的合并。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"数据结构与算法（15）伸展树","slug":"数据结构与算法（15）伸展树","date":"2020-03-12T12:24:46.000Z","updated":"2020-03-12T15:49:11.293Z","comments":true,"path":"2020/03/12/数据结构与算法（15）伸展树/","link":"","permalink":"http://nekomoon404.github.io/2020/03/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8815%EF%BC%89%E4%BC%B8%E5%B1%95%E6%A0%91/","excerpt":"","text":"与前一章的AVL树一样，伸展树也是二叉搜索树的一种形式，相对于AVL，伸展树的实现更为简捷。伸展树无需时刻严格地保持全树的平衡，但却能够在任何足够长的真实操作序列中保持分摊意义上的高效率。伸展树也不需要对基本的二叉树节点结构，做任何附加的要求或改动，更不需要记录平衡平衡因子或高度之类的额外信息，故适用范围更广。 1.构思1.1.局部性信息处理的典型模式是，将所有数据项视作一个集合，并将其组织为某种适宜的数据结构，进而借助操作接口高效访问。通常在任意数据结构的生命周期内，不仅执行不同操作的概率往往极不均衡，而且各操作之间具有极强的相关性，并在整体上多呈现出极强的规律性。其中最为典型的，就是所谓的“数据局部性”（data locality），它包括两个方面的含义： 刚刚被访问过的元素，极有可能在不久之后再次被访问到； 将被访问的下一个元素，极有可能就处于不久之前被访问过的某个元素的附近。 充分利用好此类特性，即可进一步地提高数据结构和算法的效率。就二叉搜索树（BST）而言，数据局部性具体表现为： 刚刚被访问过的节点，极有可能在不久之后再次被访问到； 将被访问的下一节点，极有可能就处于不久之前被访问过的某个节点的附近。 因此需要将刚被访问的节点，及时地“转移”至树根（附近），即可加速后续的操作，转移前后的搜索树必须相互等价，为此仍需借助等价变换的技巧。 1.2.逐层伸展一种直接方式是，每访问过一个节点之后，随即反复地以它的父节点为轴，经适当的旋转将其提升一层，直至最终成为树根。即自下而上，逐层单旋：zig(v-&gt;parent)或zag(v-&gt;parent)，直至v最终被推送至根。 以下图为例，深度为3的节点E刚被访问（查找，插入或“删除”），都可通过3次旋转，将概述等价变换为以E为根的另一颗二叉树。随着节点E的逐层上升，两侧子树的结构也不断地调整，故这一过程也形象地称为伸展（splaying）。 但目前的策略仍存在致命的缺陷——对于很多访问序列，单词访问的分摊时间复杂度在极端情况下可能高达$\\Omega(n)$。以下图为例，若从空树开始依次插入关键码$\\{1,2,3,4,5\\}$，全树的拓扑结构始终呈单链条结构，等价于一维列表。接下来若通过search()接口，再由小到大地依次访问各节点依次，则概述在各次访问之后的结构形态将如图（b~f）所示。 可见在各次访问之后，为将对应节点伸展调整至树根，分别需做$4、4、3、2、1$次旋转。一般地，若节点数为n，则旋转操作的总次数应为： (n-1)+\\{(n-1)+(n-2）+\\dots+1\\}=(n^2+n-2)/2=\\Omega(n^2)分摊到每次访问平均需要$\\Omega(n)$时间，而这一效率不仅远远低于AVL树，而且甚至与原始的二叉搜索树的最坏情况相当。图(a)与图(f)中二叉树的结构完全相同，也就是说经过以上连续的5次访问之后，全树的结构将会复原，这就意味着以上情况可以持续地再现。 这一访问序列导致$\\Omega(n)$平均单次访问时间的原因，可以解释为：在这一可持续重复的过程中，二叉搜索树的高度始终不小于$\\lfloor n/2 \\rfloor$（向下取整）；而且至少有一半的节点在接受访问时，不仅没有靠近树根，而且反过来恰好处于最底层。从树高的角度看，问题根源也可再进一步地解释为：在持续访问的过程中，树高依算术级数逐步从$n-1$递减到$\\lfloor n/2 \\rfloor$，然后在逐步递增回到$n-1$。 1.3.双层伸展为克服上述伸展调整策略的缺陷，一种简便且有效的方法就是：将逐层伸展改为双层伸展。具体地，每次都从当前节点v向上追溯两层（而不是仅一层），并根据其父亲p以及祖父g的相对位置，进行相应的旋转，可分为三种具体的情况： zig - zag / zag - zig：（与AVL树的双旋等效，与逐层伸展也别无二致） zig - zig / zag - zag： 注意顺序是先p后g，若颠倒次序，局部的细微差别将彻底地改变整体（p变成了g的父亲） zig / zag： 若v最初的深度为奇数，则经过若干次双层调整至最后一次调整时，v的父亲p即是树根r。此时必有parent(v) == root(T)，且每轮调整中这种情况至多（在最后）出现一次。 与逐层伸展相比，双层伸展中的zig-zag和zag-zig调整与逐层伸展完全一致，而zig-zig和zag-zag调整则有所不同，事实上后者才是双层伸展策略优于逐层伸展策略的关键所在。 以下图为例做一个对比，最深节点(1)被访问之后再经过双层调整，不仅同样可将该节点伸展至树根，而且同时可使树的高度接近于减半。 即树的形态而言，双层伸展策略可“智能”地折叠被访问的子树分支，从而有效地避免对长分支的连续访问，这就意味着即使节点v的深度为$\\Omega(n)$，双层伸展策略即可将v推至树根，亦可令对应分支的长度以几何级数（大致折半）的速度收缩。 通过双层伸展的策略，伸展树虽不能杜绝最坏情况的发生，却能有效地控制最坏情况发生的频度，从而在分摊意义上保证整体的高效率。Tarjan等人证明了伸展树的单次操作均可在分摊的$O(\\log n)$时间内完成。 2.实现2.1.接口定义基于BST类，可定义伸展树模板类Splay，直接沿用二叉搜索树类，并根据伸展树的平衡规则，重写了上基本操作接口search()，insert()和remove()，另外针对伸展树调整操作，设有一个内部保护型接口splay()。 123456789#include \"BST/BST.h\" //基于BST实现Splaytemplate &lt;typename T&gt; class Splay : public BST&lt;T&gt; &#123; //由BST派生的Splay树模板类protected: BinNodePosi(T) splay ( BinNodePosi(T) v ); //将节点v伸展至根public: BinNodePosi(T) &amp; search ( const T&amp; e ); //查找（重写） BinNodePosi(T) insert ( const T&amp; e ); //插入（重写） bool remove ( const T&amp; e ); //删除（重写）&#125;; 2.2.调整算法双层伸展的调整方法，可实现为一下的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344template &lt;typename NodePosi&gt; inline //在节点*p与*lc（可能为空）之间建立父（左）子关系void attachAsLChild ( NodePosi p, NodePosi lc ) &#123; p-&gt;lc = lc; if ( lc ) lc-&gt;parent = p; &#125;template &lt;typename NodePosi&gt; inline //在节点*p与*rc（可能为空）之间建立父（右）子关系void attachAsRChild ( NodePosi p, NodePosi rc ) &#123; p-&gt;rc = rc; if ( rc ) rc-&gt;parent = p; &#125;template &lt;typename T&gt; //Splay树伸展算法：从节点v出发逐层伸展BinNodePosi(T) Splay&lt;T&gt;::splay ( BinNodePosi(T) v ) &#123; //v为因最近访问而需伸展的节点位置 if ( !v ) return NULL; BinNodePosi(T) p; BinNodePosi(T) g; //*v的父亲与祖父 while ( ( p = v-&gt;parent ) &amp;&amp; ( g = p-&gt;parent ) ) &#123; //自下而上，反复对*v做双层伸展 BinNodePosi(T) gg = g-&gt;parent; //每轮之后*v都以原曾祖父（great-grand parent）为父 if ( IsLChild ( *v ) ) if ( IsLChild ( *p ) ) &#123; //zig-zig /*DSA*/printf ( \"\\tzIg-zIg :\" ); print ( g ); print ( p ); print ( v ); printf ( \"\\n\" ); attachAsLChild ( g, p-&gt;rc ); attachAsLChild ( p, v-&gt;rc ); attachAsRChild ( p, g ); attachAsRChild ( v, p ); &#125; else &#123; //zig-zag /*DSA*/printf ( \"\\tzIg-zAg :\" ); print ( g ); print ( p ); print ( v ); printf ( \"\\n\" ); attachAsLChild ( p, v-&gt;rc ); attachAsRChild ( g, v-&gt;lc ); attachAsLChild ( v, g ); attachAsRChild ( v, p ); &#125; else if ( IsRChild ( *p ) ) &#123; //zag-zag /*DSA*/printf ( \"\\tzAg-zAg :\" ); print ( g ); print ( p ); print ( v ); printf ( \"\\n\" ); attachAsRChild ( g, p-&gt;lc ); attachAsRChild ( p, v-&gt;lc ); attachAsLChild ( p, g ); attachAsLChild ( v, p ); &#125; else &#123; //zag-zig /*DSA*/printf ( \"\\tzAg-zIg :\" ); print ( g ); print ( p ); print ( v ); printf ( \"\\n\" ); attachAsRChild ( p, v-&gt;lc ); attachAsLChild ( g, v-&gt;rc ); attachAsRChild ( v, g ); attachAsLChild ( v, p ); &#125; if ( !gg ) v-&gt;parent = NULL; //若*v原先的曾祖父*gg不存在，则*v现在应为树根 else //否则，*gg此后应该以*v作为左或右孩子 ( g == gg-&gt;lc ) ? attachAsLChild ( gg, v ) : attachAsRChild ( gg, v ); updateHeight ( g ); updateHeight ( p ); updateHeight ( v ); &#125; //双层伸展结束时，必有g == NULL，但p可能非空 if ( p = v-&gt;parent ) &#123; //若p果真非空，则额外再做一次单旋 /*DSA*/if ( IsLChild ( *v ) ) &#123; printf ( \"\\tzIg :\" ); print ( p ); print ( v ); printf ( \"\\n\" ); &#125; /*DSA*/else &#123; printf ( \"\\tzAg :\" ); print ( p ); print ( v ); printf ( \"\\n\" ); &#125; if ( IsLChild ( *v ) ) &#123; attachAsLChild ( p, v-&gt;rc ); attachAsRChild ( v, p ); &#125; else &#123; attachAsRChild ( p, v-&gt;lc ); attachAsLChild ( v, p ); &#125; updateHeight ( p ); updateHeight ( v ); &#125; v-&gt;parent = NULL; return v;&#125; //调整之后新树根应为被伸展的节点，故返回该节点的位置以便上层函数更新树根 2.3.查找在伸展树中查找任一关键e，首先调用二叉搜索树BST的通用算法searchIn()，尝试查找具有关键码e的节点，无论查找是否成功，都继而调用splay()算法，将查找终止位置处的节点伸展到树根。 12345template &lt;typename T&gt; BinNodePosi(T) &amp; Splay&lt;T&gt;::search ( const T&amp; e ) &#123; //在伸展树中查找e BinNodePosi(T) p = searchIn ( _root, e, _hot = NULL ); _root = splay ( p ? p : _hot ); //将最后一个被访问的节点伸展至根 return _root;&#125; //与其它BST不同，无论查找成功与否，_root都指向最后被访问的节点 2.4.插入为将关键码e插至伸展树T中，首先调用伸展树查找接口Splay::search(e)查找该关键码。于是最后被访问的节点t，将通过伸展树被提升为树根，其左右子树分别被记为$T_L$和$T_R$。接下来，根据e与t的大小关系，以t为界将T分裂为两棵子树。如设e大于t，可切断t与其右孩子之间的联系，再将以e为关键码的新节点v作为树根，并以t作为其左孩子，以$T_R$作为其右子树。v小于t的情况与此完全对称。 伸展树的插入算法的具体实现如下： 1234567891011121314template &lt;typename T&gt; BinNodePosi(T) Splay&lt;T&gt;::insert ( const T&amp; e ) &#123; //将关键码e插入伸展树中 if ( !_root ) &#123; _size++; return _root = new BinNode&lt;T&gt; ( e ); &#125; //处理原树为空的退化情况 if ( e == search ( e )-&gt;data ) return _root; //确认目标节点不存在 _size++; BinNodePosi(T) t = _root; //创建新节点。以下调整&lt;=7个指针以完成局部重构 if ( _root-&gt;data &lt; e ) &#123; //插入新根，以t和t-&gt;rc为左、右孩子 t-&gt;parent = _root = new BinNode&lt;T&gt; ( e, NULL, t, t-&gt;rc ); //2 + 3个 if ( HasRChild ( *t ) ) &#123; t-&gt;rc-&gt;parent = _root; t-&gt;rc = NULL; &#125; //&lt;= 2个 &#125; else &#123; //插入新根，以t-&gt;lc和t为左、右孩子 t-&gt;parent = _root = new BinNode&lt;T&gt; ( e, NULL, t-&gt;lc, t ); //2 + 3个 if ( HasLChild ( *t ) ) &#123; t-&gt;lc-&gt;parent = _root; t-&gt;lc = NULL; &#125; //&lt;= 2个 &#125; updateHeightAbove ( t ); //更新t及其祖先（实际上只有_root一个）的高度 return _root; //新节点必然置于树根，返回之&#125; //无论e是否存在于原树中，返回时总有_root-&gt;data == e 尽管伸展树并不需要记录和维护节点的高度，为与其他平衡二叉搜索树的实现保持统一，这里对节点的高度做了及时的更新，在实际应用中可视情况省略这类更新。 2.5.删除为从伸展树T中删除关键码为e的节点，首先亦调用接口Splay::search(e)查找该关键码，且不妨设命中节点为v。于是v将随机通过伸展被提升为树根，其左右子树分别记作$T_L$和$T_R$。接下来将v摘除，然后在$T_R$中再次查找关键码e，尽管这一查找注定失败，却可以将$T_R$中的最小节点m伸展提升为树根。 得益于二叉搜索树的顺序性，此时节点m的左子树必然为空；同时$T_L$中所有节点都小于m，于是只需将$T_L$作为左子树与m相互连接，即可得到一棵完整的二叉搜索树。这样不仅删除了v，而且既然新树根m在原树中是v的直接后继，故数据局部性也得到了利用。当然其中第二次查找也可在$T_L$（若非空）中进行。 伸展的删除算法的具体实现如下： 12345678910111213141516171819template &lt;typename T&gt; bool Splay&lt;T&gt;::remove ( const T&amp; e ) &#123; //从伸展树中删除关键码e if ( !_root || ( e != search ( e )-&gt;data ) ) return false; //若树空或目标不存在，则无法删除 BinNodePosi(T) w = _root; //assert: 经search()后节点e已被伸展至树根 if ( !HasLChild ( *_root ) ) &#123; //若无左子树，则直接删除 _root = _root-&gt;rc; if ( _root ) _root-&gt;parent = NULL; &#125; else if ( !HasRChild ( *_root ) ) &#123; //若无右子树，也直接删除 _root = _root-&gt;lc; if ( _root ) _root-&gt;parent = NULL; &#125; else &#123; //若左右子树同时存在，则 BinNodePosi(T) lTree = _root-&gt;lc; lTree-&gt;parent = NULL; _root-&gt;lc = NULL; //暂时将左子树切除 _root = _root-&gt;rc; _root-&gt;parent = NULL; //只保留右子树 search ( w-&gt;data ); //以原树根为目标，做一次（必定失败的）查找///// assert: 至此，右子树中最小节点必伸展至根，且（因无雷同节点）其左子树必空，于是 _root-&gt;lc = lTree; lTree-&gt;parent = _root; //只需将原左子树接回原位即可 &#125; release ( w-&gt;data ); release ( w ); _size--; //释放节点，更新规模 if ( _root ) updateHeight ( _root ); //此后，若树非空，则树根的高度需要更新 return true; //返回成功标志&#125; //若目标节点存在且被删除，返回true；否则返回false 2.6.优缺点优点： 无需记录节点高度或平衡因子；编程实现简单易行 //优于AVL树 分摊复杂度$O(\\log n)$——与AVL树相当 局部性强、缓存命中率极高时（即k &lt;&lt; n &lt;&lt; m） 效率甚至可以更高——自适应的$O(\\ log k)$，任何连续的m次查找，都可在$O(m\\log k + n \\log n)$时间内完成。 缺点： 仍不能保证单词最坏情况的出现 //不适用于对效率敏感的场合 复杂度的分析稍显复杂","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"数据结构与算法（14）AVL树","slug":"数据结构与算法（14）AVL树","date":"2020-03-05T01:29:44.000Z","updated":"2020-03-05T08:40:33.761Z","comments":true,"path":"2020/03/05/数据结构与算法（14）AVL树/","link":"","permalink":"http://nekomoon404.github.io/2020/03/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8814%EF%BC%89AVL%E6%A0%91/","excerpt":"","text":"AVL树是由G. M. Adelson-Velsky和E. M. Landis与1962年发明的一种平衡二叉搜索树，并以他们名字的首字母命名。通过合理设定适度平衡的标准，并借助以上等价变换，AVL树可以实现近乎理想的平衡。在渐进意义下，AVL树始终将高度控制在$O(\\log n)$以内，从而保证每次查找，插入或删除操作，均可在$O(\\log n)$的时间内完成。 1.定义及性质任一节点v的平衡因子（balance factor）定义为“其左、右子树的高度差”，即： balFac(v) = height( lc(v) ) - height( rc(v) ) 在AVL树中，任意节点的平衡因子的绝对值不超过1，即|balFac(v)| $\\le$ 1，注意空树高度取-1，单节点子树（叶节点）高度取0。AVL树未必理想平衡，但必然适度平衡。 适度平衡要求当节点数固定为n时，树的高度渐进地不超过$O(\\log n)$。欲证明AVL树满足适度平衡条件，可转化为证明当树的高度固定时，树中的节点数不会太少。考虑高度为h的所有AVL树，并取S为其中节点数最少的一棵，如下图，$S_L$和$S_R$分别为$S$的左右子树，$S_L$和$S_R$也都是AVL树，而且高度不超过h-1，又因为$S$的节点数最少，所以$S_L$和$S_R$中一个高度为h-1，一个高度为h-2。从而可以得到以下递推关系，并加以整理得到fibonacci数列的形式： \\begin{align*} S(h)&=1+S(h-1)+S(h-2)\\\\ S(h)+1&=[S(h-1)+1]+[S(h-2)+1]\\\\ fib(h+3)&=fib(h+2)+fib(h+1) \\end{align*}从高度为0，1，2的树的节点数可以推出节点数$S(h)$和fibonacci数的关系，即$S(h)=fib(h+3)$。 可得对于高度为h的AVL数，节点数n满足$n\\ge \\Omega(\\Phi ^h)$，因此对于节点数固定为n的AVL数，其高度h满足$h\\le O(\\log n)$。 基于BST模板类，可直接派生出AVL模板类，并根据AVL树的重平衡规则与算法，重写了insert()和remove()接口。另外，为简化对节点平衡性的判断，算法实现时可借用宏定义。 12345678910#define Balanced(x) ( stature( (x).lc ) == stature( (x).rc ) ) //理想平衡条件#define BalFac(x) ( stature( (x).lc ) - stature( (x).rc ) ) //平衡因子#define AvlBalanced(x) ( ( -2 &lt; BalFac(x) ) &amp;&amp; ( BalFac(x) &lt; 2 ) ) //AVL平衡条件template &lt;typename T&gt; class AVL : public BST&lt;T&gt; &#123; //由BST派生AVL树模板类public: BinNodePosi(T) insert ( const T&amp; e ); //插入（重写） bool remove ( const T&amp; e ); //删除（重写）// BST::search()等其余接口可直接沿用&#125;; 按照BST规则插入或删除节点之后，AVL中节点的高度可能会发生变化，平衡性可能会破坏，以致于不再满足AVL树的条件。如此因节点x的插入或删除而暂时失衡的节点，构成失衡的节点集，记作UT(x)。 为了恢复AVL的平衡性需要借助等价变换，它有以下优势： 局部性：所有的旋转都在局部进行 //每行只需$O(1)$时间 快速性：在每一深度只需检查并旋转至多一次 //共$O(\\log n)$次 2.节点插入新引入节点x后，UT(x)中的节点都是x的祖先，且高度不低于x的祖父。将其中的最深者记为g(x)，在 x与g(x)的通路上，设p为g(x)的孩子，v为p的孩子，既然g(x)不低于x的祖父，则p必是x的真祖先。 首先需要找到如上定义的g(x)，可以从x出发沿parent指针逐层上行并核对平衡因此，首次遇到的失衡祖先即为g(x)，既然原树是平衡的，故这一过程需要$O(\\log n)$。既然g(x)是因x的引入而失衡，则p和v的高度均不低于其各自的兄弟，因此借助宏定义的tallerChild()，即可反过来由g(x)找到p和v。 1234567#define tallerChild(x) ( \\ stature( (x)-&gt;lc ) &gt; stature( (x)-&gt;rc ) ? (x)-&gt;lc : ( /*左高*/ \\ stature( (x)-&gt;lc ) &lt; stature( (x)-&gt;rc ) ? (x)-&gt;rc : ( /*右高*/ \\ IsLChild( * (x) ) ? (x)-&gt;lc : (x)-&gt;rc /*等高：与父亲x同侧者（zIg-zIg或zAg-zAg）优先*/ \\ ) \\ ) \\) 以下根据节点g(x)、p和v之间具体的联接方向，采用不同的局部调整方案。 单旋： 当g、p和v在同一侧时，如下图都在右侧（可称为zag-zag，对称地若都在左侧可称为zig-zig），在$T_2$或者$T_3$处插入新的节点，插入的节点在底部以浅灰色表示，g的平衡因子由原来的-1变为-2，失衡。这种情况只需g经单次旋转zag(g(x))，图中的子树高度复原，更高的祖先也比平衡，全树复衡。 双旋： 当g、p和v不在同一侧时，如下图中p是g的右孩子，v是p的左孩子（可称为zag-zig，对称地若p是g的左孩子，v是p的右孩子可称为zig-zag），在$T_1$或者$T_2$处插入新的节点，插入新节点后，g的平衡因子由原来的-1变为-2，失衡。这种情况需要先做顺时针旋转zig(p)，在做逆时针旋转zag(g)，得到一棵平衡的等价二叉树。 无论单旋还是双旋，经局部调整之后，不仅g(x)能够重获平衡，而且局部子树的高度也必将平衡。这就意味着，g(x)以上所有祖先的平衡因子亦将统一地复原，即在AVL树中插入新节点后，仅需要至多两次旋转，即可使整数恢复平衡。 AVL树插入节点的代码实现： 12345678910111213template &lt;typename T&gt; BinNodePosi(T) AVL&lt;T&gt;::insert ( const T&amp; e ) &#123; //将关键码e插入AVL树中 BinNodePosi(T) &amp; x = search ( e ); if ( x ) return x; //确认目标节点不存在 BinNodePosi(T) xx = x = new BinNode&lt;T&gt; ( e, _hot ); _size++; //创建新节点x// 此时，x的父亲_hot若增高，则其祖父有可能失衡 for ( BinNodePosi(T) g = _hot; g; g = g-&gt;parent ) &#123; //从x之父出发向上，逐层检查各代祖先g if ( !AvlBalanced ( *g ) ) &#123; //一旦发现g失衡，则（采用“3 + 4”算法）使之复衡，并将子树 FromParentTo ( *g ) = rotateAt ( tallerChild ( tallerChild ( g ) ) ); //重新接入原树 break; //g复衡后，局部子树高度必然复原；其祖先亦必如此，故调整随即结束 &#125; else //否则（g依然平衡），只需简单地 updateHeight ( g ); //更新其高度（注意：即便g未失衡，高度亦可能增加） &#125; //至多只需一次调整；若果真做过调整，则全树高度必然复原 return xx; //返回新节点位置&#125; //无论e是否存在于原树中，总有AVL::insert(e)-&gt;data == e 效率：该算法首先按照二叉搜索树的常规算法在$O(\\log n)$时间内插入新节点x，既然原树是平衡的，故至多坚持$O(\\log n)$个节点即可确定g(x)，而至多旋转两次即可使局部乃至全树恢复平衡，因此AVL树的节点插入操作可以在$O(\\log n)$时间内完成。 3.节点删除与插入操作不同的是，在删除节点x后，以及在随后的调整过程中，失衡节点集UT(x)始终至多只含一个节点。若该节点g(x)存在，其高度必与失衡前相同，而且g(x)有可能就是x的父亲。 与插入操作同理，从_hot节点出发沿parent指针上行经过$O(\\log n)$时间即可确定g(x)位置，作为失衡节点的g(x)在不包含x的一侧必有一个非空孩子p，且p的高度至少为1。选取p的孩子节点v时可按以下规则：若两个孩子不等高，则v取其中的更高者；若两个孩子等高，则取与p同向者。 以下不妨假设失衡后g(x)的平衡因子为+2，根据g、p和v的关系，通过等价变换，同样可以使得这一局部恢复平衡。 单旋： 如下图g、p和v在同一侧，在$T_3$中删除节点使得节点g(x)失衡，通过一次顺时针旋转zig( g )即可恢复局部的平衡。这里约定图中以虚线连接的灰色方块所对应的节点，不能同时为空；$T_2$底部的灰色方块所对应的节点，可以为空，也可以非空。 双旋： 如下图g、p和v不在同一侧，在$T_3$中删除节点使得节点g(x)失衡，这种情况需要先做一次逆时针旋转zag( p )，再做一次顺时针旋转zig( g )，即可恢复局部平衡。 失衡传播： 经过旋转操作使局部恢复平衡后，子树的高度未必复原，如上图中子树的高度降低，因此其更高的祖先可能失衡。在删除节点之后的调整过程中，这种由于低层失衡节点的重平衡而致使其更高层祖先失衡的现象，称作“失衡传播”。 失衡传播的方向必然是自底而上的，因而不致于影响到狗带节点。在此过程中的任一时刻，至多只有一个失衡的节点；高层的某一节点由平衡转为失衡，只能发生在下层失衡节点恢复平衡之后。因此可沿parent指针逐层遍历所有祖先，每找到一个失衡的祖先节点，即可套用以上方法式使之恢复平衡。 AVL树删除节点的代码实现： 12345678910template &lt;typename T&gt; bool AVL&lt;T&gt;::remove ( const T&amp; e ) &#123; //从AVL树中删除关键码e BinNodePosi(T) &amp; x = search ( e ); if ( !x ) return false; //确认目标存在（留意_hot的设置） removeAt ( x, _hot ); _size--; //先按BST规则删除之（此后，原节点之父_hot及其祖先均可能失衡） for ( BinNodePosi(T) g = _hot; g; g = g-&gt;parent ) &#123; //从_hot出发向上，逐层检查各代祖先g if ( !AvlBalanced ( *g ) ) //一旦发现g失衡，则（采用“3 + 4”算法）使之复衡，并将该子树联至 g = FromParentTo ( *g ) = rotateAt ( tallerChild ( tallerChild ( g ) ) ); //原父亲 updateHeight ( g ); //并更新其高度（注意：即便g未失衡，高度亦可能降低） &#125; //可能需做Omega(logn)次调整——无论是否做过调整，全树高度均可能降低 return true; //删除成功&#125; //若目标节点存在且被删除，返回true；否则返回false 效率：较之插入操作，删除操作可能需要在重平衡上要多花费 一些时间，既然需要做重平衡的节点都是x的祖先，故重平衡过程累计只需$O(\\log n)$时间，因此AVL树的节点删除操作总体的时间复杂度依然是$O(\\log n)$。 4.统一重平衡算法这一节主要解决如何实现第2、3节中插入/删除操作实现代码中的重平衡算法rotateAt()，上述重平衡的方法实质上可以转换成一种更简明的方法，无论是插入或删除操作，都要从刚发生修改的位置x出发向上直到遇到最低的失衡节点g(x)，在g(x)更高一侧的子树中，其孩子节点p和孙子节点v必然存在，按中序遍历次序，将其重命名为：$a &lt; b &lt; c$。它们总共拥有互不相交的四棵（可能为空）子树，按中序遍历序列，将其重命中为：$T_0&lt;T_1&lt;T_2&lt;T_3$。 将原先以g为根的子树S，替换为一棵新子树S’： 等价变换后得到的新子树也是一棵AVL树，保持中序遍历次序：$T_0&lt;a&lt;T_1&lt;b&lt;T_2&lt;c&lt;T_3$。实际上，这一理解涵盖了此前两节所有的单旋和双旋情况（就相当于不进行“旋转”操作，而是直接将节点和子树“拆开”来按中序遍历次序保持不变重新拼到一起，使之恢复平衡）。相应的重构过程，仅涉及局部的三个节点及四棵子树，故称作“ 3 + 4 ”重构。 这种重构算法可以实现为下面的代码： 123456789101112131415161718/********************************************************************************** * 按照“3 + 4”结构联接3个节点及其四棵子树，返回重组之后的局部子树根节点位置（即b） * 子树根节点与上层节点之间的双向联接，均须由上层调用者完成 * 可用于AVL和RedBlack的局部平衡调整 *********************************************************************************/template &lt;typename T&gt; BinNodePosi(T) BST&lt;T&gt;::connect34 ( BinNodePosi(T) a, BinNodePosi(T) b, BinNodePosi(T) c, BinNodePosi(T) T0, BinNodePosi(T) T1, BinNodePosi(T) T2, BinNodePosi(T) T3) &#123; //*DSA*/print(a); print(b); print(c); printf(\"\\n\"); a-&gt;lc = T0; if ( T0 ) T0-&gt;parent = a; a-&gt;rc = T1; if ( T1 ) T1-&gt;parent = a; updateHeight ( a ); c-&gt;lc = T2; if ( T2 ) T2-&gt;parent = c; c-&gt;rc = T3; if ( T3 ) T3-&gt;parent = c; updateHeight ( c ); b-&gt;lc = a; a-&gt;parent = b; b-&gt;rc = c; c-&gt;parent = b; updateHeight ( b ); return b; //该子树新的根节点&#125; 根据g、p和v之间的联接关系，可以将g、p和v，以及它们的四棵子树与$a,b,c,T_0,T_1,T_2.T_3$对应起来，如” zig-zig “的联接方式，对应的是connect34(v, p, g, v-&gt;lc, v-&gt;rc, p-&gt;rc, g-&gt;rc)；而” zig-zag “的联接方式，对应的是connect34(p, v, g, p-&gt;lc, v-&gt;lc, v-&gt;rc, g-&gt;rc)，总之就是按照中序遍历次序代入就可。 利用connect34()算法，即可视不同情况，按如下的代码实现重平衡算法rotateAt()，rotateAt()算法的复杂度依然是$O(1)$。 123456789101112131415161718192021222324/********************************************************************************** * BST节点旋转变换统一算法（3节点 + 4子树），返回调整之后局部子树根节点的位置 * 注意：尽管子树根会正确指向上层节点（如果存在），但反向的联接须由上层函数完成 *********************************************************************************/template &lt;typename T&gt; BinNodePosi(T) BST&lt;T&gt;::rotateAt ( BinNodePosi(T) v ) &#123; //v为非空孙辈节点 /*DSA*/if ( !v ) &#123; printf ( \"\\a\\nFail to rotate a null node\\n\" ); exit ( -1 ); &#125; BinNodePosi(T) p = v-&gt;parent; BinNodePosi(T) g = p-&gt;parent; //视v、p和g相对位置分四种情况 if ( IsLChild ( *p ) ) /* zig */ if ( IsLChild ( *v ) ) &#123; /* zig-zig */ //*DSA*/printf(\"\\tzIg-zIg: \"); p-&gt;parent = g-&gt;parent; //向上联接 return connect34 ( v, p, g, v-&gt;lc, v-&gt;rc, p-&gt;rc, g-&gt;rc ); &#125; else &#123; /* zig-zag */ //*DSA*/printf(\"\\tzIg-zAg: \"); v-&gt;parent = g-&gt;parent; //向上联接 return connect34 ( p, v, g, p-&gt;lc, v-&gt;lc, v-&gt;rc, g-&gt;rc ); &#125; else /* zag */ if ( IsRChild ( *v ) ) &#123; /* zag-zag */ //*DSA*/printf(\"\\tzAg-zAg: \"); p-&gt;parent = g-&gt;parent; //向上联接 return connect34 ( g, p, v, g-&gt;lc, p-&gt;lc, v-&gt;lc, v-&gt;rc ); &#125; else &#123; /* zag-zig */ //*DSA*/printf(\"\\tzAg-zIg: \"); v-&gt;parent = g-&gt;parent; //向上联接 return connect34 ( g, v, p, g-&gt;lc, v-&gt;lc, v-&gt;rc, p-&gt;rc ); &#125;&#125; 对AVL树的综合评价： 优点：无论查找、插入或删除，最坏情况下的复杂度均为$O(\\log n)$，需要$O(n)$的存储空间。 缺点：需要借助高度或平衡因子，为此需要改造元素结构，或额外封装。实测复杂度与理论值尚有差距，删除操作后，最多需要重平衡$\\Omega(\\log n)$次，若需频繁地进行插入/删除操作，未免得不偿失。单次动态调整后，全树的拓扑结构的变化量可能高达$\\Omega(\\log n)$，我们希望将它们控制到更低，比如在下一章将要介绍的红黑树，则可以将这个变化量严格地控制在每次不超过常数无，论对于插入操作还是删除操作都是如此。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"Cpp基础（14）容器","slug":"Cpp基础（14）容器","date":"2020-03-03T00:21:37.000Z","updated":"2020-03-03T02:41:42.312Z","comments":true,"path":"2020/03/03/Cpp基础（14）容器/","link":"","permalink":"http://nekomoon404.github.io/2020/03/03/Cpp%E5%9F%BA%E7%A1%80%EF%BC%8814%EF%BC%89%E5%AE%B9%E5%99%A8/","excerpt":"","text":"容器是可容纳各种数据类型的数据结构 包括顺序容器和关联容器 还有一类不提供真正的用于存储元素的数据结构实现，称作容器适配器 容器适配器不支持迭代器，由使用者选择合适的底层数据结构 1.顺序容器在顺序容器中，元素的插入位置与元素的值无关： vector （声明于 &lt; vector&gt;） 顺序表：实现了一个动态数组，可以在常数时间内完成随机存取元素，可以自动调整大小，在尾端增删元素时具有较佳的性能 array （C++11 中新增，声明于 &lt; array&gt;）顺序表：封装了一个静态数组，只能在初始化时指定大小 deque （声明于 &lt; deque&gt;）双端队列：实现了一个动态数组，可以在常数时间内完成随机存取元素，可以快速地在数组的头尾两端增删元素 list （声明于 &lt; list&gt;）双向链表：不支持随机存取，但在任何位置增删元素都能在常数时间完成 forward_list （ C++11 中新增，声明于 ）单向链表：list 类的单链表版，去掉了一些操作 1.1. vectorvector 实现了一个动态数组，在实例化 vector 模板类时，需要在 &lt;&gt; 内指定容器中元素的具体类型，元素可以是基本数据类型，也可以是自定义的类。vector 可以在常数时间内完成随机存取元素，支持随机访问迭代器。 vector 可以自动调整大小，在尾端增删元素时具有较佳的性能，可以视为在常数时间内完成；在其他位置增删元素时较慢，与具体的位置有关。所有 STL 算法都能对 vector 操作。 构造 vector 对象时，需要声明具体的数据类型：可以指定一个初始长度，但之后还可以通过其他成员函数改变长度；还可以传入两个参数，分别指定初始长度和元素的初始值。也可以根据一个已有的数组来构造 vector 对象，传入的两个参数分别表示数组的起始和终止位置，是个左闭右开的区间。 1234567891011121314#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;vector&gt;using namespace std;int main() &#123; vector&lt;int&gt; v1; // 长度为 0 的 int 数组 vector&lt;int&gt; v2(5); // 长度为 5 的 int 数组 vector&lt;int&gt; v3(4, 7); // 长度为 4 的 int 数组，且初值都为 7 vector&lt;string&gt; v4(3, “Hi”); // 长度为 3 且值都为 Hi 的 string 数组 double x[] = &#123;1.1, 2.3, 5.8, 13.21&#125;; vector&lt;double&gt; v5(x, x + 4); // 从 x[0..3] 构造一个 double 数组 vector&lt;double&gt; v6(x + 1, x + 2); // 从 x[1] 构造一个 double 数组 return 0;&#125; 访问vector中的元素时，可以通过迭代器来访问或修改元素，也可以通过 [] 或 at() 成员函数访问或修改元素。 123456789101112131415int main() &#123; int p[] = &#123;2, 3, 5, 7, 11&#125;; vector&lt;int&gt; v(p, p + 5); vector&lt;int&gt;::iterator it; //迭代器 for (it = v.begin(); it != v.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; *it = *it * 2 + 1; &#125; // 输出 2 3 5 7 11 v[3] -= 2; // 修改 v[3] v.at(4) = 17; // 修改 v[4] for (int i = 0; i &lt; v.size(); ++i) &#123; cout &lt;&lt; v[i] &lt;&lt; \" \"; &#125; // 输出 5 7 11 13 17 return 0;&#125; vector 在数组尾部增删元素较快，使用 push_back() 在尾部增加元素，使用 pop_back() 在尾部删除元素。 可以使用 insert() 在任意位置插入一个或一段元素，但效率较低，插入的位置由迭代器来表示；erase() 则可以删除一个或一段元素，用法与 insert() 类似，效率也低。 1234567891011121314int main() &#123; vector&lt;char&gt; v1, v2(3, 'D'); for (int i = 0; i &lt; 4; ++i) &#123; v1.push_back(i + 'A'); &#125; // 依次加入 A B C D v1.pop_back(); // 删除 D v1.insert(v1.begin() + 2, 'Z'); v1.insert(v1.begin(), v2.begin(), v2.end()); vector&lt;char&gt;::iterator it; for (it = v1.begin(); it != v1.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 D D D A B Z C return 0;&#125; 可以使用 resize() 来修改数组的长度，若修改后的长度比现有的要小，则舍弃后面的元素，否则添加缺省元素。 123456789101112131415int main() &#123; vector&lt;int&gt; v; for (int i = 0; i &lt; 10; ++i) &#123; v.push_back(i); &#125; // 依次添加 1 至 10 v.resize(5); // 留下 v[0..4] // 将长度扩展到 8，用 100 填充新元素 v.resize(8, 100); // 将长度扩展到 12，用缺省值 0 填充新元素 v.resize(12); for (int i = 0; i &lt; v.size(); ++i) &#123; cout &lt;&lt; v[i] &lt;&lt; \" \"; &#125; // 输出 0 1 2 3 4 100 100 100 0 0 0 0 return 0;&#125; 1.2. dequedeque 实现了一个双端队列，通过动态数组来实现，可以快速地在数组的头尾两端增删元素，支持随机存储迭代器，可以在常数时间内完成随机存取元素。 所有适用于 vector 的操作都适用于 deque，除此之外，deque 还提供了这些成员函数： front() 取第一个元素（队首）的值 back() 取最后一个元素（队尾）的值 push_front() 将元素插入到最前面 pop_front() 删除第一个元素 1.3. listlist 实现了一个双向链表，不支持随机存取，支持双向迭代器，与 vector 相比，不支持用 [] 来访问任意位置的元素，也不支持将迭代器移动多个单位（即不支持 begin() + i 这样的操作），但支持向前或向后移动（即支持 ++ 和 — ），可以在末尾或开头快速增加或删除元素，在任何位置增删元素都能在常数时间完成。 1234567891011121314int main() &#123; int p[] = &#123;2, 3, 5, 7&#125;; list&lt;int&gt; li(p + 1, p + 4); list&lt;int&gt;::iterator it; for (it = li.begin(); it != li.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 3 5 7 li.pop_back(); li.push_back(11); li.pop_front(); li.push_front(2); for (it = li.begin(); it != li.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 2 5 11 return 0;&#125; list 的构造函数与 vector 类似，可以指定一个初始长度，还可以分别指定初始长度和元素的初始值，也可以根据一个已有的数组来构造 list 对象。与 vector 类似，可以使用 insert() /erase() 在任意位置插入/删除一个或一段元素，且效率较高；与 vector 类似，可以使用 resize() 修改长度。 可以使用成员函数 reverse() 将整个链表反序。 12345678910111213141516#include &lt;list&gt;#include &lt;iostream&gt;using namespace std;int main() &#123; list&lt;int&gt; li; list&lt;int&gt;::iterator it; for (int i = 0; i &lt; 7; ++i) li.push_back(i); for (it = li.begin(); it != li.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 0 1 2 3 4 5 6 li.reverse(); for (it = li.begin(); it != li.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 6 5 4 3 2 1 0 return 0;&#125; 可以使用成员函数 remove() 删除指定值的元素。 12345678910111213141516#include &lt;list&gt;#include &lt;iostream&gt;using namespace std;int main() &#123; int p[] = &#123;2, 3, 3, 6, 6, 6&#125;; list&lt;int&gt; li(p, p + 6); list&lt;int&gt;::iterator it; for (it = li.begin(); it != li.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 2 3 3 6 6 6 li.remove(3); // 删除链表中所有的 3 for (it = li.begin(); it != li.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 2 6 6 6 return 0;&#125; 可以使用成员函数unique() 删除所有和前一个元素相同的元素，注意删除时只与前一个元素比较是否相同，所以最后得到的链表里仍然可能有重复的元素。还可以通过函数自定义元素相同的含义，将函数值返回 true 时，就删除当前元素 12345678910111213#include &lt;list&gt;#include &lt;iostream&gt;using namespace std;int main() &#123; double p[] = &#123;0.1, 0.1, 0.2, 0.3, 0.1, 0.8, 1.3&#125;; list&lt;double&gt; li(p, p + 7); list&lt;double&gt;::iterator it; li.unique(); for (it = li.begin(); it != li.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 0.1 0.2 0.3 0.1 0.8 1.3 return 0;&#125; 1234567891011121314bool is_near(double d1, double d2) &#123; return fabs(d1 - d2) &lt; 0.5; // 相差不超过 0.5 就认为是相同的元素&#125;int main() &#123; double p[] = &#123;0.1, 0.1, 0.2, 0.3, 0.1, 0.8, 1.3&#125;; list&lt;double&gt; li(p, p + 7); list&lt;double&gt;::iterator it; li.unique(is_near); for (it = li.begin(); it != li.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 0.1 0.8 1.3 return 0;&#125; 由于 list 不支持随机存取，因此无法使用STL中的 sort 算法模板进行排序，只能使用 list 自己的成员函数sort() 来排序，缺省按照 operator &lt; 的含义来排序，可以重载 &lt; 操作符。也可以传入函数指针，通过自定义的函数来重新定义元素的大小。 1234567891011121314bool cmp(int a, int b) &#123; return a % 10 &lt; b % 10; &#125;int main() &#123; int p[] = &#123;2, 11, 27, 9&#125;; list&lt;int&gt; li(p, p + 4); list&lt;int&gt;::iterator it; li.sort(); for (it = li.begin(); it != li.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 按大小排序，输出 2 9 11 27 li.sort(cmp); for (it = li.begin(); it != li.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 按 % 10 之后的大小排序，输出 11 2 27 9 return 0;&#125; 可以使用成员函数 merge() 合并两个链表，并清空被合并的那个链表。合并的两个链表中的元素必须按照某种规则排好序，否则会出错，因此 merge() 经常配合 sort() 一起使用，合并后的链表也是排好序的，可以自定义排序规则。 123456789101112int main() &#123; int p[] = &#123;2, 3, 5, 7, 11&#125;; list&lt;int&gt; li1(p, p + 4); // 2 3 5 7 list&lt;int&gt; li2(p + 2, p + 5); // 5 7 11 list&lt;int&gt;::iterator it; li1.merge(li2); // li1 和 li2 都是按升序排列好的 for (it = li1.begin(); it != li1.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 2 3 5 5 7 7 11 cout &lt;&lt; li2.size(); // 输出 0 return 0;&#125; 可以使用成员函数 splice() 在指定位置前面插入另一链表中的一个或多个元素，并在另一链表中删除这些被插入的元素。 123456789101112int main() &#123; int p[] = &#123;2, 3, 5, 7, 11&#125;; list&lt;int&gt; li1(p, p + 5), li2(3, 1); list&lt;int&gt;::iterator it = li1.begin(); for (int i = 0; i &lt; 3; ++i) ++it; // 找到 li1 中的 7 li2.splice(li2.begin(), li1, it); // 在 li2 开头插入 7 for (it = li2.begin(); it != li2.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 7 1 1 1 cout &lt;&lt; li1.size(); // li1 中的 7 被删除，输出 4 return 0;&#125; 总结：常用的这三种顺序容器的主要差别在于访问/修改元素与增加/删除元素的速度上，因此需要根据实际情况选用合适的容器。 2.关联容器在关联容器中，元素的插入位置与元素的值有关，必须按相应的排序准则来确定，在查找元素时具有非常好的性能: set / multiset （声明于 #include&lt;set&gt;） 集合：实现了一棵平衡二叉搜索树，使用元素本身作为键值（key）；set 容器中不允许存在相同元素，multiset 容器中允许存在相同的元素 map / multimap （声明于 #include&lt;map&gt;）映射表：实现了一棵平衡二叉搜索树，存放的是成对的键值和数据（key / value），并根据键值对元素进行排序，可快速地根据键值来检索元素；map 容器中不允许存在键值相同的元素，而 multimap 容器中则允许 C++11 中新增的 unordered_set / unordered_multiset （声明于 &lt; unordered_set&gt;）和 unordered_map / unordered_multimap （声明于 &lt; unordered_map&gt;）映射表：通过哈希表实现，功能与 set/multiset 和 map/multimap 相似，但不对键值排序 键值与数据（key-value pair）：在关联容器中，通过键值来查询对应的数据，对于集合容器来说，键值与数据相等。 pair：可以使用 pair 模板来生成 key-value 对，pair 声明于 &lt; utility&gt; 头文件中，pair 中有两个成员变量 first 和 second：first 表示 key，second 表示 value。实例化的 pair 模板类可以用来绑定两个对象为一个新的对象。 比较两个 pair 对象的大小： p1 &lt; p2：如果 p1.first &lt; p2.first 或者 !(p2.first &lt; p1.first) &amp;&amp; p1.second &lt; p2.second，则返回 true p1 == p2：如果 p1.first == p2.first &amp;&amp; p1.second == p2.second，则返回 true 声明一个 pair 对象： pair&lt;T1, T2&gt; p：创建一个空的 pair 对象，它的两个元素分别是 T1 和 T2 类型，采用值初始化 pair&lt;T1, T2&gt; p(v1, v2)：创建一个 pair 对象，它的两个元素分别是 T1 和 T2 类型，其中 first 成员初始化为 v1，second 成员初始化为 v2 make_pair(v1, v2)：以 v1 和 v2 值创建一个新的 pair 对象，其元素类型分别是 v1 和 v2 的类型 2.1. set/multisetset / multiset 是集合容器，实现了一棵平衡二叉搜索树，使用元素本身作为键值（key）。set 容器中不允许存在相同元素，而 multiset 容器中允许内部元素有序排列，新元素插入的位置取决于其值。 支持双向迭代器，不支持 [] 下标访问元素，但可以通过 find() 等成员函数快速查找元素，增加、删除、修改、查询元素的算法时间复杂度均为$O( \\log n)$。可根据 operator &lt; 来比较两个元素的大小，对于自定义的类，需要重载其 &lt; 操作符。 2.1.1. set可使用成员函数 insert() 向集合中增加元素，根据元素对应的 operator &lt; 来确定元素应处的位置，插入容器中已有的元素时，插入操作失败（即不会插入重复的元素）。 1234567891011121314int main() &#123; set&lt;int&gt; s; set&lt;int&gt;::iterator it; for (int i = 0; i &lt; 5; ++i) s.insert(i); for (it = s.begin(); it != s.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 0 1 2 3 4 for (int i = 0; i &lt; 5; ++i) &#123; s.insert(i * 2 + 1); // 想要插入 1 3 5 7 9 &#125; // 但是已经存在 1 和 3，不会重复插入 for (it = s.begin(); it != s.end(); ++it) &#123; cout &lt;&lt; *it &lt;&lt; \" \"; &#125; // 输出 0 1 2 3 4 5 7 9 return 0;&#125; 对于自定义的类，需要重载 operator &lt; 1234567891011121314151617181920class Customer &#123; int age; string name;public: Customer(int _a, string _n) : age(_a), name(_n) &#123; &#125; bool operator &lt; (const Customer &amp;c) const &#123; if (age == c.age) return name &lt; c.name; return age &lt; c.age; &#125; // 若 age 相同则比较 name，否则比较 age 的大小 void print() const &#123; cout &lt;&lt; name &lt;&lt; \" \" &lt;&lt; age &lt;&lt; endl; &#125;&#125;;int main() &#123; set&lt;Customer&gt; s; Customer c1(18, \"Alice\"); s.insert(c1); Customer c2(37, \"Bob\"); s.insert(c2); Customer c3(26, \"Daisy\"); s.insert(c3); set&lt;Customer&gt;::iterator it; for (it = s.begin(); it != s.end(); ++it) it-&gt;print(); // 依次输出 Alice 18 、Daisy 26 和 Bob 37 return 0;&#125; 几种查找元素的接口： 使用成员函数 find() 来查找容器中的某个元素 x，若能找到，则返回指向 x 的迭代器，否则返回 end()，在这里，两个元素的相等是根据 operator &lt; 判断的（即同时满足 x &lt; y 为假且 x &gt; y 为假），与 operator == 运算符无关。 使用成员函数 lower_bound() 来查找容器中的某个元素 x，返回第一个大于等于 x 的元素的迭代器，若找不到则返回 end()。 使用成员函数 upper_bound() 来查找容器中的某个元素 x，返回第一个大于 x 的元素的迭代器，若找不到则返回 end()。 使用成员函数 equal_range() 来查找容器中的某个元素 x，返回一个迭代器对（pair），其 first 值就是 lower_bound() ，其 second 值就是 upper_bound()。 使用成员函数 count() 来计算等于某个值的元素个数。 注意通过迭代器访问元素时，先判断迭代器是否等于 end()。 1234567891011121314151617#include &lt;set&gt;#include &lt;iostream&gt;using namespace std;void disp(const set&lt;int&gt; &amp;s, set&lt;int&gt;::iterator it) &#123; if (it == s.end()) cout &lt;&lt; \"Not found.\" &lt;&lt; endl; else cout &lt;&lt; \"Find Element \" &lt;&lt; *it &lt;&lt; endl;&#125;int main() &#123; int f[] = &#123;2, 3, 5, 8, 13, 21&#125;; set&lt;int&gt; s; s.insert(f, f + 6); disp(s, s.find(3)); // 输出 Find Element 3 disp(s, s.find(4)); // 输出 Not found. disp(s, s.lower_bound(8)); // 输出 Find Element 8 disp(s, s.upper_bound(8)); // 输出 Find Element 13 disp(s, s.lower_bound(20)); // 输出 Find Element 21 return 0;&#125; 使用成员函数 erase() 来删除某个元素，传入的参数可以是元素值，若该元素不存在容器中，则删除失败；也可以是指向容器中某个元素的迭代器，迭代器必须是有效的；还可以传入两个迭代器，表示一段左闭右开的区间。 erase()与 find() 、lower_bound()、upper_bound() 以及 begin()、end()、rbegin()、rend() 等配合使用，可以达到一些常见的效果，使用时注意 STL 中的区间一般都是左闭右开的，例如 [ begin(), end() )以及 lower_bound() 与 upper_bound() 在相等元素上的处理区别。 如下程序，假设普通函数 disp(set&lt; int&gt;) 用于输出容器中的所有元素 1234567891011121314151617int main() &#123; int f[] = &#123;2, 3, 5, 8, 13, 21&#125;; set&lt;int&gt; s; s.insert(f, f + 6); disp(s); // 输出 2 3 5 8 13 21 s.erase(8); // 删除值为 8 的元素 disp(s); // 输出 2 3 5 13 21 s.erase(s.begin()); // 删除第一个元素（2） disp(s); // 输出 3 5 13 21 set&lt;int&gt;::iterator p; p = s.lower_bound(5); // 找到 5 s.erase(s.begin(), p); // 删除 3 disp(s); // 输出 5 13 21 p = s.upper_bound(13); // 找到 13 s.erase(s.begin(), p); // 删除 5 和 13 disp(s); // 输出 21 return 0;&#125; 2.1.2. multisetmultiset 在使用上与 set 基本一致，主要区别在于 multiset 允许插入值相同的元素。在使用 find() 查找时，会返回第一个等于该查找值的元素，在使用 erase() 删除等于某个值的元素时，会删除所有等于该值的元素假设 multiset&lt; int&gt; s 中的元素如下： 如下程序，假设普通函数 disp() 用于输出容器set&lt; int&gt;和multiset&lt; int&gt;中的所有元素。 1234567891011121314151617int main() &#123; int f[] = &#123;1, 2, 3, 1, 1, 2, 3, 6&#125;; set&lt;int&gt; s1; s1.insert(f, f + 8); disp(s1); // 输出 1 2 3 6 multiset&lt;int&gt; s2; s2.insert(f, f + 8); disp(s2); // 输出 1 1 1 2 2 3 3 6 s2.erase(2); // 删除所有的 2 disp(s2); // 输出 1 1 1 3 3 6 multiset&lt;int&gt;::iterator it = s2.find(3); s2.erase(it); // 删除第一个 3 disp(s2); // 输出 1 1 1 3 6 // equal_range(1) 找到所有的 1 s2.erase((s2.equal_range(1)).first, (s2.equal_range(1)).second); disp(s2); // 输出 3 6 return 0;&#125; 示例：s1 是一个不允许重复元素的集合，且元素按降序排列，因此输出 9 7 5 3 1；s2 是一个允许有重复元素的容器，缺省将元素按升序排列，因此输出 1 3 3 5 7 9。 12345678910111213141516int main() &#123; int p[] = &#123;9, 1, 3, 5, 7, 3&#125;; set&lt;int, greater&lt;int&gt; &gt; s1; set&lt;int, greater&lt;int&gt; &gt;::iterator i1; multiset&lt;int&gt; s2; multiset&lt;int&gt;::iterator i2; s1.insert(p, p + 5); for (i1 = s1.begin(); i1 != s1.end(); i1++) &#123; cout &lt;&lt; *i1 &lt;&lt; \" \"; &#125; cout &lt;&lt; endl; s2.insert(p, p + 6); for (i2 = s2.begin(); i2 != s2.end(); ++i2) &#123; cout &lt;&lt; *i2 &lt;&lt; \" \"; &#125; cout &lt;&lt; endl; return 0;&#125; 2.2. map/multimapmap / multimap 是一张映射表，通过平衡二叉搜索树来实现，存放的是成对的 key-value，实例化时需要在 &lt;&gt; 中依次指定 key 和 value 的类型，根据 key 对元素进行排序，可快速地根据 key 来检索元素。map 容器中不允许存在 key 相同的元素，而 multimap 容器中则允许，用法与 set / multiset 类似。 支持双向迭代器，可以通过 find() 等成员函数快速查找元素，增加、删除、修改、查询元素的算法时间复杂度均为 $O(\\log n)$。根据 operator &lt; 来比较两个元素的大小，对于自定义的类，需要重载其 &lt; 操作符。 2.2.1. map可以使用成员函数 insert() 向 map 容器中插入元素，注意插入的元素是成对的 key-value，因此是一个个的 pair。可以通过迭代器访问元素，也可以使用 [] 运算符访问容器中已有的 key 对应的元素。 123456789101112int main() &#123; map&lt;string, int&gt; m; m.insert(make_pair(\"Alice\", 18)); m.insert(make_pair(\"Daisy\", 26)); m.insert(make_pair(\"Bob\", 37)); map&lt;string, int&gt;::iterator it; for (it = m.begin(); it != m.end(); ++it) &#123; cout &lt;&lt; it-&gt;first &lt;&lt; \"-&gt;\" &lt;&lt; it-&gt;second &lt;&lt; endl; &#125; // 依次输出 Alice-&gt;18 Bob-&gt;37 Daisy-&gt;26 int p = m[\"Alice\"]; // p = 18 return 0;&#125; 旧一些的编译器只支持通过 make_pair() 的方式插入元素，新一些的编译器则可以直接通过 [] 来插入元素，如 m[“Alice”] = 18; 。因此当使用 [] 访问元素时，如果 [] 中的 key 不存在于容器中，则可能运行出错，也可能访问到一个未正确初始化的元素（其值不一定有意义）。当向容器中插入一个 key 已经存在的元素时，插入失败，但是可以通过 [] 来修改 key 对应的 value。 使用成员函数 erase() 删除元素，可以传入 key 作为参数，也可以传入指定元素的迭代器；还可以传入两个迭代器，表示删除一个左闭右开的区间内的所有元素。 使用成员函数 find()、lower_bound()、upper_bound() 或者 begin() 、rbegin() 等时，返回的都是 pair 迭代器，因为返回值对应于容器中的元素，而 map 容器中的元素是一个个 pair。 成员函数 equal_range() 的返回值则是一个由两个 pair 组成的 pair 迭代器，例如对于 map&lt;string, int&gt; 容器来说，equal_range() 的返回值类型为 pair&lt;map&lt;string, int&gt;::iterator, map&lt;string, int&gt;::iterator&gt;。因为 equal_range() 返回的是 lower_bound() 和 upper_bound() 构成的 pair，而 map 的 lower_bound() 和 upper_bound() 都是 pair 迭代器。 2.2.2. multimapmultimap 在使用上与 map 基本一致，主要区别在于 multimap 允许插入 key 相同的元素。multimap 不支持使用 [] 运算符访问元素，因为元素的 key 可能重复，用 [] 访问时可能不知道应该访问哪一个。 12345678910111213int main() &#123; map&lt;char, int&gt; m1; multimap&lt;char, int&gt; m2; for (int i = 0; i &lt; 4; ++i) &#123; m1.insert(make_pair(i + 'A', i)); m1.insert(make_pair(i + 'A', i * 2 + 1)); m2.insert(make_pair(i + 'A', i)); m2.insert(make_pair(i + 'A', i * 2 + 1)); &#125; disp(m1); // 输出 A-&gt;0 B-&gt;1 C-&gt;2 D-&gt;3 disp(m2); // 输出 A-&gt;0 A-&gt;1 B-&gt;1 B-&gt;3 C-&gt;2 C-&gt;5 D-&gt;3 D-&gt;7 return 0;&#125; 3.容器适配器在容器适配器中，不提供真正的用于存储元素的数据结构实现，不支持迭代器，由使用者选择合适的底层数据结构： stack （声明于 #include&lt;stack&gt;）栈：是项的有限序列，并满足序列中被删除、检索和修改的项只能是最近插入序列的项，即按照后进先出的原则 queue （声明于 #include&lt;queue&gt;）队列：插入只可以在尾部进行，删除、检索和修改只允许从头部进行，即按照先进先出的原则 priority_queue （声明于 #include&lt;queue&gt;）优先队列：最高优先级元素总是第一个出队，可视作堆 容器适配器可以用某种顺序容器来实现，即让已有的顺序容器以栈/队列的方式工作。 3.1. stackstack 栈是一种后进先出的数据结构，只能插入、删除、访问栈顶元素。对应的操作为： push() 入栈 / 插入元素 pop() 出栈 / 删除元素 top() 返回栈顶元素的引用 / 访问元素 此外还可以通过成员函数 empty() 来判断栈是否为空，如果想要清空一个栈，只能通过 while (!empty()) pop(); 这样的方式。如下图依次演示了：1 入栈、2 入栈、3 入栈、3 出栈、1 入栈、4 入栈。 stack 在缺省情况下用 deque 实现：template&lt;class T, class Cont = deque&lt;T&gt; &gt;，例如 stack&lt;int&gt; s; 声明了一个元素为 int 型的栈，用 deque 实现。但也可以用 vector 或 list 来实现，例如 stack&lt;string, vector&lt;string&gt; &gt; s; 声明了一个元素为 string 型的栈，用 vector 实现。在声明时可以同时对栈初始化，初始化方式与 vector / deque / list 类似。 3.2. queuequeue 队列是一种先进先出的数据结构，只能在队尾插入元素，只能访问或删除队首元素，对应的操作为： push() 入队 / 插入元素 pop() 出队 / 删除元素 front() 返回队首元素的引用 / 访问元素 queue 在缺省情况下用 deque 实现：template&lt;class T, class Cont = deque&lt;T&gt; &gt;。例如 queue&lt;int&gt; q; 声明了一个元素为 int 型的队列，用 deque 实现。但也可以用 vector 或 list 来实现，例如 queue&lt;string, vector&lt;string&gt; &gt; q; 声明了一个元素为 string 型的队列，用 vector 实现。在声明时可以同时对队列初始化，初始化方式与 vector / deque / list 类似。 3.3. priority_queuepriority_queue 是一个优先队列，也可以看做是一个堆（heap），能保证最大的元素总是在最前面，一些操作为： pop() 删除的是队列中最大的元素（因为最大的元素总是在队首） 没有名叫 front() 成员函数，通过 top() 返回最大元素的引用 通过 push() 添加元素，每次 push() 之后队首元素可能发生变化 priority_queue用法基本和 queue 类似，可以用 vector 和 deque 实现，缺省情况下用vector 实现。缺省情况是根据 less&lt;T&gt; 模板比较大小的，实例化时是个函数对象。可以使用其他的函数对象类模板来定义大小，或者重载元素的 &lt; 运算符，例如 priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt; &gt; 就声明了一个将最小的元素排在队首的优先队列。 例如下面声明了三种队列，设 disp() 可以按序输出队列中的元素。 12345678910111213141516int main() &#123; int v[] = &#123;1, 3, 7, 2, 6, 4, 5&#125;; queue&lt;int&gt; q1; // 普通的队列 priority_queue&lt;int&gt; q2; // 优先队列，降序 // 优先队列，升序 priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt; &gt; q3; for (int i = 0; i &lt; 7; ++i) &#123; q1.push(v[i]); q2.push(v[i]); q3.push(v[i]); &#125; // 依次入队 1 3 7 2 6 4 5 disp(q1); // 输出 1 3 7 2 6 4 5 disp(q2); // 输出 7 6 5 4 3 2 1 disp(q3); // 输出 1 2 3 4 5 6 7 return 0;&#125; 这一章中各容器在数据结构与算法中有详细的介绍，包括原理，实现，效率与优化等等，可以参考我的博客中数据结构与算法categories中的相关文章(┌・ω・)┌✧","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"数据结构与算法（13）二叉搜索树","slug":"数据结构与算法（13）二叉搜索树","date":"2020-03-01T09:53:30.000Z","updated":"2020-03-03T14:50:58.915Z","comments":true,"path":"2020/03/01/数据结构与算法（13）二叉搜索树/","link":"","permalink":"http://nekomoon404.github.io/2020/03/01/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8813%EF%BC%89%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/","excerpt":"","text":"1.概述回顾在第二，三章作为基础所介绍的两类基本数据结构：向量和列表，虽然它们都已经可以解决相当多的问题，然而对于进一步的算法设计要求来说它们都显得力不从心。基本的数据结构，可以在一定程度上支持静态查找 但很遗憾，并不能高效地兼顾静态查找与动态修改。 在第五章我们做了改进的尝试，通过对一维列表的拓展引入了树结构或者说二叉树结构，二叉树结构可以认为是二维的列表，即列表在维度上的一种扩充。 而这一章要介绍的二叉搜索树（binary search tree，BST），它首先在形式上继承了二叉树也就是列表结构的特点，同时也巧妙地借鉴了向量，或者更准确地讲是有序向量的特点和优势。相对而言后一方面的借鉴更为重要，如果此前对列表结构的借鉴只是外表的形式，那么这种对有序向量特点的借鉴才是一种质的提高，这也是BST相对于其它的数据结构最为 “传神”的部分。实际上BST中所有这些传神的部分都集中体现在其中的一个子集：平衡二叉搜索树（balanced binary search tree，BBST），在接下来的两章中会介绍其中最具代表性的几个变种。 1.1.循关键码访问二叉搜索树的数据项之间，按照各自的关键码彼此区分（call-by-key）。 前提条件是：关键码之间支持大小比较与相等比对。 因此为了简化和抽象不妨将整个数据集中的数据项都已统一地表示和实现为词条entry的形式： 123456789template &lt;typename K, typename V&gt; struct Entry &#123; //词条模板类 K key; V value; //关键码、数值 Entry ( K k = K(), V v = V() ) : key ( k ), value ( v ) &#123;&#125;; //默认构造函数 Entry ( Entry&lt;K, V&gt; const&amp; e ) : key ( e.key ), value ( e.value ) &#123;&#125;; //基于克隆的构造函数 bool operator&lt; ( Entry&lt;K, V&gt; const&amp; e ) &#123; return key &lt; e.key; &#125; //比较器：小于 bool operator&gt; ( Entry&lt;K, V&gt; const&amp; e ) &#123; return key &gt; e.key; &#125; //比较器：大于 bool operator== ( Entry&lt;K, V&gt; const&amp; e ) &#123; return key == e.key; &#125; //判等器：等于 bool operator!= ( Entry&lt;K, V&gt; const&amp; e ) &#123; return key != e.key; &#125; //判等器：不等于&#125;; //得益于比较器和判等器，从此往后，不必严格区分词条及其对应的关键码 若二叉树中各节点所对应的词条之间支持大小比较，则在不致歧义的情况下，我们可以不必严格区分树中的节点、节点对应的词条以及词条内部所存的关键码。 1.2.顺序性在二叉搜索树BST中处处中都满足顺序性： 任一节点r的左子树中的所有节点都不大于r； 任一节点r的右子树中的所有节点都不小于r。 为回避边界情况，这里暂且假设所有节点互不相等，于是上述顺序性便可简化表达为： 任一节点r的左子树中的所有节点都小于r； 任一节点r的右子树中的所有节点都大于r。 当然在实际应用中，对相等元素的禁止既不自然也不必要。顺序性虽然只是对局部特征的刻画，但可由此导出某种全局特征：BST的中序遍历序列，必然单调非降。这一性质也是BST的充要条件，即任何一颗二叉树是二叉搜索树当且仅当中序遍历序列单调非降。（二叉树图中往下垂直投影就可以得到中序遍历序列） 二叉搜索树属于二叉树的特例，故可以基于BinTree模板类派生出BST模板类： 12345678910111213141516171819#include \"BinTree/BinTree.h\" //引入BinTreetemplate &lt;typename T&gt; class BST : public BinTree&lt;T&gt; &#123; //由BinTree派生BST模板类protected: BinNodePosi(T) _hot; //“命中”节点的父亲 BinNodePosi(T) connect34 ( //按照“3 + 4”结构，联接3个节点及四棵子树 BinNodePosi(T), BinNodePosi(T), BinNodePosi(T), BinNodePosi(T), BinNodePosi(T), BinNodePosi(T), BinNodePosi(T) ); BinNodePosi(T) rotateAt ( BinNodePosi(T) x ); //对x及其父亲、祖父做统一旋转调整public: //基本接口：以virtual修饰，强制要求所有派生类（BST变种）根据各自的规则对其重写 virtual BinNodePosi(T) &amp; search ( const T&amp; e ); //查找 virtual BinNodePosi(T) insert ( const T&amp; e ); //插入 virtual bool remove ( const T&amp; e ); //删除 /*DSA*/ /*DSA*/void stretchToLPath() &#123; stretchByZag ( _root ); &#125; //借助zag旋转，转化为左向单链 /*DSA*/void stretchToRPath() &#123; stretchByZig ( _root ); &#125; //借助zig旋转，转化为右向单链 /*DSA*/void stretch(); /*DSA*/void imitate ( const BST&lt;T&gt; &amp; ); //临摹&#125;; 在继承原模板类BinTree的同时，BST内部也继续沿用了二叉树节点模板类BinNode。BST中新增了三个接口search()，insert()，和remove()，这三个标准接口的调用参数都是属于元素类型T的对象引用，这正是此类结构“循关键码访问”方式的具体体现。以BST为基类可以进一步派生出二叉搜索树的多个变种，因此这里的三个接口在前面加上virtual定义为虚函数。由于这些操作接口涉及词条的大小和相等的关系，因此这里假定基本元素类型T直接支持比较和判等操作，或者已重载过对应的操作符。 2.算法及实现2.1. BST：查找二叉搜索树的查找算法，亦采用了减而治之的思路与策略，其执行过程可描述为：从树根出发，逐步地缩小查找范围，知道发现目标（成功）或缩小至空树（失败）。 一般地，在上述查找过程中，一旦发现当前节点为NULL，即说明查找范围已经缩小至空，查找失败；否则，视关键码比较结果，向左（更小）或向右（更大）深入，或者报告成功。对照中序遍历序列可见，整个过程与有序向量的二分查找过程可视为等效。 BST的search()可以实现为下面一段代码： 123456789template &lt;typename T&gt; BinNodePosi(T) &amp; BST&lt;T&gt;::search ( const T&amp; e ) //在BST中查找关键码e&#123; return searchIn ( _root, e, _hot = NULL ); &#125; //返回目标节点位置的引用，以便后续插入、删除操作template &lt;typename T&gt; //在以v为根的（AVL、SPLAY、rbTree等）BST子树中查找关键码estatic BinNodePosi(T) &amp; searchIn ( BinNodePosi(T) &amp; v, const T&amp; e, BinNodePosi(T) &amp; hot ) &#123; if ( !v || ( e == v-&gt;data ) ) return v; //递归基：在节点v（或假想的通配节点）处命中 hot = v; //一般情况：先记下当前节点，然后再 return searchIn ( ( ( e &lt; v-&gt;data ) ? v-&gt;lc : v-&gt;rc ), e, hot ); //深入一层，递归查找&#125; //返回时，返回值指向命中节点（或假想的通配哨兵），hot指向其父亲（退化时为初始值NULL） 节点插入和删除操作，都需要首先调用查找算法，并根据查找结果确定后续的处理方式。因此这里以引用方式传递（子）树根节点，以为后续操作提供必要的信息。searchIn()采用典型的尾递归来实现。这种实现方式是为了统一并简化后续不同搜索树的各种操作接口的实现，其中的技巧体现于返回值和hot变量的语义约定。 若查找成功，则searchIn()和search()的返回值都指向一个关键码为e且真实存在的节点；若查找失败，则返回值的数值为NULL，但它作为引用将指向最后一次试图转向的空节点。对于后一种情况不妨将此空节点转换为一个数值为e的哨兵节点，这样无论成功与否，查找的返回值总是等效地指向“命中节点”。在整个查找的过程中，hot变量始终指向当前节点的父亲，因此在算法返回时，按照如上定义，_hot将统一指向“命中节点”的父亲。而_hot节点是否拥有另一个孩子与查找成功与否无关。 接着分析查找算法的效率，在二叉搜索树的每一层，查找算法至多访问一个节点，且只需常数时间，故中体所需时间应线性正比与查找路径的长度，或最终返回节点的深度。在最好情况下，目标关键码出现在树根处，只需$O(1)$时间；而规模为n的二叉搜索树，深度在最坏情况下可达$O(n)$，如退化为一条单链，可见最坏情况下查找所需时间为$O(n)$。因此若要控制单词查找在最坏情况下的运行时间，须从控制二叉树的高度入手，后面介绍的平衡二叉树正是基于这一思路而做的改进。 2.2. BST：插入为了在二叉搜索树中插入一个节点，首先要利用search(e)确定插入位置及方向（左孩子还是右孩子），再将新节点作为叶子插入。若e尚不存在，则_hot为新节点的父亲，v=search(e)为 _hot对新孩子的引用。于是只需令 _hot通过v指向新节点。 如图(a)中的二叉搜索树为例，若欲插入关键码40，则在执行search(40)之后，如图(b)，_hot将指向比较过的最后一个节点46，同时返回其左孩子（此时为空）的位置，于是接下来如图(c)，只需将其作为46的左孩子接入。为了保持二叉搜索树作为数据结构的完整性和一致性，还需从节点 _hot(46)出发，自底而上地逐个更新新插入的节点40的历代祖先的高度。 BST的插入算法的代码实现： 1234567template &lt;typename T&gt; BinNodePosi(T) BST&lt;T&gt;::insert ( const T&amp; e ) &#123; //将关键码e插入BST树中 BinNodePosi(T) &amp; x = search ( e ); if ( x ) return x; //确认目标不存在（留意对_hot的设置） x = new BinNode&lt;T&gt; ( e, _hot ); //创建新节点x：以e为关键码，以_hot为父 _size++; //更新全树规模 updateHeightAbove ( x ); //更新x及其历代祖先的高度 return x; //新插入的节点，必为叶子&#125; //无论e是否存在于原树中，返回时总有x-&gt;data == e 按照以上的实现方式，无论插入操作成功与否，都会返回一个非空位置，且该处的节点与拟插入的节点相等，如此可以确保一致性，以简化后续的操作。节点插入操作所需的时间主要消耗于对算法search()和updateHeightAbove()的调用。后者与前者一样都是在每一层至多涉及一个节点，仅消耗$O(1)$时间，故时间复杂度取决于新节点的深度，在最坏情况下不会超过全树的高度$O(h)$。 2.3. BST：删除为从二叉搜索树中删除节点，首先也要调用search()，判断目标节点是否存在与树中，若存在，则需返回其位置。具体的删除操作要分两种情况来讨论： 单分支情况： 如图(a)欲删除节点69，首先通过search(69)定外待删除节点69，该节点的右子树为空，故只需将69与替换为其左孩子64。为保持二叉搜索树作为数据结构的完整性和一致性，还需要更新全树的规模纪录，释放被摘除的节点69，并自下而上地逐个更新节点64历代祖先的高度。注意，首个需要更新高度的祖先58恰好由_hot指示。以上同时已涵盖左、右孩子均不存在（即目标节点为叶节点）的情况。如此操作之后，二叉搜索树的拓扑结构依然完整，顺序性依然满足。 双分支情况： 以上图的二叉树为例，若欲删除节点36，则如图(c)首先调用BinNode::succ()算法找到该节点的直接后继(40)，即中序遍历下的下一个节点，然后交换二者的数据项，则可将后继节点等效地视作待删除的目标节点。而该后继节点必然无左孩子，从而问题转化为上面的单分支情况，于是如图(d)将新的目标节点(36)替换为其右孩子(46)，再释放被摘除的节点(36)，并更新一系列祖先节点的高度，此时首个需要更新高度的祖先，依然恰好由_hot指示，如图(e)。尽管全树的顺序性在中途会不满足，但完成整个删除操作后，全树的顺序性必然恢复。 BST的删除算法的代码实现： 123456789101112131415161718192021222324252627282930template &lt;typename T&gt; bool BST&lt;T&gt;::remove ( const T&amp; e ) &#123; //从BST树中删除关键码e BinNodePosi(T) &amp; x = search ( e ); if ( !x ) return false; //确认目标存在（留意_hot的设置） removeAt ( x, _hot ); _size--; //实施删除 updateHeightAbove ( _hot ); //更新_hot及其历代祖先的高度 return true;&#125; //删除成功与否，由返回值指示/********************************************************************************** * BST节点删除算法：删除位置x所指的节点（全局静态模板函数，适用于AVL、Splay、RedBlack等各种BST） * 目标x在此前经查找定位，并确认非NULL，故必删除成功；与searchIn不同，调用之前不必将hot置空 * 返回值指向实际被删除节点的接替者，hot指向实际被删除节点的父亲——二者均有可能是NULL *********************************************************************************/template &lt;typename T&gt;static BinNodePosi(T) removeAt ( BinNodePosi(T) &amp; x, BinNodePosi(T) &amp; hot ) &#123; BinNodePosi(T) w = x; //实际被摘除的节点，初值同x BinNodePosi(T) succ = NULL; //实际被删除节点的接替者 if ( !HasLChild ( *x ) ) //若*x的左子树为空，则可 succ = x = x-&gt;rc; //直接将*x替换为其右子树 else if ( !HasRChild ( *x ) ) //若右子树为空，则可 succ = x = x-&gt;lc; //对称地处理——注意：此时succ != NULL else &#123; //若左右子树均存在，则选择x的直接后继作为实际被摘除节点，为此需要 w = w-&gt;succ(); //（在右子树中）找到*x的直接后继*w swap ( x-&gt;data, w-&gt;data ); //交换*x和*w的数据元素 BinNodePosi(T) u = w-&gt;parent; ( ( u == x ) ? u-&gt;rc : u-&gt;lc ) = succ = w-&gt;rc; //隔离节点*w &#125; hot = w-&gt;parent; //记录实际被删除节点的父亲 if ( succ ) succ-&gt;parent = hot; //并将被删除节点的接替者与hot相联 release ( w-&gt;data ); release ( w ); return succ; //释放被摘除节点，返回接替者&#125; //release()负责释放复杂结构，与算法无直接关系 效率：BST的删除操作所需的时间，主要消耗于对search()、succ()和updateHeightAbove()的调用。在树中的任一层，它们各自都至多消耗$O(1)$时间，故总体的渐进时间复杂度，亦不超过全树的高度$O(h)$。 3.平衡二叉搜索树3.1.树高和性能由以上的实现与分析，BST主要接口search()，insert()和remove()的运行时间在最坏情况下，均线性正比于其高度$O(h)$，因此若不能有效地控制树高，则就实际的性能而言，较之此前的向量和列表等数据结构，BST将无法体现出明显优势。比如在最坏情况下，二叉搜索树可能彻底地退化为列表，此时的查找效率甚至会降至$O(n)$，线性正比于树（列表）的规模。那么出现此类最坏或较坏情况的概率有多大，或者从平均复杂度的角度看，二叉搜索树的性能究竟如何呢？下面按两种常用的随机统计口径，就BST的平均性能做一分析和对比。 随机生成： 考查n个互异词条$\\{e_1,e_2,\\dots,e_n\\}$，对任一排列$\\sigma=(e_{i_1},e_{i_2},\\dots,e_{i_n})$，从空树开始，通过依次执行insert($e_{i_k}$)，即可得到这n个关键码的一棵二叉搜索树$T(\\sigma)$。与随机排列$\\sigma$相对应的$T(\\sigma)$称为由$\\sigma$随机生成（randomly generated）。下图以关键码为例$\\{1,2,3\\}$为例，列出了由其所有排列生成的二叉搜索树。 显然任意的n个互异关键码都可以构成$n!$种全排列，若设各排列作为输入序列的概率均等，则在随机生成下二叉搜索树的平均高度为$\\Theta(\\log n)$。 随机组成： 另一种随机策略是，假定n个互异节点同时给定，然后在遵守顺序性的前提下，随机确定它们之间的拓扑联接关系，如此所得的二叉搜索树，称为由这组节点随机组成（randomly composed）。 由n个互异节点组成的二叉搜索树，总共有： T(n)=\\sum_{k=1}^n SP(k-1)\\cdot SP(n-k)=Catalan(n)=\\frac{(2n)!}{(n+1)!\\cdot n!}若设所有BST等概率出现，则其平均高度为$\\Theta(\\sqrt{n})$。 随机生成口径的$\\Theta(\\log n)$和随机组成口径的$\\Theta(\\sqrt{n})$之间就渐进意义为言有实质的差别。原因在于随机生成下，同一组关键码的不同排列所生成的二叉搜索树，未必不同，会发现中位数越早插入，树的高度越低即越平衡，而实际上越是平衡的树，被统计的次数亦越多。从这个角度看，随机生成口径有些“乐观”，高估了二叉搜索树的平均性能，因而按照随机组成口径得到的$\\Theta(\\sqrt{n})$更可信。 实际上按照随机组成口径统计出的平均树高，仍不足以反映树高的随机分布情况，在实际应用中，理想的随机并不常见，一组关键码往往会按照（接近）单调次序出现，树高较大的情况依然可能频繁出现。另外若removeAt()操作总是固定地将待删除的二度节点与其直接后继交换，则随着操作次数的增加，二叉搜索树向左侧倾斜的趋势将愈发明显。 3.2.理想平衡与适度平衡理想平衡： 在节点数目固定的前提下，应尽可能地降低高度，相应地应尽可能使兄弟子树的高度彼此接近，即全树尽可能平衡。由n个节点组成的二叉树，高度不低于$\\log_2 n$，当高度恰为$\\log_2 n$时称作理想平衡，即大致相当于完全二叉树或者满树，但是“叶节点只能出现于最底部的两层”—这一条件过于苛刻。此类二叉树所占的比例极低，而随着二叉树规模的增大，这一比例还将减小，且对二叉树的动态操作很容易就破坏了这种理想操作。 适度平衡： 若将树高限制为“渐进地不超过$O(\\log n)$，称作适度平衡。适度平衡的BST称为平衡二叉搜索树（balanced binary search tree，BBST），如之后要介绍的伸展树，红黑树，kd-树等都属于BBST。 3.3.等价变换3.3.1.等价二叉搜索树若两棵二叉搜索树的中序遍历序列相同，则称它们彼此等价。如下图两个由11个节点组成的相互等价的二叉搜索树，它们在拓扑关系上有差异。 等价二叉搜索树的特点可概括为： 上下可变：联接关系不尽相同，承袭关系可能颠倒 左右不乱：中序遍历序列完全一致，全局单调非降 3.3.2.局部性平衡二叉搜索树的适度平衡性，都是通过对树中每一局部增加某种限制条件来保证的。除了适度平衡性外，还具有如下局部性： 单次动态修改操作后，至多$O(1)$处局部不再满足限制条件 可在$O( \\log n)$时间内，使这些局部（以至全树）重新满足 这就意味着刚刚失去平衡的二叉搜索树，必然可以迅速转换为一棵等价的BBST，等价二叉搜索树之间的上述转换过程，也称作等价变换。 3.3.3.旋转调整最基本的等价变换方法，即修复局部性失衡的方法，就是通过围绕特定节点的旋转，实现等价前提下的局部拓扑调整。 zig和zag： zig和zag旋转均属于局部操作，仅涉及常数个节点及其之间的联接关系，故均可在常数时间内完成。每经过一次zig或zag旋转之后，节点v的深度加一，节点c的深度减一；这一局部子树（乃至全树）的高度可能发生变化，但上下幅度均不超过一层。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"Cpp基础（13）STL","slug":"Cpp基础（13）STL","date":"2020-02-28T01:37:54.000Z","updated":"2020-02-28T04:05:25.307Z","comments":true,"path":"2020/02/28/Cpp基础（13）STL/","link":"","permalink":"http://nekomoon404.github.io/2020/02/28/Cpp%E5%9F%BA%E7%A1%80%EF%BC%8813%EF%BC%89STL/","excerpt":"","text":"1.C++标准库 C++ 标准库（C++ Standard Library）是一个类库和函数的集合 提供了若干泛型容器、函数对象、泛型字符串和流、常用函数等 声明在 std 名字空间中 using namespace std; 吸收了 ISO C90 C 标准程序库原有的 C 标准库中的所有头文件，都移去末尾的 .h 并在开头加上 c，如 &lt;stdio.h&gt; 变为 &lt;cstdio&gt; C++ 标准模板库（C++ Standard Template Library，STL） 是 C++ 标准库的一个子集 包括容器、算法、迭代器和函数对象 包含标准库声明的一些头文件关系： 2.C++标准库模板 C++ 中主要有两个方面体现了重用 面向对象的设计思想，包括继承和多态，以及标准类库 泛型程序设计（generic programming）的思想，包括模板机制，以及标准模板库（STL） 泛型程序设计，简单来说就是使用模板的程序设计法 将一些常用的数据结构（如链表、队列、二叉树）和算法（如排序、查找）写成模板，无论数据结构里放的是什么类型的对象 标准模板库（Standard Template Library，STL）就是一些常用数据结构和算法模板的集合 STL大致可以视为由四部分组成：容器、迭代器、算法、函数对象 2.1.容器 容器是可容纳各种数据类型的数据结构 包括顺序容器和关联容器 还有一类不提供真正的用于存储元素的数据结构实现，称作容器适配器 容器适配器不支持迭代器，由使用者选择合适的底层数据结构 在顺序容器中，元素的插入位置与元素的值无关： vector （声明于 &lt; vector&gt;） 顺序表：实现了一个动态数组，可以在常数时间内完成随机存取元素，可以自动调整大小，在尾端增删元素时具有较佳的性能 array （C++11 中新增，声明于 &lt; array&gt;）顺序表：封装了一个静态数组，只能在初始化时指定大小 deque （声明于 &lt; deque&gt;）双端队列：实现了一个动态数组，可以在常数时间内完成随机存取元素，可以快速地在数组的头尾两端增删元素 list （声明于 &lt; list&gt;）双向链表：不支持随机存取，但在任何位置增删元素都能在常数时间完成 forward_list （ C++11 中新增，声明于 ）单向链表：list 类的单链表版，去掉了一些操作 在关联容器中，元素的插入位置与元素的值有关，必须按相应的排序准则来确定，在查找元素时具有非常好的性能: set / multiset （声明于 #include&lt;set&gt;） 集合：实现了一棵平衡二叉搜索树，使用元素本身作为键值（key）；set 容器中不允许存在相同元素，multiset 容器中允许存在相同的元素 map / multimap （声明于 #include&lt;map&gt;）映射表：实现了一棵平衡二叉搜索树，存放的是成对的键值和数据（key / value），并根据键值对元素进行排序，可快速地根据键值来检索元素；map 容器中不允许存在键值相同的元素，而 multimap 容器中则允许 C++11 中新增的 unordered_set / unordered_multiset （声明于 &lt; unordered_set&gt;）和 unordered_map / unordered_multimap （声明于 &lt; unordered_map&gt;）映射表：通过哈希表实现，功能与 set/multiset 和 map/multimap 相似，但不对键值排序 在容器适配器中，不提供真正的用于存储元素的数据结构实现，不支持迭代器，由使用者选择合适的底层数据结构： stack （声明于 #include&lt;stack&gt;）栈：是项的有限序列，并满足序列中被删除、检索和修改的项只能是最近插入序列的项，即按照后进先出的原则 queue （声明于 #include&lt;queue&gt;）队列：插入只可以在尾部进行，删除、检索和修改只允许从头部进行，即按照先进先出的原则 priority_queue （声明于 #include&lt;queue&gt;）优先队列：最高优先级元素总是第一个出队，可视作堆 所有 STL 容器的共有的成员函数： operator = / &lt; / &lt;= / &gt; / &gt;= / == / != ： 比较元素大小 empty() ：判断容器中是否有元素 max_size() ：容器中最多能装多少元素 size() ：容器中的元素个数 swap()：交换两个容器对象中的内容 只在顺序容器和关联容器中的成员函数： begin()：返回指向容器中第一个元素的迭代器 end()：返回指向容器中最后一个元素后面的位置的迭代器 rbegin()：返回指向容器中最后一个元素的迭代器 rend()：返回指向容器中第一个元素前面的位置的迭代器 erase()：从容器中删除一个或几个元素 clear()：从容器中删除所有元素 除前述共同操作外，顺序容器还有以下共同操作： front()：返回容器中第一个元素的引用 back()：返回容器中最后一个元素的引用 push_back()：在容器末尾增加新元素 pop_back()：删除容器末尾的元素 2.2.迭代器迭代器用于指向顺序容器和关联容器中的元素，实际上就是泛化的指针，有 const 和非 const 两种： const_iterator 对于遍历 const 容器是必需的，允许以只读方式访问容器的底层元素； 通过迭代器可以读取它指向的元素，通过非 const 迭代器还能修改其指向的元素，用法和指针类似。 使用“容器类名::iterator ”或“容器类名::const_iterator ”声明： 例如 vector&lt;int&gt;::iterator it; 或 set&lt;string&gt;::const_iterator it; 读取元素时就可以用 *it 来实现 迭代器上可以执行自增（++）操作，以指向容器中的下一个元素： 如果迭代器到达了容器中的最后一个元素的后面，则迭代器变成 past-the-end 值，其与 NULL 的含义类似。 不同容器上支持的迭代器功能强弱有所不同，按功能由弱到强，可以将迭代器分为 5 种： 输入迭代器 Input iterators 和 输出迭代器 Output iterators提供对数据的只读或只写访问 正向迭代器 Forward iterators提供读写操作，并能一次一个地向前推进迭代器 双向迭代器 Bidirectional iterators提供读写操作，并能一次一个地向前和向后移动 随机访问迭代器 Random access iterators提供读写操作，并能在数据中随机移动 注意：强迭代器拥有弱迭代器的所有功能，能当作弱迭代器使用。 不同迭代器所能进行的操作如下所示： 所有迭代器： ++p、p++ 输入迭代器： *p、p = q、p == q、p != q 输出迭代器： *p、p = q 正向迭代器： 以上所有 双向迭代器： 以上所有，以及 --p、p-- 随机访问迭代器： 以上所有，以及移动 i 个单元（p += i、p -= i、p + i、p – i）、大小比较（p &lt; q、p &lt;= q、p &gt; q、p &gt;= q）、数组下标访问p[i]（p 后面的第 i 个元素的引用） 容器所支持的迭代器类别如下： 示例： 1234567891011121314151617181920212223#include &lt;vector&gt;#include &lt;iostream&gt;using namespace std;int main() &#123; vector&lt;int&gt; v; // 一个存放 int 型元素的向量，一开始是空的 // 依次放入 1 2 3 4 v.push_back(1); v.push_back(2); v.push_back(3); v.push_back(4); // 使用常量迭代器打印这个容器中的元素 vector&lt;int&gt;::const_iterator i; for (i = v.begin(); i != v.end(); ++i) cout &lt;&lt; *i &lt;&lt; \" \"; cout &lt;&lt; endl; // 输出 1 2 3 4 // 非常量迭代器修改容器中的元素 vector&lt;int&gt;::iterator j; for (j = v.begin(); j != v.end(); ++j) *j = 100; // 再次使用常量迭代器打印这个容器中的元素 for (i = v.begin(); i != v.end(); i++) cout &lt;&lt; *i &lt;&lt; \" \"; cout &lt;&lt; endl; // 输出 100 100 100 100 return 0;&#125; 2.3.算法算法就是一个个函数模板，STL 中提供了能在各种容器中通用的算法，如插入、删除、查找、排序等，约有70种标准算法。算法可以处理容器，也可以处理内置类型的数组。 算法通过迭代器来操纵容器中的元素，许多算法需要两个参数，一个是起始元素的迭代器，一个是终止元素的后面一个元素的迭代器。有的算法返回一个迭代器，比如 find() 算法，其功能是在容器中查找一个元素，并返回一个指向该元素的迭代器。 头文件 #inlcude&lt;algorithm&gt; 中实现了一些常见的针对序列区间的算法：包括不修改值的序列操作、修改值的序列操作、分割与合并、查找与排序、堆操作、排列相关操作等。 头文件 #include&lt;numeric&gt; 中则实现了一些特别针对数值序列的算法 accumulate()：累加一个区间中的值 adjacent_difference()：依次计算一个区间内每一对相邻元素的差 inner_product()：计算两个向量的内积 下面介绍一些&lt; algorithm&gt;中实现的常见算法： 不修改值的序列操作for_each()：遍历一个区间内的元素find()：在一个区间中进行查找指定的元素count()：在一个区间中计数指定元素search()：在一个区间中查找指定的子序列 修改值的序列操作copy()：复制一个区间的内容swap()：交换两个对象的值replace()：替换一个区间中的某个值unique()：去重，删除相邻的相同元素random_shuffle()：随机洗牌，重排区间中的元素 分割与合并partition()：将一个区间根据指定规则分割为两个merge()：将有序区间合并set_intersection()：找出两个区间中相同的元素set_difference()：找出两个区间中不同的元素 查找binary_search()：二分查找min_element()：查找最小的元素max_element()：查找最大的元素 排序sort()：用快速排序算法给一个区间中的元素排序 堆（Heap）make_heap()：根据指定序列构建堆push_heap()：向堆中插入元素pop_heap()：弹出堆顶元素 排列next_permutation()：产生指定序列的下一个排列prev_permutation()：产生指定序列的上一个排列 2.4.函数对象函数对象即重载了操作符 () 的对象，类的实例都可以称作为函数对象，本身是对象，但是用起来看上去象函数调用，实际上也执行了函数调用。 STL 中的头文件 &lt; functional&gt; 里实现了一些函数对象类模板，例如 equal_to&lt; T&gt;、greater&lt; T&gt;、less&lt; T&gt; 等这些模板都可以用来生成函数对象。 函数对象可以包含其他成员变量或成员函数，函数对象的优点是可以在对象内部修改而不用改动外部接口，可以存储先前调用结果的数据成员，编译器可通过内联函数对象来增强性能。 例如下面声明了一个函数对象 average 计算均值： 1234567891011121314151617class Average &#123;public: double operator()(int a1, int a2) &#123; return (double)(a1 + a2) / 2; &#125; double operator()(int a1, int a2, int a3) &#123; return (double)(a1 + a2 + a3) / 3; &#125;&#125;; // 重载 () 运算符时，参数可以是任意多个int main() &#123; Average average; // 函数对象 // 调用 average.operator(2, 3, 5) // 但是使用时看上去象普通函数调用 cout &lt;&lt; average(2, 3, 5) &lt;&lt; endl; // 输出 3.33333 cout &lt;&lt; average(7, 11) &lt;&lt; endl; // 输出 9 return 0;&#125; 例2： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class IntMod &#123; int p;public: IntMod(int _p) : p(_p) &#123; &#125; bool operator() (int v1, int v2) &#123; return v1 % p &gt;= v2 % p; &#125; bool operator() (float v1, float v2) &#123; return (int)(v1) * p % 10 &gt;= (int)(v2) * p % 10; &#125;&#125;; //IntMod 类重载了 ()对于 int 型，比较 %p 之后的大小//对于 float 型，先转换成 int 型，然 后比较 *p 后 %10 的大小template &lt;class T&gt;T myMax(T *pV, int n, IntMod fun) &#123; T result = *pV; for (int i = 1; i &lt; n; ++i) &#123; if (!fun(result, *(pV + i))) &#123; result = *(pV + i); &#125; &#125; return result;&#125;//函数模板 myMax() 寻找最大值，可以处理不同类型的数组//根据 IntMod 类中定义的大小关系，找到不同类型数组中的最大值int main()&#123; int a[5] = &#123;3, 9, 12, 5, 20&#125;; float b[5] = &#123;3, 9, 12, 5, 20&#125;; IntMod obj1(8), obj2(3); cout &lt;&lt; myMax(a, 5, obj1) &lt;&lt; endl; //第一行输出对应 int 型数组 a，数组长度为 5，对 8 模后取余 //即找到 %8 之后最大的整数，因此最大值是 5，输出 5 cout &lt;&lt; myMax(b, 5, obj1) &lt;&lt; endl; //第二行输出对应 float 型数组 b，数组长度为5,*8后对10模后取余 //即找到 *8 %10 之后最大的数，因此最大值是 12，输出 12 cout &lt;&lt; myMax(a, 5, obj2) &lt;&lt; endl; //第三行输出对应 int 型数组 a，数组长度为 5，对 3模后取余 //即找到 %3 之后最大的整数，若有相等的情况找靠前的那个，因此最大值是 5，输出 5 cout &lt;&lt; myMax(b, 5, obj2) &lt;&lt; endl; //输出3 return 0;&#125;","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"数据结构与算法（12）广度/深度优先搜索","slug":"数据结构与算法（12）广度-深度优先搜索","date":"2020-02-26T01:31:21.000Z","updated":"2020-02-27T13:21:14.014Z","comments":true,"path":"2020/02/26/数据结构与算法（12）广度-深度优先搜索/","link":"","permalink":"http://nekomoon404.github.io/2020/02/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8812%EF%BC%89%E5%B9%BF%E5%BA%A6-%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/","excerpt":"","text":"1.广度优先搜索1.1.算法 始自顶点s的广度优先搜索（Breadth-First-Search） 访问顶点s 依次访问s所有尚未访问的邻接顶点 依次访问它们尚未访问的邻接顶点 …….如此反复，直至没有尚未访问的邻接顶点 这种搜索将首先访问s，在这个图中通过将s染黑表示它已经接受了访问。接下来需要访问S所有尚未访问的邻接顶点，由s通往它的那些刚被访问的邻居的边都被加粗，这暗示着这些边都已经被算法所采纳和保留，这些边都是非常重要的，它们携带了整个遍历过程中所发现的一些信息。反过来在原图中还会有一些边并不采纳（浅色线部分）在经过广度优先遍历之后，它们将不再保留而是被舍弃掉。 这个算法将不断地如此迭代反复，直到所有的顶点都接受了访问。所谓广度优先搜索的确是一种遍历，它会按照刚才所介绍的策略确定不同顶点接受访问的次序，并且按照这种次序对各顶点逐个地访问，而整个搜索过程的最终产物或成果不过是选自原图的一系列的加粗的边。 这里按照与起点s的距离将所有的顶点划分为若干个等价类，在同一等价类内部各顶点的边都不会被采纳，而只有连接于相邻等价类之间的某些边才会被采纳。所有被保留下来并且采纳的这些边将足以把所有的顶点连接起来构成一个连通图，且它是一个极大无环图。这就相当于一棵树，这棵树中涵盖了原图的所有的顶点，所以称之为支撑树（Spanning Tree）。对于树而言，以上策略及过程完全等同于层次遍历。 1.2.实现上述的策略可以实现而这样一段代码： 123456789101112131415161718template &lt;typename Tv, typename Te&gt; //广度优先搜索BFS算法（单个连通域）void Graph&lt;Tv, Te&gt;::BFS ( int v, int&amp; clock ) &#123; //assert: 0 &lt;= v &lt; n Queue&lt;int&gt; Q; //引入辅助队列 status ( v ) = DISCOVERED; Q.enqueue ( v ); //初始化起点 while ( !Q.empty() ) &#123; //在Q变空之前，不断 int v = Q.dequeue(); dTime ( v ) = ++clock; //取出队首顶点v for ( int u = firstNbr ( v ); -1 &lt; u; u = nextNbr ( v, u ) ) //枚举v的所有邻居u if ( UNDISCOVERED == status ( u ) ) &#123; //若u尚未被发现，则 status ( u ) = DISCOVERED; Q.enqueue ( u ); //发现该顶点 status ( v, u ) = TREE; parent ( u ) = v; //引入树边拓展支撑树 &#125; else &#123; //若u已被发现，或者甚至已访问完毕，则 status ( v, u ) = CROSS; //将(v, u)归类于跨边 &#125; status ( v ) = VISITED; //至此，当前顶点访问完毕 &#125;&#125; 可以看到遍历的起点总是某个预先指定的顶点v，既然图的广度优先遍历可以视作为树的层次遍历的一种推广，所以与后者相仿这里依然借助一个队列结构来实现。在v入队之前将它的状态由最初的undiscovered转化为discovered，接下来的while循环每次都通过dequeue()取出队首的顶点并且重新命名为v。 请注意 在每一个顶点刚刚出队并随即接受访问的同时，我们还需要给它打上一个时间标签dTime，在算法的入口处还有一个名为clock的引用型参数，它就像是一块钟表在整个算法的运行过程中都会给出时间的进度，任何时候如果你希望加注当前的时间标签，只需要将这块表取出来并读取上面的时刻。 按照算法的策略我们需要枚举出当前节点v的所有邻居，通过for循环语句来实现，firstNbr以及nextNbr接口在上文有介绍过。 经过整个的遍历搜索过程，每一个顶点的状态都会由最初的undiscovered转化为discovered，并最终转化为visited，这样的三个状态也就构成了每一个顶点在它的生命期内的三部曲。 下面以一个无向图为例来理解算法的过程： 1.3.全图BFS与起始顶点s相连通的每一个顶点都会被bfs搜索、发现并访问，即s顶点所属的那个连通域确实可以被悉数的遍历。然而问题是并非每幅图都只包含一个连通域，那么在含有多个连通域的时候从任何一个起点s出发未必能够抵达其它的连通域。那么这种情况如何处理，如何使得bfs搜索足以覆盖整幅图呢，可以采用下面的方法： 12345678template &lt;typename Tv, typename Te&gt; //广度优先搜索BFS算法（全图）void Graph&lt;Tv, Te&gt;::bfs ( int s ) &#123; //assert: 0 &lt;= s &lt; n reset(); int clock = 0; int v = s; //初始化 do //逐一检查所有顶点 if ( UNDISCOVERED == status ( v ) ) //一旦遇到尚未发现的顶点 BFS ( v, clock ); //即从该顶点出发启动一次BFS while ( s != ( v = ( ++v % n ) ) ); //按序号检查，故不漏不重&#125; 这里毕竟引入了一层新的循环而且至少从表面看来，这个循环的迭代次数将多达线性次。但这里并非对每一个顶点都启动一轮bfs搜索，而是只有在当前的顶点能够经过这个if判断（顶点尚未被发现）之后才启动这样一次搜索。这种处理方式可以保证对于每一个连通域只有一个顶点可能作为起点引起它所属的那个连通域被完全的遍历掉，每一个连通域启动而且只启动一次广度优先搜索，因此所有花费在搜索上的时间累计也不过是对全图的一次遍历，而不是多次。 1.4.复杂度广度优先搜索算法的复杂度取决于不同实现方法，尤其是图结构自身的实现算法，这里不妨就以我们的实现版本为例，算法主体的复杂度部分是由while以及for所构成的两重循环。 1.5.最短路径最好来讨论BFS算法的一个特性：最短距离性。 回顾此前所介绍的树结构，相对于树根节点任何一个节点v都对应于一条唯一的通路，这条路径的长度称作顶点v的深度depth(v)，于是我们可以对所有的顶点自上而下按照它们的深度进行等价类划分，在每一个等价类中的所有顶点所具有的深度指标都是彼此相等的。而树的层次遍历也可以认为是按照这一指标非降的次序，将所有的顶点逐一枚举出来。 那么这样一个遍历的过程是否也可以转化为图结构的遍历过程呢？表面看来似乎不太容易，因为此时与树结构极不相同的就是从起始顶点s出发可能有多条路径都最后通往同一个顶点，而且可能出现分叉。然而这样一个问题不难解决，实际上我们只需考察顶点之间的最短通路，并且将这两个顶点之间的距离取作这条最短通路的长度dist(v, s)。 巧合的是图的BFS搜索与树的层次遍历一样都具有这样一种单调性，即BFS所给出的顶点序列按照这样到起点的距离也是按照非降次单调排列的。在我们最终所生成的BFS树中，每个顶点与s之间的那条通路恰好就是在原图中这两个顶点之间的最短通路。 2.深度优先搜索2.1.算法这一节将介绍与上一节中的广度优先搜索完全对称的另一种搜索算法：深度优先搜索。相对于此前的广度优先搜索，深度优先搜索的算法策略更为简明，然而深度优先搜索的过程更为复杂，其功能也相对而言更为强大，因此也成为有效解决很多实际问题的。 深度优先搜索的基本算法框架如下： 首先确定一个搜索的起点s，找到它的任意的一个邻居，并且将控制权交给这个新的顶点。接下来新的顶点一旦接过控制权，它也会仿效这种策略在它的所有邻居中任选其一，并且将控制权交给这个尚未访问的邻居。当然如果有已经被访问过的，对应的这条边将不会被采用，而是以某种适当的形式加以标注（图中以浅色线表示）。假设这个顶点已经没有任何邻居尚未访问，那么按照算法的策略，我们将在这个位置返回（回溯），顺着此前的通路回到它的前驱顶点。 可以看到遍历的效果与此前的BFS类似，我们依然会得到一棵DFS树，也就是图中这些粗边所构成的一棵支撑树，同样地未被这棵树所采纳的那些边会被分类，而且这种分类要更为细致。 这样一个遍历和递归的过程可以实现为下面的代码： 1234567891011121314151617181920212223template &lt;typename Tv, typename Te&gt; //深度优先搜索DFS算法（单个连通域）void Graph&lt;Tv, Te&gt;::DFS ( int v, int&amp; clock ) &#123; //assert: 0 &lt;= v &lt; n dTime ( v ) = ++clock; status ( v ) = DISCOVERED; //发现当前顶点v for ( int u = firstNbr ( v ); -1 &lt; u; u = nextNbr ( v, u ) ) //枚举v的所有邻居u switch ( status ( u ) ) &#123; //并视其状态分别处理 case UNDISCOVERED: //u尚未发现，意味着支撑树可在此拓展 status ( v, u ) = TREE; parent ( u ) = v; DFS ( u, clock ); break; case DISCOVERED: //u已被发现但尚未访问完毕，应属被后代指向的祖先 status ( v, u ) = BACKWARD; break; default: //u已访问完毕（VISITED，有向图），则视承袭关系分为前向边或跨边 status ( v, u ) = ( dTime ( v ) &lt; dTime ( u ) ) ? FORWARD : CROSS; break; &#125; status ( v ) = VISITED; fTime ( v ) = ++clock; //至此，当前顶点v方告访问完毕&#125;template &lt;typename Tv, typename Te&gt; //深度优先搜索DFS算法（全图）void Graph&lt;Tv, Te&gt;::dfs ( int s ) &#123; //assert: 0 &lt;= s &lt; n reset(); int clock = 0; int v = s; //初始化 do //逐一检查所有顶点 if ( UNDISCOVERED == status ( v ) ) //一旦遇到尚未发现的顶点 DFS ( v, clock ); //即从该顶点出发启动一次DFS while ( s != ( v = ( ++v % n ) ) ); //按序号检查，故不漏不重&#125; 2.2.实例（无向图）下面是一个无向图的实例，每一行的3格分别代表顶点及其dTime和fTime，为了方便理解将当前顶点在图中变为大写字母。 2.3.实例（有向图）有向图的深度优先搜索要更为复杂，所涉及的情况也会更多。不妨来看下面的一个实例，首先确认这是一幅有向图，在这个图中我们将每一个顶点都绘制成长方形，顶点的标识居中，在它的左右空白处将分别记录下它在遍历过程中所获得的dTime和fTime两个时间标签。当前顶点用深色加粗边框表示且字母大写，被访问过的顶点用双线边框表示，处于VISITED状态的顶点用黑色方框表示。 综观整个过程，我们总共用了10秒遍历完了由这五个顶点所构成的一个子图，更确切地讲它们构成了在这个图中从顶点a出发可以达到的区域，也称作可达区域。那么图中的其余部分呢？比如说这里的顶点d以及e呢？ 回顾对广度优先遍历算法bfs的处理手法，就不难发现我们完全可以效仿那种做法，在这样的dfs算法之外再包装一层循环来枚举图中的所有顶点。这样的话就可以无一遗漏地遍历图中的所有顶点，而且只要处理得当对所有可达域的遍历都不会彼此有所重叠，从而在时间效率上也依然可以得到保证。 最后不妨来盘点一下遍历所获得的成果，首先是这些粗边它们构成了两棵遍历树，整体地构成了一个遍历森林；此外我们还对所有未被采纳的边进行了分类：跨越边、前向边以及后向边，无一遗漏。在通过遍历所获得的所有这些信息中遍历树或者说遍历森林无疑是最为重要的，然而相对于原图，它们毕竟只是一个子集，这样一个子集所携带的难道是原图的所有信息吗？从某种意义上讲的确是这样的，而其中至关重要的一点就在于我们通过遍历不仅获得了这样一棵树，而且为每一个顶点都标记了dTime和fTime两类时间标签，而这两类时间标签的作用是非常巨大的。 2.4.括号引理/嵌套引理通过深度优先搜索DFS为图中所有顶点所标注的两个时间标签dTime和fTime，实际上蕴含了大量有用的信息这一点可以由括号引理或嵌套引理来加以印证。 为此首先要引入顶点的活动期的概念，也就是由它的dTime和fTime两个时间标签所界定的那样一段时间范围，即这个顶点在整个DFS搜索过程中处于活跃状态的时间范围。嵌套引理指出任何有向图经过了DFS搜索之后，在对应的DFS森林或者DFS树中任何一对顶点之间存在直系的血缘关系，当且仅当它们的活跃期存在包含与被包含的关系。 为了获得对这个引理更为直观的认识，我们不妨以横向作为时间轴，依然以上节的有向图为例，将每个顶点都沿水平方向适当展开使得它们恰好覆盖各自所对应的活跃期。不难看出祖先的活跃期的确会覆盖它的后代，而反过来没有直接血缘关系的节点比如说F和B，或者B和G，它们的活跃期都的确彼此没有任何公共的部分。 这样一种特性是非常强大的一个工具，比如在算法中我们经常需要做的一个判断就是：任意的一对顶点 v 和 u 之间在遍历树中是否存在一个直系血缘的关系。如果没有这样一种简便的机制，我们将不得不从 u 出发顺着parent引用不断地溯流而上直到遇到顶点v，才能够确定它们的确存在祖先和后代的直系关系；或者不得不一直追溯到整个遍历的起点，从而断定u和v之间并没有直系血缘关系。而现在借助时间标签，我们可以快速准确地在$O(1)$的时间内就得出相应的结论。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"数据结构与算法（11）图","slug":"数据结构与算法（11）图","date":"2020-02-25T01:23:52.000Z","updated":"2020-02-25T07:59:39.508Z","comments":true,"path":"2020/02/25/数据结构与算法（11）图/","link":"","permalink":"http://nekomoon404.github.io/2020/02/25/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8811%EF%BC%89%E5%9B%BE/","excerpt":"","text":"1.概述1.1.基本术语 G = ( V; E) = ( 顶点集; 边集) 顶点（vertex）：n = |V| 边（edge）|弧（arc）：e = |E| 邻接关系（adjacency）：定义同一条边的两个顶点之间的关系 自环（self-loop）：同一顶点自我相邻 简单图（simple graph）：不含自环，这一章讨论的都是简单图 关联关系（incidence）：顶点与其所属的边之间的关系 度（degree）：于同一顶点关联的边数 此前所学的几种数据结构都可以视作是图的特例，比如在向量和列表等线性结构中只有互为前驱与后继的元素之间才能够定义邻接关系，而树结构中只能在父节点与子节点之间才能够定义邻接关系。 图更为一般化，其中的任何两个节点之间都允许存在这样的一个邻接关系，我们这里讨论的图排除自环的存在。 1.2.无向图/有向图 若关联顶点u和v次序无所谓，则(u, v)为无向边（undirected edge） 所有边均为无方向的图，即为无向图（undigraph） 反之，有向图（digraph）中均为有向边（directed edge） u，v分别称作边(u, v)的尾（tail）、头（head） 无向边、有向边并存的图，称作混合图（mixed graph） 在图这一章我们关注有向图，因为通过有向图完全可以表示并且实现无向图以及混合图，简单的做法是将任何一条无向边转化为彼此对称的一对有向边。 1.3.路径/环路 路径：$\\pi$ = &lt;$V_0,V_1,\\dots,V_k&gt;$ 长度： | $\\pi$ | = k 简单路劲：$V_i=V_j$ 除非$i=j$，即路劲中不含重复顶点 环/环路：$V_0=V_k$ 有向无环图（DAG） 欧拉环路：| $\\pi$ | = | E |，即各边恰好出现一次 哈密尔顿环路：| $\\pi$ | = | V |，即各顶点恰好出现一次 2.基于邻接矩阵实现图结构2.1. Graph模板类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051typedef enum &#123; UNDISCOVERED, DISCOVERED, VISITED &#125; VStatus; //顶点状态typedef enum &#123; UNDETERMINED, TREE, CROSS, FORWARD, BACKWARD &#125; EType; //边在遍历树中所属的类型template &lt;typename Tv, typename Te&gt; //顶点类型、边类型class Graph &#123; //图Graph模板类private: void reset() &#123; //所有顶点、边的辅助信息复位 for ( int i = 0; i &lt; n; i++ ) &#123; //所有顶点的 status ( i ) = UNDISCOVERED; dTime ( i ) = fTime ( i ) = -1; //状态，时间标签 parent ( i ) = -1; priority ( i ) = INT_MAX; //（在遍历树中的）父节点，优先级数 for ( int j = 0; j &lt; n; j++ ) //所有边的 if ( exists ( i, j ) ) type ( i, j ) = UNDETERMINED; //类型 &#125; &#125; void BFS ( int, int&amp; ); //（连通域）广度优先搜索算法 void DFS ( int, int&amp; ); //（连通域）深度优先搜索算法 void BCC ( int, int&amp;, Stack&lt;int&gt;&amp; ); //（连通域）基于DFS的双连通分量分解算法 bool TSort ( int, int&amp;, Stack&lt;Tv&gt;* ); //（连通域）基于DFS的拓扑排序算法 template &lt;typename PU&gt; void PFS ( int, PU ); //（连通域）优先级搜索框架public:// 顶点 int n; //顶点总数 virtual int insert ( Tv const&amp; ) = 0; //插入顶点，返回编号 virtual Tv remove ( int ) = 0; //删除顶点及其关联边，返回该顶点信息 virtual Tv&amp; vertex ( int ) = 0; //顶点v的数据（该顶点的确存在） virtual int inDegree ( int ) = 0; //顶点v的入度（该顶点的确存在） virtual int outDegree ( int ) = 0; //顶点v的出度（该顶点的确存在） virtual int firstNbr ( int ) = 0; //顶点v的首个邻接顶点 virtual int nextNbr ( int, int ) = 0; //顶点v的（相对于顶点j的）下一邻接顶点 virtual VStatus&amp; status ( int ) = 0; //顶点v的状态 virtual int&amp; dTime ( int ) = 0; //顶点v的时间标签dTime virtual int&amp; fTime ( int ) = 0; //顶点v的时间标签fTime virtual int&amp; parent ( int ) = 0; //顶点v在遍历树中的父亲 virtual int&amp; priority ( int ) = 0; //顶点v在遍历树中的优先级数// 边：这里约定，无向边均统一转化为方向互逆的一对有向边，从而将无向图视作有向图的特例 int e; //边总数 virtual bool exists ( int, int ) = 0; //边(v, u)是否存在 virtual void insert ( Te const&amp;, int, int, int ) = 0; //在顶点v和u之间插入权重为w的边e virtual Te remove ( int, int ) = 0; //删除顶点v和u之间的边e，返回该边信息 virtual EType &amp; type ( int, int ) = 0; //边(v, u)的类型 virtual Te&amp; edge ( int, int ) = 0; //边(v, u)的数据（该边的确存在） virtual int&amp; weight ( int, int ) = 0; //边(v, u)的权重// 算法 void bfs ( int ); //广度优先搜索算法 void dfs ( int ); //深度优先搜索算法 void bcc ( int ); //基于DFS的双连通分量分解算法 Stack&lt;Tv&gt;* tSort ( int ); //基于DFS的拓扑排序算法 void prim ( int ); //最小支撑树Prim算法 void dijkstra ( int ); //最短路径Dijkstra算法 template &lt;typename PU&gt; void pfs ( int, PU ); //优先级搜索框架&#125;; 2.2.邻接矩阵与关联矩阵 邻接矩阵（adjacency matrix）：用二维矩阵记录顶点之间的邻接关系，矩阵元素与图中存在的边一一对应 $A(i, j)=1$，若顶点$i$与$j$之间存在一条边；否则为0 （可以推广到带权图，即网络） 既然只考察简单图，则对角线元素统一设置为0 空间复杂度为$\\Theta(n^2)$，与图中实际的边数无关 关联矩阵（incidence matrix）：用二维矩阵记录顶点与边之间的关联关系 空间复制度为$\\Theta(n*e)=O(n^3)$ 空间利用率 &lt; 2/n 下面是几个实例： 2.3. Vertex下面是顶点（vertex）的一种实现方式： 123456789101112typedef enum &#123; UNDISCOVERED, DISCOVERED, VISITED &#125; VStatus; //顶点状态template &lt;typename Tv&gt; struct Vertex &#123; //顶点对象（为简化起见，并未严格封装） Tv data; int inDegree, outDegree;//数据、出入度数 VStatus status; //（如上三种）状态 int dTime, fTime; //时间标签 int parent; //在遍历树中的父节点 int priority; //在遍历树中的优先级数（最短通路、极短跨边等） Vertex ( Tv const&amp; d = ( Tv ) 0 ) : //构造新顶点 data ( d ), inDegree ( 0 ), outDegree ( 0 ), status ( UNDISCOVERED ), dTime ( -1 ), fTime ( -1 ), parent ( -1 ), priority ( INT_MAX ) &#123;&#125; //暂不考虑权重溢出&#125;; 边（Edge）的一种实现方式： 12345678typedef enum &#123; UNDETERMINED, TREE, CROSS, FORWARD, BACKWARD &#125; EStatus; //边在遍历树中所属的类型template &lt;typename Te&gt; struct Edge &#123; //边对象（为简化起见，并未严格封装） Te data; //数据 int weight; //权重 EStatus status; //（如上五种）状态 Edge ( Te const&amp; d, int w ) : data ( d ), weight ( w ), status ( UNDETERMINED ) &#123;&#125; //构造&#125;; 2.4. GraphMatrix现在就可以给出基于邻接矩阵实现图结构的一种可行方式，GraphMatrix类派生于此前所定义的Graph模板类，将顶点集实现为向量结构，其长度恰好等于顶点的规模即n；将边集实现为向量的向量，相当于一个n×n的矩阵，它恰好就是此前所构思的邻接矩阵。 1234567891011121314template &lt;typename Tv, typename Te&gt; //顶点类型、边类型class GraphMatrix : public Graph&lt;Tv, Te&gt; &#123; //基于向量，以邻接矩阵形式实现的图private: Vector&lt; Vertex&lt; Tv &gt; &gt; V; //顶点集（向量） Vector&lt; Vector&lt; Edge&lt; Te &gt; * &gt; &gt; E; //边集（邻接矩阵）public: GraphMatrix() &#123; n = e = 0; &#125; //构造 ~GraphMatrix() &#123; //析构 for ( int j = 0; j &lt; n; j++ ) //所有动态创建的 for ( int k = 0; k &lt; n; k++ ) //边记录 delete E[j][k]; //逐条清除 &#125; /* 操作接口：顶点查询、顶点修改、边查询、边修改 */&#125;; 由于我们此前对向量所重载的方括号操作符[]，这里只需用E[i][j]这样一种形式即可指代在顶点i与 j 之间的一条边，既可以读出这条边的信息，也可以反过来修改其中的某些信息。 2.5.顶点操作按照这种实现方式，我们可以简明实现顶点操作中的大部分基本操作： 123456789// 顶点的基本操作：查询第i个顶点（0 &lt;= i &lt; n） virtual Tv&amp; vertex ( int i ) &#123; return V[i].data; &#125; //数据 virtual int inDegree ( int i ) &#123; return V[i].inDegree; &#125; //入度 virtual int outDegree ( int i ) &#123; return V[i].outDegree; &#125; //出度 virtual VStatus&amp; status ( int i ) &#123; return V[i].status; &#125; //状态 virtual int&amp; dTime ( int i ) &#123; return V[i].dTime; &#125; //时间标签dTime virtual int&amp; fTime ( int i ) &#123; return V[i].fTime; &#125; //时间标签fTime virtual int&amp; parent ( int i ) &#123; return V[i].parent; &#125; //在遍历树中的父亲 virtual int&amp; priority ( int i ) &#123; return V[i].priority; &#125; //在遍历树中的优先级数 对于任意顶点i，如何枚举其所有的邻接顶点neighbor？为此首先需要实现一个名为nextNbr的接口，它的功能语义是如果我们现在已经枚举到顶点i的编号为 j 的邻居，那么它将返回接下来的下一个邻居。与顶点 i 潜在的可以相邻的点，无非就是它在邻接矩阵中所对应的那一行中数值为1的单元，对应于与i邻接的一个顶点。而第一个有效的邻居是通过firstNbr接口实现，它调用了nextNbr，将顶点n（并不实际存在）作为上一个有效的邻居。 1234567virtual int nextNbr ( int i, int j ) &#123;//相对于顶点j的下一邻接顶点 while ( ( -1 &lt; j ) &amp;&amp; ( !exists ( i, --j ) ) ); //逆向线性试探，O(n) return j; &#125; //改用邻接表可提高至O(1 + outDegree(i))virtual int firstNbr ( int i ) &#123; return nextNbr ( i, n ); &#125; //首个邻接顶点 2.6.边操作同样地，利用邻接矩阵我们也可以便捷地实现很多边的基本操作 1234567// 边的确认操作 virtual bool exists ( int i, int j ) //边(i, j)是否存在 &#123; return ( 0 &lt;= i ) &amp;&amp; ( i &lt; n ) &amp;&amp; ( 0 &lt;= j ) &amp;&amp; ( j &lt; n ) &amp;&amp; E[i][j] != NULL; &#125;// 边的基本操作：查询顶点i与j之间的联边（0 &lt;= i, j &lt; n且exists(i, j)） virtual EStatus &amp; status ( int i, int j ) &#123; return E[i][j]-&gt;status; &#125; //边(i, j)的状态 virtual Te&amp; edge ( int i, int j ) &#123; return E[i][j]-&gt;data; &#125; //边(i, j)的数据 virtual int&amp; weight ( int i, int j ) &#123; return E[i][j]-&gt;weight; &#125; //边(i, j)的权重 边插入： 假设我们需要在顶点i与顶点j之间连接一条有向边，假设这条边尚不存在，那么只需要将待插入的那条边的信息比如它的权重等等，封装为一个具体的边记录，然后将这个新的边记录地址存入于邻接矩阵中对应的那个单元，也可以说这个单元将指向这个新的边记录。 1234567virtual void insert ( Te const&amp; edge, int w, int i, int j ) &#123; //插入权重为w的边e = (i, j) if ( exists ( i, j ) ) return; //确保该边尚不存在 E[i][j] = new Edge&lt;Te&gt; ( edge, w ); //创建新边 e++; //更新边计数 V[i].outDegree++; //更新关联顶点i的出度 V[j].inDegree++; //更新关联顶点i的入数&#125; 边删除： 不妨假设从顶点 i 通往顶点 j 之间存在一条边，因此在邻接矩阵中对应的那一项就非空，而且这一项将指向一个对应的边记录。为了删除这条边，只需将这条边对应的记录释放并且归还给系统，然后令在邻接矩阵中对应于这一项的引用指向空。 12345678virtual Te remove ( int i, int j ) &#123; //删除顶点i和j之间的联边（exists(i, j)） Te eBak = edge ( i, j ); delete E[i][j]; E[i][j] = NULL; //备份后删除边记录 e--; V[i].outDegree--; V[j].inDegree--; //更新边计数与关联顶点的度数 return eBak; //返回被删除边的信息&#125; 2.7.顶点插入与删除顶点的插入与删除相对于边的操作要更为复杂，原因在于在此前的边操作中整个矩阵的规模并不会发生变化，而顶点的插入以及稍后的删除则会改变。为了在其中引入一个新的顶点，首先要将邻接矩阵中已有的各行分别向后扩展一个单元，即增加一列；接下来针对新引入的顶点还需在邻接矩阵中增加对应的一行；当然还需在第一级的边表中增加一个相应地单元用来指示或者说记录新引入的行向量；最后对应于这个新引入的顶点还需要在顶点向量中加入一个新的对应元素。 这样的四个步骤可以实现为这段代码： 12345virtual int insert ( Tv const&amp; vertex ) &#123; //插入顶点，返回编号 for ( int j = 0; j &lt; n; j++ ) E[j].insert ( NULL ); n++; //各顶点预留一条潜在的关联边 E.insert ( Vector&lt;Edge&lt;Te&gt;*&gt; ( n, n, ( Edge&lt;Te&gt;* ) NULL ) ); //创建新顶点对应的边向量 return V.insert ( Vertex&lt;Tv&gt; ( vertex ) ); //顶点向量增加一个顶点&#125; 顶点删除就是上述步骤的逆过程。 1234567891011virtual Tv remove ( int i ) &#123; //删除第i个顶点及其关联边（0 &lt;= i &lt; n） for ( int j = 0; j &lt; n; j++ ) //所有出边 if ( exists ( i, j ) ) &#123; delete E[i][j]; V[j].inDegree--; e--; &#125; //逐条删除 E.remove ( i ); n--; //删除第i行 Tv vBak = vertex ( i ); V.remove ( i ); //删除顶点i for ( int j = 0; j &lt; n; j++ ) //所有入边 if ( Edge&lt;Te&gt; * x = E[j].remove ( i ) ) &#123; delete x; V[j].outDegree--; e--; &#125; //逐条删除 return vBak; //返回被删除顶点的信息&#125; 2.8.优缺点邻接矩阵的优点有： 直观，易于理解和实现 适用范围广泛：digrah / network / cyclic / … 尤其适用于稠密图（dense graph） 判断两点之间是否存在联边：$O(1)$ 获取顶点的（出/入）度数：$O(1)$ 添加、删除边后更新度数：$O(1)$ 扩展性（scalability）： 得益于Vector良好的空间控制策略，空间溢出等情况可“透明地”予以处理 邻接矩阵的缺点则是： 空间利用率，它始终是需要$\\Theta(n^2)$空间，与边数无关 在实际问题中的图通常不会有$n^2$级的边数，不妨考虑下平面图（planar graph），即可嵌入平面的图，其中不相邻的边不相交。根据欧拉推导的公式：对于所有的平面图有，$v-e+f-c=1$，各字母分别表示顶点数、边数、区域面片数、连通域数。因此平面图的边数有：$e\\le 3\\times n-6=O(n) \\ll n^2$，此时空间利用率$\\approx 1/n$。而对于一般的稀疏图（sparse graph），空间利用率同样很低，因此可以采用压缩存储技术予以改进。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"Cpp基础（12）文件操作和模板","slug":"Cpp基础（12）文件操作和模板","date":"2020-02-22T08:51:42.000Z","updated":"2020-02-25T08:51:42.000Z","comments":true,"path":"2020/02/22/Cpp基础（12）文件操作和模板/","link":"","permalink":"http://nekomoon404.github.io/2020/02/22/Cpp%E5%9F%BA%E7%A1%80%EF%BC%8812%EF%BC%89%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%92%8C%E6%A8%A1%E6%9D%BF/","excerpt":"","text":"1.文件操作本节介绍如何利用C++语言来处理文件的一些操作，我们知道所有的数据在计算机上保存的时候都是具有一定的层次化的结构的，一个数据在计算机实质上被保存的就是一个个0,1的比特位，它是每位这样存放的。但是去处理每个比特位的话那么可能在很多时候这个数据在构建时就会变得非常的繁琐，并且具有很强的不规律性，所以我们进一步把这8个比特位构成的称之为字节。那么每一个byte它对应描述了一定的内容，而这些各个字节组成的一些具体的内容又称之为域或者记录。 数据的层次： 位 bit 字节 byte 域/记录 把所有记录顺序地写入一个文件 $\\to$ 顺序文件 1.1.文件和流 顺序文件：一个有限字符构成的顺序字符流 C++标准库中：ifstream，ofstream和fstream共3个类 $\\to$ 用于文件操作，统称为文件流类。 ifstream用于将文件读取，从文件中读取数据 ofstream用于向文件中写入数据 fstream即可以从文件中读取数据，又可以向文件中写入数据 （图中箭头表示派生） 1.2.文件操作 使用/创建文件的基本流程： 1）打开文件：通过指定文件名，建立文件和文件流对象的关联；指明文件的使用方式； 2）读/写文件：利用读/写指针进行相应的操作 3）关闭文件 1.3.建立顺序文件12#include&lt;fstream&gt; //包含头文件ofstream outFile(\"clients.dat\", ios::out | ios:: binary); //打开文件 也可以先创建ofstream对象，再用open函数打开； 一般要判断打开是否成功； 文件名可以给出绝对路径，也可以给相对路径； 没有交代路径信息，就是在当前文件夹下找文件 1234#include&lt;fstream&gt;ofstream fout;fout.open(\"test.out\", ios::out | ios::binary);if(!fout) &#123; cerr &lt;&lt; \"File open error!\" &lt;&lt; endl;&#125; 1.4.文件的读写指针 对于输入文件，有一个读指针； 对于输出文件，有一个写指针； 对于输入输出文件，有一个读写指针； 标识文件操作的当前位置，该指针在哪里 $\\to$ 读写操作就在哪里进行 写指针：（location可以为负值） 1234567ofstream fout(\"a1.out\", ios::app);long location = fout.tellp(); //取得写指针的位置location = 10L;fout.seekp(location); //将写指针移动到第10个字节处fout.seekp(location, ios::beg); //从头数locationfout.seekp(location, ios::cur); //从当前位置数locationfout.seekp(location, ios::end); //从尾部数location 读指针： 1234567ifstream fin(\"a1.out\", ios::in);long location = fin.tellg(); //取得写指针的位置location = 10L;fin.seekg(location); //将写指针移动到第10个字节处fin.seekg(location, ios::beg); //从头数locationfin.seekg(location, ios::cur); //从当前位置数locationfin.seekg(location, ios::end); //从尾部数location 1.5.二进制文件读写123456int x=10;fout.seekp(20,ios::beg);fout.write( (const char*)(&amp;x), sizeof(int) );fin.seekg(0,ios::beg);fin.read( (char*)(&amp;x), sizeof(int) ); 二进制文件读写，直接读/写二进制数据，记事本看未必正确 例子：从键盘输入几个学生的姓名和成绩，并以二进制文件形式存起来 1234567891011121314151617181920#include&lt;iostream&gt;#include&lt;fstream&gt;#include&lt;cstring&gt;using namespace std;class Student &#123;public: char szName[20]; int nScore;&#125;;int main() &#123; Student s; ofstream OutFile(\"D:\\\\tmp\\\\student.dat\", ios::out | ios::binary); while (cin &gt;&gt; s.szName &gt;&gt; s.nScore) &#123; if (stricmp(s.szName, \"exit\") == 0) //名字为exit则结束 break; OutFile.write((char*)&amp;s, sizeof(s)); &#125; OutFile.close(); return 0;&#125; 对二进制文件来进行读写的时候最好是能够保持一致，即以二进制的形式去写入就以二进制的形式来读出。例子2： 123456789101112131415161718192021222324#include&lt;iostream&gt;#include&lt;fstream&gt;#include&lt;cstring&gt;using namespace std;class Student &#123;public: char szName[20]; int nScore;&#125;;int main() &#123; Student s; ifstream inFile(\"student.dat\", ios::in | ios::binary); if (!inFile) &#123; cout &lt;&lt; \"error\" &lt;&lt; endl; return 0; &#125; while (inFile.read((char*)&amp;s, sizeof(s))) &#123; int nReadedBytes = inFile.gcount(); cout &lt;&lt; s.szName &lt;&lt; \" \" &lt;&lt; s.nScore &lt;&lt; endl; &#125; inFile.close(); return 0;&#125; 在刚才的两个例子当中介绍了如何对一个文件进行二进制形式的一个写入，或者是对一个文件具体以二进制的形式来进行读出，而在很多时候我们通常会对一个文件同时要进行读和写的操作。 例子3：将student.dat文件的Jane的名字改成Mike 12345678910111213141516171819202122232425#include&lt;iostream&gt;#include&lt;fstream&gt;#include&lt;cstring&gt;using namespace std;class Student &#123;public: char szName[20]; int nScore;&#125;;int main() &#123; Student s; fstream iofile(\"D:\\\\tmp\\\\student.dat\", ios::in | ios::out | ios::binary); if (!iofile) &#123; cout &lt;&lt; \"error\"; return 0; &#125; iofile.seekp(2 * sizeof(s), ios::beg); //定位写指针到第三个记录 iofile.write(\"Mike\", strlen(\"Mike\") + 1); iofile.seekg(0, ios::beg); //定位读指针到开头 while (iofile.read((char*)&amp;s, sizeof(s))) cout &lt;&lt; s.szName &lt;&lt; \" \" &lt;&lt; s.nScore &lt;&lt; endl; iofile.close(); return 0;&#125; 显示关闭文件（写或读之后都要显示关闭文件） 12345ifstream fin(\"test.dat\", ios::in);fin.close();ofstream fout(\"test.dat\",ios::out);fout.close(); 例子4：文件拷贝，用法示例：mycopy src.dat dest.dat，即将src.dat拷贝到dest.at，如果dest.dat原来就有，则原来的文件会被覆盖。 12345678910111213141516171819202122232425262728#include&lt;iostream&gt;#include&lt;fstream&gt;using namespace std;//main函数的两个参数分别记录命令行操作相应的参数的个数以及参数对应的字符串int main(int argc, char* argv[]) &#123; if (argc != 3) &#123; cout &lt;&lt; \"File name missing!\" &lt;&lt; endl; return 0; &#125; ifstream inFile(argv[1], ios::binary | ios::in); //打开文件用于读 if (!inFile) &#123; cout &lt;&lt; \"Source file open error.\" &lt;&lt; endl; return 0; &#125; ofstream outFile(argv[2], ios::binary | ios::out); //打开文件用于写 if (!outFile) &#123; cout &lt;&lt; \"New file open error.\" &lt;&lt; endl; inFile.close(); //打开的文件一定要关闭 return 0; &#125; char c; //仅做示例，实际文件拷贝时不会是逐字节拷贝 while (inFile.get(c)) //每次读取一个字符 outFile.put(c); //每次写入一个字符 outFile.close(); inFile.close(); return 0;&#125; 2.函数模板通过模板的使用就正式开始了泛型程序程序设计这样一个模块的学习。 泛型程序设计（Generic Programming） 算法实现时不指定具体要操作的数据的类型； 泛型—算法实现一遍 $\\to$ 适用于多种数据结构 优势：减少重复代码的编写 大量编写模板，使用模板的程序设计： 函数模板 类模板 函数模板的形式： 1234template&lt;class 类型参数1, class 类型参数2,...&gt; 返回值类型 模板名(形参表)&#123; 函数体&#125; 例子： 123456789101112template&lt;class T&gt; void Swap(T &amp;x, T &amp;y)&#123; T tmp = x; x = y; y = tmp;&#125;int main()&#123; int n = 1, m = 2; Swap(n, m); //编译器自动生成 void Swap(int &amp;, int &amp;)函数 double f = 1.2, g = 2.3; Swap(f, g); //编译器自动生成 void Swap(double &amp;, double &amp;)函数 return 0;&#125; 函数模板中可以有不止一个类型参数 12345template&lt;class T1, class T2&gt; T2 print(T1 arg1. T2 arg2)&#123; cout&lt;&lt;arg1&lt;&lt;\" \"&lt;&lt;arg2&lt;&lt;endl; return arg2;&#125; 函数模板可以重载，只要它们的形参表不同即可，例如下面两个模板可以同时存在： 12345678template&lt;class T1, class T2&gt; void print(T1 arg1. T2 arg2)&#123; cout&lt;&lt;arg1&lt;&lt;\" \"&lt;&lt;arg2&lt;&lt;endl;&#125;template&lt;class T&gt; void print(T arg1. T arg2)&#123; cout&lt;&lt;arg1&lt;&lt;\" \"&lt;&lt;arg2&lt;&lt;endl;&#125; C++编译器遵循以下优先顺序： Step1：先找参数完全匹配的普通函数（非由模板实例化而得的函数）； Step2：再找参数完全匹配的模板函数； Step3：再找实参经过自动类型转换后能够匹配的普通函数； Step4：上面的都找不到，则报错。 要注意赋值兼容原则引起函数模板中类型参数的二义性 12345678template&lt;class T1, class T2&gt; void print(T1 arg1. T2 arg2)&#123; cout&lt;&lt;arg1&lt;&lt;\" \"&lt;&lt;arg2&lt;&lt;endl;&#125;......print(5, 7); //ok, replace T with intprint(5.8, 7.8); //ok, replace T with doubleprint(5, 7.8); //error, replace T with int or double? 二义性 函数模板的类型参数可以用于函数模板的局部变量声明，也可以用于声明函数模板的返回值。 3.类模板 类模板：在定义类的时候给它一个/多个参数，这个/些参数表示不同的数据类型；在调用类模板时，指定参数，由编译系统根据参数提供的数据类型自动产生相应的模板类。 1234template&lt;类型参数表&gt; class 类模板名&#123; 成员函数和成员变量&#125;; 类模板里的成员函数，如在类模板外面定义时： 12345template&lt;型参数表&gt;返回值类型 类模板名&lt;类型参数名列表&gt;::成员函数名(参数表)&#123; ......&#125; 3.1类模板的定义 用类模板定义对象的写法如下： 1类模板名&lt;真实类型参数表&gt; 对象名(构造函数实际参数表); 如果类模板有无参构造函数，那么也可以只写： 1类模板名&lt;真实类型参数表&gt; 对象名; 下面是一个实例： 123456789101112131415161718192021222324#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;template&lt;class T1,class T2&gt;class Pair &#123;public: T1 key; T2 value; Pair(T1 k, T2 v) :key(k), value(v) &#123; &#125;; bool operator&lt;(const Pair&lt;T1, T2&gt; &amp; p) const;&#125;;template&lt;class T1, class T2&gt;bool Pair&lt;T1,T2&gt;::operator&lt;(const Pair&lt;T1, T2&gt; &amp; p) const&#123; return key &lt; p.key;&#125;int main() &#123; Pair&lt;string, int&gt;student(\"Tom\", 19); //实例化出一个类Pair&lt;string,int&gt; cout &lt;&lt; student.key &lt;&lt; \" \" &lt;&lt; student.value; return 0;&#125; 输出结果：Tom 19 3.2.使用类模板声明对象 编译器由类模板生成类的过程叫类模板的实例化 编译器自动用具体的数据类型 $\\to$ 替换类模板中的类型参数，生成模板类的代码 由类模板实例化得到的类叫模板类 为类型参数指定的数据类型不同，得到的模板类不同 同一个类模板的两个模板类是不兼容的 123Pair&lt;string,int&gt; *p;Pair&lt;string,double&gt; a;p = &amp; a; //Wrong 3.3.函数模板作为类模板成员1234567891011template&lt;class T&gt;class A&#123;public: template&lt;class T2&gt; void Func(T2 t)&#123; cout &lt;&lt; t; &#125; //若改为template&lt;class T&gt;将报错&#125;;int main()&#123; A&lt;int&gt; a; a.Func('K'); //成员函数模板Func被实例化 return 0;&#125; 程序输出：K 3.4.类模板与非类型参数 类模板的参数声明中可以包括非类型参数，如template&lt;class T, int elementsNumber&gt; 非类型参数：用来说明类模板中的属性，且非类型参数必须实例化，实参必须是编译时常量表达式 类型参数：用来说明类模板中的属性类型，成员操作的参数类型和返回值类型 123456789template&lt;class T,int size&gt;class Array&#123; T array[size];public: void Print()&#123; for(int i=0; i &lt; size; ++i) cout &lt;&lt; array[i] &lt;&lt; endl; &#125;&#125;; 注意：由不同的参数生成的模板类是不同的，Array&lt;int,40&gt;和Array&lt;int,50&gt;完全是两个类，这两个类的对象之间不能互相赋值，Array&lt;int,40&gt;实例化后得到的模板类是一个拥有一个固定长度为5的int型数组的成员变量的类。 3.5.类模板与继承 类模板派生出类模板 模板类（即类模板中类型/非类型参数实例化后的类）派生出类模板 普通类派生出类模板 模板类派生出普通类 3.6.类模板与静态成员类模板中可以定义静态成员： 从该类模板实例化得到的模板类的所有对象都包含同样的静态成员 例如，声明一个类模板A，有一个静态成员变量count用于计数，有一个静态成员函数printCount()用于输出计数结果。由类模板 A 实例化的两个模板类 A&lt;int&gt; 和 A&lt;double&gt; 各自拥有一个名为 count 的静态成员变量，和一个名为 printCount() 的静态成员函数。A&lt;int&gt; 和 A&lt;double&gt; 是不同的模板类，不能共享静态成员变量 count，因而需要分别初始化。在声明了 1 个 A&lt;int&gt; 对象和 2 个 A&lt;double&gt; 对象之后，A&lt;int&gt;::count 的值变为 1，A&lt;double&gt;::count 的值变为 2 1234567891011121314151617181920212223#include &lt;iostream&gt;using namespace std;template &lt;class T&gt;class A &#123; static int count;public: A() &#123; ++count; &#125; ~A() &#123; --count; &#125; A(const A &amp;a) &#123; ++count; &#125; static void printCount() &#123; cout &lt;&lt; count &lt;&lt; endl; &#125;&#125;;template&lt;&gt; int A&lt;int&gt;::count = 0;template&lt;&gt; int A&lt;double&gt;::count = 0;int main() &#123; A&lt;int&gt; ia; A&lt;double&gt; da1, da2; ia.printCount(); // 输出 1 A&lt;double&gt;::printCount(); // 输出 2 return 0;&#125; 3.7.类模板与友元类模板与友元大致分为四种情况： 函数、类、类的成员函数作为类模板的友元 例如，下面的代码中声明了一个类模板 C，可以将普通函数 func1() 声明为 C 的友元，使得 func1() 可以访问 val；还可以将普通类 A 声明为类模板 C 的友元；也可以将普通类 B 的成员函数 func2() 声明为类模板 C 的友元。 123456789101112131415class A &#123; &#125;;class B &#123; public: void func2(); &#125;;template &lt;class T&gt;class C &#123; T val; friend void func1(); friend class A; friend void B::func2();public: C(T _val) : val(_val) &#123; &#125;&#125;;C&lt;int&gt; c1(4);C&lt;double&gt; c2(2.8);void func1() &#123; cout &lt;&lt; c1.val &lt;&lt; endl; &#125;void B::func2() &#123; cout &lt;&lt; c2.val &lt;&lt; endl; &#125; 函数模板作为类模板的友元 例如，下面声明了重载流运算符的两个函数模板为类模板 Array 的友元，两个函数模板的具体实现如下，作为友元可以访问 size 和 ptrElement ，分别用于依次读入和输出数组中的每个元素。声明 Array&lt;int&gt; 对象 a 时，自动生成了类模板 Array&lt;int&gt;，并根据两个函数模板自动重载了对应的流运算符：istream &amp;operator &gt;&gt; (istream &amp;, Array&lt;int&gt; &amp;)ostream &amp;operator &lt;&lt; (ostream &amp;, const Array&lt;int&gt; &amp;) 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;using namespace std;template &lt;class T1&gt;class Array &#123; T1 *ptrElement; int size;public: Array(int _size = 0) : size(_size) &#123; ptrElement = new T1[_size]; &#125; ~Array() &#123; delete[] ptrElement; &#125; template&lt;class T2&gt; friend istream &amp;operator &gt;&gt; (istream &amp;, Array&lt;T2&gt; &amp;); template&lt;class T3&gt; friend ostream &amp;operator &lt;&lt; (ostream &amp;, const Array&lt;T3&gt; &amp;);&#125;;template&lt;class T2&gt;istream &amp;operator &gt;&gt; (istream &amp;s, Array&lt;T2&gt; &amp;a) &#123; for (int i = 0; i &lt; a.size; ++i) &#123; s &gt;&gt; *(a.ptrElement + i); &#125; return s;&#125;template&lt;class T3&gt;ostream &amp;operator &lt;&lt; (ostream &amp;s, const Array&lt;T3&gt; &amp;a) &#123; for (int i = 0; i &lt; a.size; ++i) &#123; s &lt;&lt; *(a.ptrElement + i) &lt;&lt; \" \"; &#125; return s;&#125;int main()&#123; Array&lt;int&gt; a(5); cin &gt;&gt; a; // 若输入 1 2 3 4 5 cout &lt;&lt; a; // 则输出 1 2 3 4 5 return 0;&#125; 函数模板作为类的友元 例如下面的普通类 A 和 B 都将 print() 函数模板声明为自己的友元，根据函数模板自动生成 print(const A &amp;) 和 print(const B&amp;) 来分别输出类 A 的 int 型私有成员变量 v 和类 B 的 double 型私有成员变量 v。 123456789101112131415161718192021222324252627class A &#123; int v;public: A(int _v = 3) : v(_v) &#123; &#125; template &lt;class T&gt; friend void print(const T &amp;);&#125;;class B &#123; double v; char *buf; void func() &#123; &#125;public: B(double _v = 2.1) : v(_v) &#123; &#125; template &lt;class T&gt; friend void print(const T &amp;);&#125;;class A &#123; int v; ... &#125;;class B &#123; double v; ... &#125;;template &lt;class T&gt;void print(const T &amp;p) &#123; cout &lt;&lt; p.v &lt;&lt; endl; &#125;int main() &#123; A a; print(a); // 输出 3 B b; print(b); // 输出 2.1 return 0;&#125; 类模板作为类模板的友元 例如下面的类模板 B 将类模板 A 声明为了自己的友元，在声明 b 对象时，自动生成模板类 B&lt;int&gt;；在声明 a 对象时，则自动生成模板类 A&lt; B&lt;int&gt; &gt;。由于 B&lt;int&gt; 声明了模板类 A 类为自己的友元，因此 A&lt; B&lt;int&gt; &gt; 对象 a 可以通过 func() 访问并输出 B&lt;int&gt; 对象 b 的私有成员变量 v。 12345678910111213141516171819202122#include &lt;iostream&gt;using namespace std;template &lt;class T&gt;class A &#123;public: void func(const T &amp;p) &#123; cout &lt;&lt; p.v; &#125;&#125;;template &lt;class T&gt;class B &#123; T v;public: B(T _v) : v(_v) &#123; &#125; template &lt;class T2&gt; friend class A; // 把类模板 A 声明为友元&#125;;int main() &#123; B&lt;int&gt; b(5); A&lt; B&lt;int&gt; &gt; a; // 用 B&lt;int&gt; 替换 A 中的 T a.func(b); // 输出 5 return 0;&#125; 类模板的一个综合示例： 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;template&lt;class T, int size&gt;class A&#123; T array[size];public: A(T* array_) &#123; for (int i = 0; i &lt; size; i++) &#123; array[i] = array_[i]; &#125; &#125; T&amp; operator[](int index) &#123; return array[index]; &#125; T sum() &#123; T s = array[0]; for (int i = 1; i &lt; size; i++) &#123; s += array[i]; &#125; return s; &#125;&#125;;int main() &#123; int b1[10] = &#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;; A&lt;int, 10&gt; a1 = b1; cout &lt;&lt; a1[2] &lt;&lt; endl; //输出：2 double b2[5] = &#123;4.2, 0.0, 3.1, 2.7, 5.2&#125;; A&lt;double, 5&gt; a2 = b2; cout &lt;&lt; a2.sum() &lt;&lt; endl; //输出：15.2 string b3[4] = &#123;\"Hello\", \", \", \"world\", \"!\"&#125;; A&lt;string, 4&gt; a3 = b3; cout &lt;&lt; a3.sum() &lt;&lt; endl; //输出：Hello, world! return 0; &#125; 4. string类string类本身是一个模板类，而它本身其实是由一个类模板实例化得到的，string类是一个非常好用的类库，在很多字符串处理的场合都会用到string类。 与string类有关的一些函数： 4.1.基础操作 string类是一个模板类，它的定义为：typedef basic_string&lt;char&gt;string; 使用string类要包含头文件：#include&lt;string&gt; string对象的初始化，string 类提供了多种构造函数： 可以根据字符串常量（const char *）来构造 string 对象 也可以指定字符串长度和填充的字符来构造 string 对象 123string s1(\"Hello\"); //一个参数的构造函数string S2(8,'x'); //两个参数的构造函数string month = \"March\"; 不提供以字符和整数为参数的构造函数，以下都是错误的初始化方法： 1234string error = 'c'; //错string error2('u'); //错string error3 = 22; //错string error4(8); //错 可以将字符赋值给string对象 12string s;s = 'n'; 构造的string对象太长而无法表达 $\\to$ 会抛出length_error异常 string对象的长度用成员函数length()读取： 12string s(\"hello\");cout &lt;&lt; s.length() &lt;&lt; endl; string支持流读取运算符： 12string stringObject;cin &gt;&gt; stringObject; string支持getline函数 12string s;getline(cin,s); 4.2. string的赋值和连接 用’=‘赋值 12string s1(\"cat\"), s2;s2 = s1; 用assign成员函数复制，也可以部分复制 123string s1(\"catpig\"), s2, s3;s2.assign(s1);s3.assign(s1,1,3); //从s1中下标为1的字符开始复制3个字符给s3 单个字符复制：s2[5] = s1[3] = &#39;a&#39;; 逐个访问string对象中的字符 123string s1(\"Hello\");for(int i = 0; i &lt; s.length(); i++) cout &lt;&lt; s1.at(i) &lt;&lt; endl; 成员函数at会做范围检查，如果超出范围，会抛出out_of_range异常，而下标运算符不做范围检查 用 + 运算符连接字符串 123string s1(\"good\"), s2(\"morning\");s1 += s2;cout &lt;&lt; s1; 用成员函数append连接字符串 123456string s1(\"good\"), s2(\"morning\");s1.append(s2);cout &lt;&lt; s1; //输出goodmornings2.append(s1,3,s1.size()); //s1.size()返回s1的字符数，//取下标从3开始，s1.size()个字符，如果字符串内没有足够字符，则复制到字符串的最后一个字符cout &lt;&lt; s2; //输出morningd 子串：成员函数substr() 123string s1(\"hello world\")s2 = s1.substr(4,5); //从下标4开始5个字符cout &lt;&lt; s2 &lt;&lt; endl; //输出：o wor 4.3.比较string 可以用关系运算符比较string对象的大小，==，&gt;，&gt;=，&lt;，&lt;=，!=，利用字符在字典中的顺序进行比较，且对字母的大小写敏感，返回值都是bool类型，成立返回true，否则返回false。 也可以使用成员函数 compare(string) 比较字符串大小，返回值为 int 型 若当前对象比被比较的对象（即括号中的对象）大，则返回 1；若相等则返回 0 ；若比被比较的对象小，则返回 -1。 比较原则为逐字符比较；若前面的字符全部相等，则较长的字符串大。 也可以使用成员函数 compare(int, int, string) 或 compare(int, int, string, int, int) 比较两个子串的大小，返回值为 int 型。 比较规则与 compare(string) 相同 使用时依次指定子串的起始位置和长度即可 1234567891011int main() &#123; string s1(\"Alice\"), s2(\"Bob\"); string s3(\"Alice and Bob\"); cout &lt;&lt; s2.compare(s1) &lt;&lt; endl; // 'B' &gt; 'A'，\"Bob\" 比 \"Alice\" 大，输出 1 cout &lt;&lt; s1.compare(s3) &lt;&lt; endl; // s1 是 s3 的一个前缀，s3 比 s1 大，输出 -1 cout &lt;&lt; s2.compare(0, 3, s3, 10, 3); // 输出 0 // (s2[0..2] = \"Bob\") == (s3[10..12] = \"Bob\") return 0;&#125; 4.4.寻找string中的字符 成员函数find()，如下例在s1中从前向后查找lo第一次出现的地方，如果找到，返回lo开始的位置，即l所在的位置下标；如果找不到，返回string::nps(string中定义的静态常量) 12string s1&#123;\"hello world\"&#125;;s1.find(\"lo\"); 成员函数rfind()，如下例在s1中从后向前查找lo第一次出现的地方，如果找到，返回lo开始的位置，即l所在的位置下标；如果找不到，返回string::nps 12string s1&#123;\"hello world\"&#125;;s1.rfind(\"lo\"); 成员函数find_first_of()，如下例在s1中从前向后查找abcd中任何一个字符第一次出现的地方，如果找到，返回找到字母的位置；如果找不到，返回string::nps 12string s1&#123;\"hello world\"&#125;;s1.find_first_of(\"abcd\"); 成员函数find_last_of()，如下例在s1中从前向后查找abcd中任何一个字符最后一次出现的地方，如果找到，返回找到字母的位置；如果找不到，返回string::nps 12string s1&#123;\"hello world\"&#125;;s1.find_last_of(\"abcd\"); 成员函数find_first_not_of()，如下例在s1中从前向后查找不在abcd中的字符第一次出现的地方，如果找到，返回找到字符的位置；如果找不到，返回string::nps 12string s1&#123;\"hello world\"&#125;;s1.find_first_not_of(\"abcd\"); 成员函数find_last_not_of()，如下例在s1中从后向前查找不在abcd中的字符第一次出现的地方，如果找到，返回找到字符的位置；如果找不到，返回string::nps 12string s1&#123;\"hello world\"&#125;;s1.find_last_not_of(\"abcd\"); 4.5.替换string的字符 成员函数erase()，删除字符串中指定位置及以后的内容；缺省值为 0 ，即清除字符串的所有内容（变成一个空串 “” ） 12345string s1(\"hello world\");s1.erase(5); //去掉下标5及之后的字符cout &lt;&lt; s1;cout &lt;&lt; s1.length(); cout &lt;&lt; s1.size(); //输出：hello55 成员函数find() 1234string s1(\"hello worlld\");cout &lt;&lt; s1.find(\"ll\", 1) &lt;&lt; endl;cout &lt;&lt; s1.find(\"ll\", 2) &lt;&lt; endl;cout &lt;&lt; s1.find(\"ll\", 3) &lt;&lt; endl; //分别从下标1,2,3开始查找ll，输出：2;2;9 成员函数replace(int, int, string)将字符串中的某一段替换为指定子串，前两个参数指定需要替换的起始位置和长度，第三个参数是进行替换的子串 123456string s1(\"hello world\");s1.replace(2,3,\"haha\"); //将s1中从下标2开始的3个字符换成\"haha\"cout &lt;&lt; s1; //输出hehaha worlds1.replace(2,3,\"haha\",1,2); //将s1中从下标2开始的3个字符换成\"haha\"中下标1开始的2个字符cout &lt;&lt; s1; //输出heha world 4.6.在string中插入字符 成员函数insert() 123456string s1(\"hello world\");string s2(\"show insert\");s1.insert(5,s2); //将s2插入s1下标5的位置cout &lt;&lt; s1 &lt;&lt; endl; //输出：helloshow insert worlds1.insert(2,s2,5,3); //将s2中下标5开始的3个字符插入s1下标2的位置cout &lt;&lt; s1 &lt;&lt; endl; //输出：heinslloshow insert world 可以使用成员函数 push_back(char) 在字符串末尾增加一个字符；C++11 标准中还增加了成员函数 pop_back() ，可以用来删除字符串中的最后一个字符。 12345string s = \"\";for (int i = 0; i &lt; 7; ++i) &#123; s.push_back('A' + i);&#125;cout &lt;&lt; s &lt;&lt; endl; // 输出 ABCDEFG 4.7.将string转换成C语言式 成员函数c_str()，返回传统的const char*类型字符串，且该字符串以’\\0‘结尾；data()也可以，但不会在字符串末尾增加结束符。 1234string s1(\"hello world\");printf(\"%s\\n\",s1.c_str()); //输出：hello worldconst char *p = s1.data();printf(\"%s\\n\", p); // 输出 Hello world C++11 中提供了 std::to_string() 函数，可以将 int / unsigned int / float / double 等数值转换为 string 对象，效果与 C 语言中的 %i / %u / %f / %lf 等相同 123456789101112#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;int main() &#123; int x = 42; double p = 3.14; string s = to_string(x); s += \" \" + to_string(p); cout &lt;&lt; s &lt;&lt; endl; // 输出 42 3.140000 return 0;&#125;","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"数据结构与算法（10）二叉树的遍历","slug":"数据结构与算法（10）二叉树的遍历","date":"2020-02-21T03:42:13.000Z","updated":"2020-02-22T04:27:41.943Z","comments":true,"path":"2020/02/21/数据结构与算法（10）二叉树的遍历/","link":"","permalink":"http://nekomoon404.github.io/2020/02/21/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%8810%EF%BC%89%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86/","excerpt":"","text":"遍历：按照某种次序访问树中各节点，每个节点被访问恰好一次。 $T=V\\cup L \\cup R=\\{root\\} \\cup L_ subtree(T) \\cup R _ subtree(T)$ 遍历结果 ~ 遍历过程 ~ 遍历次序 ~ 遍历策略 先序遍历：V | L | R 中序遍历：L | V | R 后序遍历：L | R | V 层次遍历：自上为下，先左后右 1.先序遍历1.1递归以上三种典型的遍历策略都不难实现，因为它们的定义本身就是递归式的，以先序遍历为例，只需四句就可以实现。 1234567template &lt;typename T, typename VST&gt; //元素类型、操作器void travPre_R(BinNodePosi(T) x, VST&amp; visit) &#123; //二叉树先序遍历算法（递归版） if (!x) return; visit(x-&gt;data); travPre_R(x-&gt;lc, visit); travPre_R(x-&gt;rc, visit);&#125; //T(n) = O(1) + T(a) + T(n-a-1) = O(n) 这个算法的时间复杂度是线性的，即$O(n)$，然而这只具有渐近的意义。在实际的运行过程中，因为递归程序的实现机制，并不可能做到针对具体的问题来量体裁衣，而只能采用通用的方法。在运行栈中尽管每一个递归实例都的确只对应于一帧，但是因为它们必须具有通用格式，所以并不能做到足够的小。而针对于具体的问题，只要我们能够进行精巧的设计，完全是可以使得每一帧做到足够小的，尽管从big O的意义上讲，这两种策略所对应的每一帧都可以认为是常数，但是这种常数的差异实际上是非常巨大的。 因此作为树算法的一个重要基石，遍历算法非常有必要从递归形式改写为迭代形式，同时经过这样的改写之后，我们也可以对整个遍历算法的过程以及原理获得更加深刻的认识。稍加观察不难发现此处的两句递归调用都非常类似于尾递归，其特征是递归调用出现在整个递归实例体的尾部，这种递归是非常容易化解为迭代形式的，为此我们只需引入一个栈。 1.2.迭代（版本1）改写之后的第一个跌打版本，如这段代码所示，作为初始化取一个栈s用以存放树节点的位置，即它们的引用。首先将当前的树根x推入栈中，以下进入一个主体的循环，每一次弹出当前的节点并且随即对它进行访问，此后如果当前这个节点拥有右孩子就将右孩子推入栈中，如果有左孩子 那么左孩子也会随后入栈，此后整个循环又进入下一步迭代直到整个栈变空。 123456789template &lt;typename T, typename VST&gt; //元素类型、操作器void travPre_I1(BinNodePosi(T) x, VST&amp; visit) &#123; //二叉树先序遍历算法（迭代版#1） Stack&lt;BinNodePosi(T)&gt; S; //辅助栈 if (x) S.push(x); //根节点入栈 while (!S.empty()) &#123; //在栈变空之前反复循环 x = S.pop(); visit(x-&gt;data); //弹出并访问当前节点，其非空孩子的入栈次序为先右后左 if (HasRChild(*x)) S.push(x-&gt;rc); if (HasLChild(*x)) S.push(x-&gt;lc); &#125;&#125; 需要注意的是：左右孩子的入栈次序是先右后左，这是因为包括先序遍历在内的所有遍历，都先遍历左子树再去遍历右子树，在这个算法模式中既然每个节点都是在被弹出栈的时刻才接受访问，所以根据栈后进先出的特性，自然应该将希望后出栈的右子树先入栈了。 下面是一个实例： 正确性： 无遗落：每个节点都会被访问到 归纳假设：若深度为d的节点都能被正确访问到，则深度为d+1的也是 根先：对于任一子树，根被访问后才会访问其他节点 只需注意到：若u是v的真祖先，则u必先于v被访问到 左先右后：同一节点的左子树，限于右子树被访问 效率：$O(n)$ 每步迭代，都有一个节点出栈并被访问； 每个节点入/出栈一次仅且一次； 每步迭代只需$O(1)$时间。 可以看到算法所输出的节点序列恰好就是我们所希望得到的先序遍历序列，第一个迭代版算法非常简明，然而遗憾的是这种算法策略并不容易直接推广到此后要研究的中序遍历和后序遍历算法，因此我们或许应该另辟蹊径寻找其它等效的策略。 1.3.迭代（版本2）不妨从一个规模略大同时更具一般性的例子入手： 可以发现这样一个规律：一旦树根节点接过控制权并接受访问，接下来被访问的就是它的左孩子以及左孩子的左孩子，以及同样地，当不能下去的时候才会进行一次新的转移，而每转移到一个具体的局部，做的事情都是尝试着沿着这样的一个左孩子的分支不断地下行。 对于任何一棵子树，都将起始于树根的接下来总是沿着左侧孩子分支不断下行的这样一条链称作是当前这棵子树的左侧链，而这个算法就是沿着这个左侧链逐渐展开。 沿着左侧分支：各节点与其右孩子（可能为空）一一对应； 从宏观上，整个遍历过程可划分为：自上而下对左侧分支的访问，及随后自下而上对一系列右子树的遍历； 不同右子树的遍历相互独立，自成一个子任务。 新版本的迭代算符首先需要实现一个标准的例程visitAlongLeftBranch，它的任务就是来实现从根节点开始沿着left branch 不断下行，依次访问沿途所有节点的这样一个过程。 这个主算法则是反复地在每一个局部调用visitAlongLeftBranch这个例程来实现。 12345678910111213141516171819//从当前节点出发，沿左分支不断深入，直至没有左分支的节点；沿途节点遇到后立即访问template &lt;typename T, typename VST&gt; //元素类型、操作器static void visitAlongVine(BinNodePosi(T) x, VST&amp; visit, Stack&lt;BinNodePosi(T)&gt;&amp; S) &#123; while (x) &#123; visit(x-&gt;data); //访问当前节点 S.push(x-&gt;rc); //右孩子入栈暂存（可优化：通过判断，避免空的右孩子入栈） x = x-&gt;lc; //沿左分支深入一层 &#125;&#125;template &lt;typename T, typename VST&gt; //元素类型、操作器void travPre_I2(BinNodePosi(T) x, VST&amp; visit) &#123; //二叉树先序遍历算法（迭代版#2） Stack&lt;BinNodePosi(T)&gt; S; //辅助栈 while (true) &#123; visitAlongVine(x, visit, S); //从当前节点出发，逐批访问 if (S.empty()) break; //直到栈空 x = S.pop(); //弹出下一子树的树根 &#125; #pop = #push = #visit = O(n) = 分摊O(1)&#125; 这里之所以使用一个栈而不是队列的用意，依然是因为栈的后进先出的特性，对于左侧链的访问的是自上而下的，存入栈中的右子树也就是自上而下的，而接着对右子树的遍历是自下而上的，对栈来说就是自顶向底的，对栈的一系列依次的pop操作则恰好可以实现栈中右子树的自顶向底的访问。 下面看一个实例：（^代表空） 2.中序遍历2.1.递归1234567template &lt;typename T, typename VST&gt; //元素类型、操作器void travIn_R(BinNodePosi(T) x, VST&amp; visit) &#123; //二叉树中序遍历算法（递归版） if (!x) return; travIn_R(x-&gt;lc, visit); visit(x-&gt;data); travIn_R(x-&gt;rc, visit);&#125; 将递归转换为迭代的难点在于尽管右子树的递归遍历是尾递归，但左子树却严格地不是。解决方法可是是：找到第一个被访问的节点，将其祖先用栈保存，这样原问题就分解为依次对若干棵子树的遍历问题。 2.2迭代同样从一个规模略大同时更具一般性的例子入手： 与先序遍历非常类似，整个中序遍历过程是从根节点开始，一直沿着左侧分支逐层向下，直到末端不能再向下的那个节点，因此可以将整个中序遍历分解为在不同尺度下的一系列的对左侧分支的逐步处理。我们可以将任何一棵二叉树抽象地规范为如下图所示的形式，整棵树可以分解为一条起自根节点的左侧链以及左侧链上各节点所对应的右孩子。 在一个局部，当前节点$L_{d-1}$将控制权交给并访问它的左孩子$L_d$后，再遍历$L_d$的右子树$T_d$，然后回到并访问节点$L_{d-1}$，再遍历其右子树$T_{d-1}$，如此反复直到遍历全树。在这样的一个过程中存在着某种逆序性，我们最初的起点是在根节点处可是首先接受访问的却是它所对应的左侧链的末端节点，如果说这个的过程是自顶而下的话，那么各节点实际被访问的次序大体而言是呈一种自下而上的过程，因此仍然要使用栈结构来实现这一过程。 12345678910111213141516template &lt;typename T&gt; //从当前节点出发，沿左分支不断深入，直至没有左分支的节点static void goAlongLeftBranch(BinNodePosi(T) x, Stack&lt;BinNodePosi(T)&gt;&amp; S) &#123; while (x) &#123; S.push(x); x = x-&gt;lChild; &#125; //当前节点入栈后随即向左侧分支深入，迭代直到无左孩子&#125;template &lt;typename T, typename VST&gt; //元素类型、操作器void travIn_I1(BinNodePosi(T) x, VST&amp; visit) &#123; //二叉树中序遍历算法（迭代版#1） Stack&lt;BinNodePosi(T)&gt; S; //辅助栈 while (true) &#123; goAlongLeftBranch(x, S); //从当前节点出发，逐批入栈 if (S.empty()) break; //直至所有节点处理完毕 x = S.pop(); //x的左子树或为空，或已遍历（等效于空），故可以 visit(x-&gt;data); //弹出栈顶节点并访问之 x = x-&gt;rChild; //转向右子树 &#125;&#125; 下面是一个实例： 我们知道递归的版本可以简明地实现$O(n)$的复杂度，尽管它的常系数非常之大，那么迭代版本的时间复杂度仍是$O(n)$，但常系数要小的多（分摊分析）。尽管单次调用goAlongLeftBranch就可能需要做$\\Omega(n)$次入栈操作需要$\\Omega(n)$时间，但这些左侧链的长度加起来也不过是n，因此迭代算法的复杂度仍是线性的，即$O(n)$。 此外还要其他版本的中序遍历的迭代实现，如版本2： 1234567891011121314template &lt;typename T, typename VST&gt; //元素类型、操作器void travIn_I2 ( BinNodePosi(T) x, VST&amp; visit ) &#123; //二叉树中序遍历算法（迭代版#2） Stack&lt;BinNodePosi(T)&gt; S; //辅助栈 while ( true ) if ( x ) &#123; S.push ( x ); //根节点进栈 x = x-&gt;lc; //深入遍历左子树 &#125; else if ( !S.empty() ) &#123; x = S.pop(); //尚未访问的最低祖先节点退栈 visit ( x-&gt;data ); //访问该祖先节点 x = x-&gt;rc; //遍历祖先的右子树 &#125; else break; //遍历完成&#125; 版本3： 1234567891011121314151617template &lt;typename T, typename VST&gt; //元素类型、操作器void travIn_I3 ( BinNodePosi(T) x, VST&amp; visit ) &#123; //二叉树中序遍历算法（迭代版#3，无需辅助栈） bool backtrack = false; //前一步是否刚从左子树回溯——省去栈，仅O(1)辅助空间 while ( true ) if ( !backtrack &amp;&amp; HasLChild ( *x ) ) //若有左子树且不是刚刚回溯，则 x = x-&gt;lc; //深入遍历左子树 else &#123; //否则——无左子树或刚刚回溯（相当于无左子树） visit ( x-&gt;data ); //访问该节点 if ( HasRChild ( *x ) ) &#123; //若其右子树非空，则 x = x-&gt;rc; //深入右子树继续遍历 backtrack = false; //并关闭回溯标志 &#125; else &#123; //若右子树空，则 if ( ! ( x = x-&gt;succ() ) ) break; //回溯（含抵达末节点时的退出返回） backtrack = true; //并设置回溯标志 &#125; &#125;&#125; 版本4： 12345678910111213template &lt;typename T, typename VST&gt; //元素类型、操作器void travIn_I4 ( BinNodePosi(T) x, VST&amp; visit ) &#123; //二叉树中序遍历（迭代版#4，无需栈或标志位） while ( true ) if ( HasLChild ( *x ) ) //若有左子树，则 x = x-&gt;lc; //深入遍历左子树 else &#123; //否则 visit ( x-&gt;data ); //访问当前节点，并 while ( !HasRChild ( *x ) ) //不断地在无右分支处 if ( ! ( x = x-&gt;succ() ) ) return; //回溯至直接后继（在没有后继的末节点处，直接退出） else visit ( x-&gt;data ); //访问新的当前节点 x = x-&gt;rc; //（直至有右分支处）转向非空的右子树 &#125;&#125; 3.层次遍历我们此前讨论的有根有序树，任何一棵二叉树都被指定了一个特殊的节点：根节点，由此就可以在垂直方向按照深度将所有节点划分为若干个等价类，因此可以认为所谓的有根性对应的就是垂直方向的次序。 进一步地位于同一深度也属于同一等价类内部的所有节点，即所有的同辈节点也可以分出次序，比如对于二叉树可以通过左右的明确定义给出同辈节点之间的相对次序，因此可以认为有序给出沿水平方向的一个次序。 因此按照垂直方向和水平方向的次序可以在所有的节点之间定义一个整体的次序，并进而对它进行遍历。自高向低而在每一层自左向右逐一地访问树中的每一个节点的遍历策略及过程就是层次遍历。 此前的三种遍历策略：先序、中序和后序都无法保证所有节点严格地按照深度次序访问，都有后代限于祖先被访问的情况，即逆序，为此需要借助栈结构。反过来，在层次遍历中，所有节点的访问都满足顺序性，因此这里就需要借助与栈结构对称的队列结构。 具体实现为： 12345678910template &lt;typename T&gt; template &lt;typename VST&gt; //元素类型、操作器void BinNode&lt;T&gt;::travLevel ( VST&amp; visit ) &#123; //二叉树层次遍历算法 Queue&lt;BinNodePosi(T)&gt; Q; //辅助队列 Q.enqueue ( this ); //根节点入队 while ( !Q.empty() ) &#123; //在队列再次变空之前，反复迭代 BinNodePosi(T) x = Q.dequeue(); visit ( x-&gt;data ); //取出队首节点并访问之 if ( HasLChild ( *x ) ) Q.enqueue ( x-&gt;lChild ); //左孩子入队 if ( HasRChild ( *x ) ) Q.enqueue ( x-&gt;rChild ); //右孩子入队 &#125;&#125; 下面是一个实例： 4.重构由任何一棵二叉树我们都可以导出三个序列：先序(preorder)、中序(inorder)和后序(postorder)遍历序列，这三个序列的长度相同，它们都是由树中的所有节点依照对应的遍历策略所确定的次序依次排列而成。那么如果我们已知某棵树的遍历序列，是否可以还原出这棵树的拓扑结构？什么情况下可以？什么情况下不可以？如果可以具体又应该使用什么样的方法？ 关于二叉树重构的第一个结论是：只需中序遍历序列再加上先序与后序遍历序列之一，即可还原二叉树的完整拓扑结构。 用数学归纳来证明：假设对于规模小于大N的所有二叉树这个规律都是成立的，接下来考察规模恰好为N的二叉树。在先序遍历序列中可以地将左子树和右子树所对应的遍历子序列切分开。这样就将原来全树的重构问题化解为两棵子树的重构问题，这两棵子树在规模上都符合归纳假设，即它们都严格地小于大N，因此根据归纳假设无论是左子树还是右子树都可以重构出来。 当然你应该不难写出一个递归式的重构算法，需要特别注意的是无论是左子树还是右子树，都有可能是空树，在这种情况下树的规模应该是零。而不借助中序遍历序列而只凭借先序和后序遍历序列，是不能保证完成对左右子树的正确切分的。因为无论是L还是R都有可能是空树，在先序遍历或者后序遍历的表达中会出现歧义，我们无法根据先序遍历序列以及后序遍历序列来区分在这种情况下除去根节点之后的部分究竟是左子树还是右子树。 在某些特定情况下由先序和后序遍历序列也可以还原树的整体结构。比如对于真二叉树，每个节点的度数都必须是偶数，即0度或2度，此时的左子树和右子树要么同时为空要么同时非空。 在任何给定的先序遍历序列中都可以找到其左子树L，进而在后序遍历序列中对它进行定位，而这个节点在它所属的这棵子树的后序遍历子序列中必然垫后，这就意味着我们可以明确地界定左右子树的范围，即左子树由哪些节点构成以及右子树由哪些节点构成都是可以确定的。 当然对称地在后序遍历序列中，右子树的树根位置也是确定的，因此通过右子树的树根节点依然可以反过来在先序遍历序列中进行定位，而且同样地可以确定左右子树的切分位置。也就是说我们在这里确实可以进行分而治之从而通过递归的形式，完整地重构出一棵真二叉树原本的结构。","categories":[],"tags":[]},{"title":"Cpp基础（11）虚函数和多态","slug":"Cpp基础（11）虚函数和多态","date":"2020-02-20T10:03:49.000Z","updated":"2020-02-22T10:03:49.000Z","comments":true,"path":"2020/02/20/Cpp基础（11）虚函数和多态/","link":"","permalink":"http://nekomoon404.github.io/2020/02/20/Cpp%E5%9F%BA%E7%A1%80%EF%BC%8811%EF%BC%89%E8%99%9A%E5%87%BD%E6%95%B0%E5%92%8C%E5%A4%9A%E6%80%81/","excerpt":"","text":"1.基本概念1.1.虚函数 在类的定义中，前面有virtual 关键字的成员函数就是虚函数。 1234class base&#123; virtual int get();&#125;;int base::get() &#123;...&#125; virtual 关键字只用在类定义里的函数声明中，写函数体时不用。 构造函数和静态成员函数不能是虚函数。 虚函数和普通函数的本质区别在于虚函数可以参与多态，而普通成员函数不能。 派生类中和基类中虚函数同名同参数表的函数，不加virtual也自动成为虚函数。 1.2.多态的表现形式一 派生类的指针可以赋给基类指针。 通过基类指针调用基类和派生类中的同名虚函数时： 若该指针指向一个基类的对象，那么被调用的是基类的虚函数； 若该指针指向一个派生类的对象，那么被调用的是派生类的虚函数。 这种机制就叫作多态。 123456789101112131415class Animal &#123;public: virtual void sayHi() &#123; cout &lt;&lt; \"Hi\" &lt;&lt; endl; &#125;&#125;;class Dog : public Animal &#123;public: void sayHi() &#123; cout &lt;&lt; \"Woof!\" &lt;&lt; endl; &#125;&#125;;void func(Animal *pa) &#123; pa-&gt;sayHi(); &#125; // 使用基类指针调用函数int main() &#123; Animal a; Dog d; func(&amp;a); // 指向基类对象，则输出 Hi func(&amp;d); // 指向派生类对象，则输出 Woof! return 0;&#125; 1.3.多态的表现形式二 派生类的指针可以赋给基类引用。 通过基类引用调用基类和派生类中的同名虚函数时： 若该引用引用的是一个基类的对象，那么被调用的是基类的虚函数； 若该引用引用的是一个派生类的对象，那么被调用的是派生类的虚函数。 不加virtual，就根据指针，引用的类型来决定调用基类还是派生类的函数；加virtual，就是虚函数，就根据指针所指，引用所引的类型来决定调用基类还是派生类的函数。派生类的函数与基类的虚函数同名且参数列表完全相同时才能体现多态性，因此一般应该禁止重新定义继承而来的非虚函数。 在多层继承的情况下，从定义virtual开始的派生类中同名函数均为虚函数，无论在这些派生类的同名函数中是否显示加virtual。如果派生类将基类中的某个非虚函数声明为虚函数，使用基类指针调用该函数时，不能体现多态。 当基类指针指向派生类对象时，会优先选择符合多态的派生类成员函数。 2.多态程序实例2.1.例1几何形体处理程序几何形体处理程序：输入若干个几何形体的参数，要求按面积排序输出。输出时要指明形状。 输入Input：第一行是几何形体数目n（不超过100），下面有n行，每行以一个字母c开头 若 c 是 ‘R’，则代表一个矩形，本行后面跟着两个整数，分别是矩形的宽和高； 若 c 是 ‘C’，则代表一个圆，本行后面跟着一个整数代表其半径； 若 c 是 ‘T’，则代表一个三角形，本行后面跟着三个整数，代表三条边的长度。 输出Output：按面积从小到大依次输出每个几何形体的种类及面积，每行一个几何形体，输出格式为：形体名称：面积 类的定义： 123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;#include&lt;stdlib.h&gt;#include&lt;math.h&gt;using namespace std;class Shape &#123;public: virtual double Area() = 0; //纯虚函数 virtual void PrintInfo() = 0;&#125;;class Rectangle :public Shape &#123;public: int w, h; virtual double Area(); virtual void PrintInfo();&#125;;class Circle :public Shape &#123;public: int r; virtual double Area(); virtual void PrintInfo();&#125;;class Triangle :public Shape &#123;public: int a,b,c; virtual double Area(); virtual void PrintInfo();&#125;; 各成员函数的实现： 123456789101112131415161718192021222324double Rectangle::Area() &#123; return w * h;&#125;void Rectangle::PrintInfo() &#123; cout &lt;&lt; \"Rectangle: \" &lt;&lt; Area() &lt;&lt; endl;&#125;double Circle::Area() &#123; return 3.14 * r * r;&#125;void Circle::PrintInfo() &#123; cout &lt;&lt; \"Circle: \" &lt;&lt; Area() &lt;&lt; endl;&#125;double Triangle::Area() &#123; double p = (a + b + c) / 2.0; return sqrt(p*(p - a)*(p - b)*(p - c));&#125;void Triangle::PrintInfo() &#123; cout &lt;&lt; \"Triangle: \" &lt;&lt; Area() &lt;&lt; endl;&#125; 用一个指向Shape基类的指针数组pShapes来存放各个几何形体，数组中的每一个元素都是基类指针，因此它可以指向不同派生类的对象。MyCompare函数比较两个几何形体面积的大小。 12345678910111213141516Shape * pShapes[100];int MyCompare(const void * s1, const void * s2) &#123; double a1, a2; Shape **p1; //s1,s2是void*，不可写 *s1 来取得s1指向的内容 Shape **p2; p1 = (Shape**)s1; //s1,s2指向Shapes数组中的元素，数组元素的类型是Shape* p2 = (Shape**)s2; //故p1,p2都是指向指针的指针，类型为Shae** a1 = (*p1)-&gt;Area(); //*p1的类型是Shape*，是基类指针，故此句为多态 a2 = (*p2)-&gt;Area(); if (a1 &lt; a2) return -1; else if (a2 &lt; a1) return 1; else return 0;&#125; 主函数： 123456789101112131415161718192021222324252627282930int main() &#123; int i; int n; Rectangle *pr; Circle *pc; Triangle *pt; cin &gt;&gt; n; for (i = 0; i &lt; n; i++) &#123; char c; cin &gt;&gt; c; switch (c) &#123; case'R': pr = new Rectangle(); cin &gt;&gt; pr-&gt;w &gt;&gt; pr-&gt;h; pShapes[i] = pr; break; case'C': pc = new Circle(); cin &gt;&gt; pc-&gt;r ; pShapes[i] = pc; break; case'T': pt = new Triangle(); cin &gt;&gt; pt-&gt;a &gt;&gt; pt-&gt;b &gt;&gt; pt-&gt;c; pShapes[i] = pt; break; &#125; &#125; qsort(pShapes, n, sizeof(Shape*), MyCompare); //按指针指向的几何形体的面积从小到大排序 for (i = 0; i &lt; n; i++) pShapes[i]-&gt;PrintInfo(); //多态，根据pShapes[i]指向的对象调用其对应的成员函数 return 0;&#125; 运用多态来实现这个问题所带来的好处就是提高了程序的可扩充性，如果要添加新的几何形体如五边形，则只需要从Shape里派生出Pentagon，以及在main函数中的swtich语句中增加一个case，其余部分不变。 用基类指针数组存放指向各种派生类对象的指针，然后遍历该数组，就能对各个派生类对象做各种操作，是很常见的做法。 2.2.例2再看下面的例子： 1234567891011121314151617#include&lt;iostream&gt;using namespace std;class Base &#123;public: void fun1() &#123; fun2(); &#125; //等价于 this-&gt;fun2，fun2是虚函数，所以这句是多态 virtual void fun2() &#123; cout &lt;&lt; \"Base::fun2()\" &lt;&lt; endl; &#125;&#125;;class Derived:public Base &#123;public: virtual void fun2() &#123; cout &lt;&lt; \"Derived::fun2()\" &lt;&lt; endl; &#125;&#125;;int main() &#123; Derived d; Base * pBase = &amp;d; pBase-&gt;fun1(); return 0;&#125; 上面程序运行的结果是Derived::fun2()，而不是Base::fun2()。这是因为pBase-&gt;fun1指至pbase是指向一个派生类的对象d的，那进到fun1里面，this指针指向的东西自然也就是这个d，所以此时this指针指向的是一个派生类的对象，那么根据多态的原则这条语句就会调用派生类的fun2，也就是Derieved类的fun2，所以会输出Derieved的fun2，即Derived::fun2()。 成员函数中调用虚函数有这样的规则： 在非构造函数，非析构函数的成员函数中调用虚函数，是多态。 在构造函数和析构函数中调用虚函数，不是多态。编译时即可确定，调用的函数是自己的类或基类中定义的函数，不会等到运行时才决定调用自己的还是派生类的函数。 1234567891011121314151617181920212223242526272829303132#include&lt;iostream&gt;using namespace std;class myclass &#123;public: virtual void hello() &#123; cout &lt;&lt; \"hello from myclass\" &lt;&lt; endl; &#125; virtual void bye() &#123; cout &lt;&lt; \"bye from myclass\" &lt;&lt; endl; &#125;&#125;;class son :public myclass &#123;public: //派生类中和基类中虚函数同名同参数表的函数，不加`virtual`也自动成为虚函数 void hello() &#123; cout &lt;&lt; \"hello from son\" &lt;&lt; endl; &#125; son() &#123; hello(); &#125;; ~son() &#123; bye(); &#125;;&#125;;class grandson :public son &#123;public: virtual void hello() &#123; cout &lt;&lt; \"hello from grandson\" &lt;&lt; endl; &#125; virtual void bye() &#123; cout &lt;&lt; \"bye from grandson\" &lt;&lt; endl; &#125; grandson() &#123; cout &lt;&lt; \"constructing grandson\" &lt;&lt; endl; &#125; ~grandson() &#123; cout &lt;&lt; \"destructing grandson\" &lt;&lt; endl; &#125;&#125;;int main() &#123; grandson gson; son *pson; pson = &amp;gson; pson-&gt;hello(); //多态 return 0;&#125; 程序运行结果： 3.多态实现原理“多态”的关键在于通过基类指针或引用调用一个虚函数时，编译时不确定到底调用的是基类还是派生类的函数，运行时才确定——这叫“动态联编”。 1234567891011121314151617#include&lt;iostream&gt;using namespace std;class Base &#123;public: int i; virtual void Print() &#123; cout &lt;&lt; \"Base:Print\"; &#125;&#125;;class Derived:public Base &#123;public: int n; virtual void Print() &#123; cout &lt;&lt; \"Derived:Print\"; &#125;&#125;;int main() &#123; Derived d; cout &lt;&lt; sizeof(Base) &lt;&lt; \".\" &lt;&lt; sizeof(Derived); return 0;&#125; 程序运行输出结果为：8,12。那么为什么这个对象的大小都多出了4个字节呢？ 多态实现的关键——虚函数表： 每一个有虚函数的类（或有虚函数的类的派生类）都有一个虚函数表，该类的任何对象中都放着虚函数表的指针。虚函数表中列出了该类的虚函数地址，多出来的4个字节就是用来放虚函数表的地址的。 多态的函数调用语句被编译成一系列根据基类指针所指向的（或基类引用所引用的）对象中存放的虚函数表的地址，在虚函数表中查找虚函数地址，并调用虚函数的指令。 多态在提高程序可扩充性时也会有一定的代价：多态程序在运行期间会有额外的时间和空间上的开销，即时间上编译时会查询虚函数表，空间上每个有虚函数的类的对象里都会多出4个字节来存放虚函数表的地址。 需要注意的是，在构造函数和析构函数中调用虚函数时，调用的是自己的类或基类中定义的函数，不会等到运行时才决定，因此不是动态联编。在普通成员函数中调用虚函数，才是动态联编，是多态。 4.虚析构函数12345678910111213141516class son &#123;public: ~son() &#123; cout &lt;&lt; \"bye from son\" &lt;&lt; endl; &#125;&#125;;class grandson :public son &#123;public: ~grandson() &#123; cout &lt;&lt; \"bye from grandson\" &lt;&lt; endl; &#125;&#125;;int main() &#123; son *pson; pson = new grandson; delete pson; return 0;&#125; 上面的程序运行会输出bye from son，pson这个基类的指针指向了派生类的对象，当delete pson后会引起一些问题，直观上看因为delete掉的是一个基类的指针，所以会去调用基类的析构函数。但是逻辑上讲这个指针本身又指向的是一个派生类的对象，那么分配的也是一个派生类对应的这样的一个内存空间，那么这时它应该调用的还有派生类的析构函数，但是目前的程序设计角度上来看编译器是不会知道它需要调用派生类的机构函数的，这样可能导致一些问题。 我们希望做的是： 通过基类的指针删除一个派生类的对象时，先调用派生类的析构函数，再调用基类的析构函数。 解决方法是： 把基类的析构函数声明为virtual，派生类的析构函数virtual可以不进行声明。 类如果定义了虚函数，则最好将析构函数也定义成虚函数。 注意：不允许以虚函数作为构造函数 一般来说，如果一个类中定义了虚函数，则应该将析构函数也定义成虚函数；同理，若一个类没有定义虚函数，但需要通过基类的指针销毁派生类对象，也应该将析构函数声明为虚函数。 12345678910111213141516class son &#123;public: virtual ~son() &#123; cout &lt;&lt; \"bye from son\" &lt;&lt; endl; &#125;&#125;;class grandson :public son &#123;public: ~grandson() &#123; cout &lt;&lt; \"bye from grandson\" &lt;&lt; endl; &#125;&#125;;int main() &#123; son *pson; pson = new grandson; delete pson; return 0;&#125; 5.纯虚函数和抽象类 纯虚函数：没有函数体的虚函数 123456789101112class A&#123;private: int a;public: virtual void Print() = 0; //纯虚函数 void fun()&#123; cout &lt;&lt; \"fun\"; &#125;&#125;;int main()&#123; A a; //错误，A是抽象类，不能创建对象 A *pa; //正确，可以定义抽象类的指针和引用 pa = new A; //错误，A是抽象类，不能创建对象&#125; 抽象类：包含纯虚函数的类 抽象类只能作为基类来派生新类使用； 不能创建抽象类的对象； 抽象类的指针和引用 $\\to$ 由抽象类派生出来的类的对象。 在抽象类中： 在成员函数内可以调用纯需函数； 在构造函数/析构函数内部不能调用纯虚函数 如果一个类从抽象类派生而来 $\\to$ 它实现了基类中的所有纯需函数，才能成为非抽象类。 123456789101112131415class A&#123;public: virtual void f() = 0; //纯虚函数 void g()&#123; this-&gt;f(); &#125; //ok A() &#123; f();&#125; //错误&#125;;class B:public A&#123;public: void f()&#123; cout &lt;&lt; \"B:f()\" &lt;&lt; endl;&#125; //将虚函数“实例化”&#125;;int main()&#123; B b; b.g(); return 0;&#125; 输出结果：B:f()","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"数据结构与算法（9）二叉树","slug":"数据结构与算法（9）二叉树","date":"2020-02-20T01:11:42.000Z","updated":"2020-02-21T03:31:16.237Z","comments":true,"path":"2020/02/20/数据结构与算法（9）二叉树/","link":"","permalink":"http://nekomoon404.github.io/2020/02/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%889%EF%BC%89%E4%BA%8C%E5%8F%89%E6%A0%91/","excerpt":"","text":"1.树回顾之前所学的向量结构（Vector）和列表结构（List），对于以查找为代表的静态操作和以插入为代表的动态操作，两者都无法兼顾静态和动态的高效性。而本章要介绍的树结构恰好能将二者的优势结合起来，即可快速查找，又可以快速插入和删除。树可以理解为列表的列表，或二维的列表。树并不是严格意义上的线性结构，但又带有一定的线性特征，因此树可以被称为半线性结构。 树是用来按照层次关系组织一系列数据项的一种方式，如：表达式、文件系统、函数调用和递归过程、Internet URL等等。 1.1.有根树 树是特殊的图T = ( V, E)，可以认为树是定义在一组元素之间的二元关系，节点（Vertex）数 |V| = n，边（edge）数 |E| = e。 为树指定任一节点 r $\\in$ V作为根后，树T 即称作有根树（rooted tree）。 对于任何一组有根树都可以通过引入一个新的顶点，并且在新的这个顶点与此前各棵有根树的树根之间引入对应的一条连边，从而构成一棵规模更大的有根树，这棵新的有根树的树根就是所引入的这个新的节点，通常记作r，暗示着它就是root树根。而对于这棵更大的树，参与组成它的每一棵有根树都相对地称作是它的子树subtree。 若：$T_1,T_2,\\dots ,T_d$是有根树，则：$T=\\left( (\\cup V_i)\\cup\\{r\\},\\,(\\cup E_i)\\cup\\{|1\\le i\\le d\\} \\right)$也是有根树。 相对于$T$，$T_i$称作以$r_i$为根的子树（subtree rooted at $r_i$），记作$T_i$ = subtree( $r_i$ )。 1.2.有序树 $r_i$称作$r$的孩子（child），$r_i$之间互称兄弟（sibling）； $r$为其父亲（parent），d = degree( r )为$r$的（出）度（degree）。 可归纳证明： e（节点数总和）= $\\sum_{r\\in V}degree(r)$ = n - 1 = $\\Theta(n)$，即任何一棵树中的边数与其中顶点的数目是同阶的，一棵树的总体规模也可度量为( n + e )，故在衡量相关复杂度时，可以n作为参照。 若指定$T_i$为$T$的第$i$棵子树，$r_i$作为$r$的第$i$个孩子（即在兄弟间定义了次序），则$T$称作有序树（ordered tree）。 1.3.连通与无环上面从递归嵌套的角度定义了树，但我们并没有看到树结构相对于一般的图结构而言在拓扑上到底有什么不同。那么接下来将从连通性和无环性两个角度来揭示树结构的特性。 V中的k+1个节点，通过E中的k条边依次相连，构成一条路径（path）： $\\pi=\\{ \\, (V_0,V_1),\\, (V_1,V_2),\\,\\dots,\\,(V_{k-1},V_k)\\, \\}$ 路径长度：$|\\pi|=$边数$=k$ 任意节点之间均有路径的图，称作连通图（connected graph） 若$V_k=V_0$，则该路径是环路（cycle/loop），不含环路的图，称作无环图（acyclic graph）。 树结构是无环连通图，即是极小连通图，也是极大无环图。 故在树中，任一节点V与根之间存在唯一路径，故可记 path(v, r) = path(v) 因此可以|path(v)|为指标，对所有节点做等价类划分。 1.4.深度与层次 在不致歧义的情况下，路径、节点和子树可相互指代： path(v) ~ v ~ subtree(v) v的深度：depth(v) = |path(v)| path(v)上的节点均为v的祖先（ancestor），v是它们的后代（descendent）；除v自身以外的祖先，是真（proper）祖先。 在任一深度：v的祖先若存在则必然唯一；v的后代未必唯一。从这个意义上讲也应该将数称为半线性结构。 根节点是所有节点的（公共）祖先，深度为0；没有后代的节点称作叶子（leaf）。 所有叶子深度中的最大者称为子树的高度：height(v) = height( subtree( v ) )；注意与树根的高度（height(T))区分开。 特别低，只有一个节点的树的高度为1，空树的高度为-1。 depth( v ) + height( v ) $\\le$ height( T ) 2.树的表示上一节介绍了树的基本概念，这一节将来讨论在计算机中如何从逻辑上来表示一棵树，从抽象数据类型的角度来看树结构应该提供大致如下这些接口： 2.1.父节点 除根外，任一节点有且仅有一个父节点。 不妨将所有的节点组织为一个序列：其中的每一个元素都分别包括三项，data是节点本身的信息，rank或者position指明的是这个节点的记录在这个序列中所对应的秩或者是位置，而parent恰好就是节点唯一的父节点所对应的秩或者是位置。树根也有一个“虚构的”父节点-1或NULL。 空间性能：$O(n)$ 时间性能： ​ parent()：$O(1)$ ​ root()：$O(n)$或$O(1)$ firstChild()：$O(n)$ nextSibling()：$O(n)$ 这样做有一定的好处，不幸的是如果要向下索取某个节点的后代比如长子，依然需要去遍历所有的元素并且逐一地确认它的父节点是否就是当前查询的元素，这个基本操作在最坏情况下需要线性时间$O(n)$；而查找兄弟节点也是类似的在最坏情况下需要遍历整棵树。因此我们下一步改进自然就集中在这两种向下方向的查询上。 2.2.孩子节点不妨对于任何一个节点都将它的孩子汇聚起来构成一个更小的数据集，为每一个节点准备一个名为children的引用，所指向的是由它的所有的孩子构成的一个序列，如使用列表来实现这个序列，列表的长度分别等于对应节点当前的度数。 这个方法解决了向下的查找问题，而向上的查找优势却丧失殆尽，不难发现为了查找某一个节点的父亲不得不去遍历整个线性序列，并且逐一地翻看它所对应的孩子记录，在最坏的情况下仍需要线性时间$O(n)$。 2.3.父节点+孩子节点如果将刚才的两个线性的序列组合起来，即对于同一个节点不仅保留它的parent域，同时还要保留它的children这样的一个引用，那么刚才两个方向的优势是可以兼而有之的。如果要去查找父亲就在parent这一列中进行查找，需要$O(1)$的时间；如果要是去查找孩子，就在children所指向的序列中再去查找，若是长子就可在$O(1)$的时间内直接返回，若是其它的孩子最多是去遍历序列。 但这种方法仍有一些不足：每一个节点的children引用所指向的序列在规模上有可能相差极其悬殊，每一个序列的长度恰好是节点对应的出度，而出度的总和为$\\sum_{r\\in V}degree(r)$ = n - 1 = $\\Theta(n)$，与n同阶。而这种组织方式有时需要长达$O(n)$的一个数据集，为此需要找到一些新的办法，更加规范并相应也更简洁和高效。 2.4.长子+兄弟反观上小节方法存在根源在于每一个节点的出度是不尽相同的，为进行改进我们必须发现每个节点所具有的某种不变性。就向下的引用而言每一个节点只需记两个信息就够了，第一个就是它的长子，第二个是每一个节点的下一个兄弟。 每个节点均设两个引用： 纵向：firstChild() 横向：nextSibling() 如此，对于度数为d的节点，可在$O(n+1)$时间内遍历其所有孩子 若再设置parent引用，则parent()接口也仅需$O(1)$时间 相对于此前的表示方法，这种表示方法的规整性非常的突出，由于每个节点只需记录两个引用，因此其点所需要占用的空间依然是常数，，而且都彼此接近，这是此前的常规方法所无法比拟的。 这种长子兄弟法不仅是树的一种很好的表示方法，而且也是对树的本质的一种更深刻的理解，在此后介绍二叉树并且用二叉树来代表所有的树的时候，我们将再次用到这样一种表示方法。对于树这样的一个全集来说尽管二叉树只是它的一个特殊的子集，但是很有趣的是在施加了某些条件之后，二叉树却足以来表示和实现所有的树，而这样一种方法背后的原理在很大程度上就是基于长子兄弟法。 3.二叉树这一节介绍树的一种特殊但又不失代表性的特例：二叉树（binary tree） 节点数不超过2的树，称作二叉树 同一节点的孩子和子树，均以左、右区分（隐含着有序性） lChild() ~ lSubtree() rChild() ~ rSubtree() 3.1.基数 二叉树中深度为k的节点至多$2^k$个 含n个节点、高度为h的二叉树中有：$h&lt;n&lt;2^{h+1}$ 当 $n=h+1$时，退化为一条单链 当$n=2^{h+1}-1$时，称作满二叉树（full binary tree） 由此也可见一棵二叉树在横向上的宽度与它在纵向上的高度是呈一个指数的关系的，宽度是高度的指数，而指数意味着爆炸（剧烈的增长），所以如果节点的总数固定，宽度大致与它相当，但是高度却会增长的非常的缓慢呈一个对数的形式。也就是说对于一棵二叉树而言，它非常倾向于“涨宽”，它“涨宽”的速度更快，而它的高度呢如果控制得当的话会增长的异常的缓慢，这个特点也是之后介绍的二叉搜索树的重要理论基础。 3.2.真二叉树上面介绍的二叉树只对每个节点的出度做了个上限的约定，即不得超过2。这样一般性的一棵二叉树在很多操作，包括算法的实现以及对算法的理解上都会引来一些不必要的麻烦。而反过来一个比较有效的改进方法就是将任何的这样一棵一般性的二叉树转化为一棵真二叉树（proper binary tree）。 通过引入$n_0+n_2$个外部节点，可是原有节点度数同一为2，如此即可将任一二叉树转化为真二叉树。 经过转换之后，从渐进意义上，全树自身的复杂度并未实质增加。 在之后实现相应的算法的时候就会看到这种添加实际上完全是假想的，即并不需要真正去引入节点，只需要假想着它们存在，你的算法就可以更加简洁的实现而且更加简洁的被理解。 3.3.描述多叉树接下来来介绍这一节最重要的一点：如何通过二叉树来描述多叉树。其实上需要的条件只有两条：有根和有序。 二叉树是多叉树的特例，但在有根且有序时其描述能力却足以覆盖后者； 即任意有根有序的多叉树均可转换为二叉树——回顾“长子-兄弟”表示法； 为此只需将节点处旋转45度，将长子，兄弟与左、右孩子等效地相互对应： firstChild() ~ lChild() nextSibling() ~ rChild() 如果说这一章的任务是描述并且实现以及利用树结构的话，不如说我们只需研究并且实现二叉树。接下来几节将介绍二叉树结构的实现和相关的算法。 4.二叉树实现在此前的几节先后介绍了树的概念，了解了树的特点，并且懂得了如何来表示一棵树。最重要的方法就是借助二叉树来表示任何一棵有根有序树，所以接着就来介绍如何在C++语言中实现一棵二叉树。 4.1.BinNode模板类二叉树的基本组成单位是二叉树节点（Binary Node， 或简称BinNode），每一个BinNode的逻辑组成可以用下图来表示。每一个BinNode节点首先应该有一个data域，记录它携带的信息，这是BinNode节点的核心的要素；它也应该配备相应的引用域，分别指向左右孩子以及父亲；此外作为在树中的一个特定元素它也需要记录一些重要的指标，比如height 高度，对于红黑树而言就会有颜色的区别，对于左式堆而言有npl指标，所以也需要为它们留有余地。 下面定义名为BinNode的类 12345678910111213141516171819202122232425#define BinNodePosi(T) BinNode&lt;T&gt;* //节点位置template &lt;typename T&gt; struct BinNode &#123; //二叉树节点模板类 BinNodePosi(T) parent, lChild, rChild; //父节点及左、右孩子 T data; int height; //数据，/高度（通用）// 构造函数 BinNode() : parent ( NULL ), lc ( NULL ), rc ( NULL ), height ( 0 ), npl ( 1 ), color ( RB_RED ) &#123; &#125; BinNode ( T e, BinNodePosi(T) p = NULL, BinNodePosi(T) lc = NULL, BinNodePosi(T) rc = NULL, int h = 0, int l = 1, RBColor c = RB_RED ) : data ( e ), parent ( p ), lc ( lc ), rc ( rc ), height ( h ), npl ( l ), color ( c ) &#123; &#125; // 操作接口 int size(); //统计当前节点后代总数，亦即以其为根的子树的规模 BinNodePosi(T) insertAsLC(T const&amp;); //作为当前节点的左孩子插入新节点 BinNodePosi(T) insertAsRC(T const&amp;); //作为当前节点的右孩子插入新节点 BinNodePosi(T) succ(); //取当前节点的直接后继 template &lt;typename VST&gt; void travLevel(VST&amp;); //子树层次遍历 template &lt;typename VST&gt; void travPre(VST&amp;); //子树先序遍历 template &lt;typename VST&gt; void travIn(VST&amp;); //子树中序遍历 template &lt;typename VST&gt; void travPost(VST&amp;); //子树后序遍历 &#125;; 4.2.BinNode接口实现这一小节介绍BinNode类的几个常用接口，首先是insertAsLC接口，我们要对传入的参数e进行封装使之成为一个新的节点，并且将它作为当前节点的左孩子接入所属的这棵树中。当然作为入口条件可以假设当前节点的左孩子现在是空的。 这个功能的实现方法是：通过BinNode构造函数创建一个新的BinNode节点，而它的父节点就是this即当前这个节点。从下面的图来看，这一步就相当于将新的BinNode节点的parent引用指向当前的这个节点。这只是自下而上一个方向的连接，为了保证整体的一致性我们还需要相应地完成自上而下的连接，也就是令当前这个节点this的左孩子引用lChild能够指向新创建的节点，这一步可以通过直接用这个新生成的节点赋予当前节点的lChild引用来实现。 insertAsRC接口实现的方式完全对称，只需相应地将左孩子引用替换为右孩子引用。两个插入操作的复杂度均为$O(1)$。 1234567template &lt;typename T&gt; BinNodePosi(T) BinNode&lt;T&gt;::insertAsLC(T const&amp; e)&#123; return lc = new BinNode(e, this);&#125; //将e作为当前节点的左孩子插入二叉树, O(1)template &lt;typename T&gt; BinNodePosi(T) BinNode&lt;T&gt;::insertAsRC(T const&amp; e)&#123; return rc = new BinNode(e, this);&#125; //将e作为当前节点的右孩子插入二叉树, O(1) BinNode的size()接口，返回包括当前节点在内所有后代的总数，可以通过递归来实现，复杂度为$O(n=|size|)$。 123456template &lt;typename T&gt; int BinNode&lt;T&gt;::size() &#123; //统计当前节点后代总数，即以其为根的子树规模 int s = 1; //计入本身 if (lChild) s += lChild-&gt;size(); //递归计入左子树规模 if (rChild) s += rChild-&gt;size(); //递归计入右子树规模 return s;&#125; //O(n = |size|) 4.3.BinTree模板类在完成了对二叉树节点类BinNode的定义之后，我们就可以基于它来实现整体的binary tree简称BinTree这样一种模板类，代码的主体结构如下： 123456789101112131415161718192021222324252627282930313233343536373839#include\"BinNode.h\"template &lt;typename T&gt; class BinTree &#123; //二叉树模板类protected: int _size; BinNodePosi(T) _root; //规模、根节点 virtual int updateHeight(BinNodePosi(T) x); //更新节点x的高度 void updateHeightAbove(BinNodePosi(T) x); //更新节点x及其祖先的高度public: BinTree() : _size(0), _root(NULL) &#123; &#125; //构造函数 ~BinTree() &#123; if (0 &lt; _size) remove(_root); &#125; //析构函数 int size() const &#123; return _size; &#125; //规模 bool empty() const &#123; return !_root; &#125; //判空 BinNodePosi(T) root() const &#123; return _root; &#125; //树根 //操作接口 BinNodePosi(T) insertAsRoot(T const&amp; e); //插入根节点 BinNodePosi(T) insertAsLC(BinNodePosi(T) x, T const&amp; e); //e作为x的左孩子（原无）插入 BinNodePosi(T) insertAsRC(BinNodePosi(T) x, T const&amp; e); //e作为x的右孩子（原无）插入 BinNodePosi(T) attachAsLC(BinNodePosi(T) x, BinTree&lt;T&gt;* &amp;T); //T作为x左子树接入 BinNodePosi(T) attachAsRC(BinNodePosi(T) x, BinTree&lt;T&gt;* &amp;T); //T作为x右子树接入 int remove(BinNodePosi(T) x); //删除以位置x处节点为根的子树，返回该子树原先的规模 BinTree&lt;T&gt;* secede(BinNodePosi(T) x); //将子树x从当前树中摘除，并将其转换为一棵独立子树 template &lt;typename VST&gt; //操作器 void travLevel(VST&amp; visit) &#123; if (_root) _root-&gt;travLevel(visit); &#125; //层次遍历 template &lt;typename VST&gt; //操作器 void travPre(VST&amp; visit) &#123; if (_root) _root-&gt;travPre(visit); &#125; //先序遍历 template &lt;typename VST&gt; //操作器 void travIn(VST&amp; visit) &#123; if (_root) _root-&gt;travIn(visit); &#125; //中序遍历 template &lt;typename VST&gt; //操作器 void travPost(VST&amp; visit) &#123; if (_root) _root-&gt;travPost(visit); &#125; //后序遍历 bool operator&lt; (BinTree&lt;T&gt; const&amp; t) //比较器（其余自行补充） &#123; return _root &amp;&amp; t._root &amp;&amp; lt(_root, t._root); &#125; bool operator== (BinTree&lt;T&gt; const&amp; t) //判等器 &#123; return _root &amp;&amp; t._root &amp;&amp; (_root == t._root); &#125;&#125;; //BinTree 需要注意的是：其中updateHeight这个接口是以virtual来修饰的，即虚函数。后面我们会看到二叉树，尤其是二叉搜索树是一个庞大的家族，其中的每一个成员对于高度的定义包括更新的方法都不尽相同，因此通过将它定义为虚方法可以便于各种派生类对这个方法进行适当的重写。 4.4.高度更新对于任何一个节点x，它的高度是在以它为根的子树中，从它通往那个最深的叶节点的路径长度。有两种特殊情况：单节点的树高度取0，空树高度取-1，这里采用宏定义的封装的方式，通过重新命名一个新的等价意义上的高度，将常规情况下的高度与退化情况下的高度统一起来，使得此后对算法的描述和理解可以更为简便，同时也不致于影响到算法的正确性。 一个节点的高度恰好等于它的左孩子与右孩子高度中的更大者再加1，因此可以相应地得到对任意节点x进行高度更新的算法。 而如果x的祖先节点存在，那么祖先节点的高度可能因为x的高度变化而变化，整个这样的过程需要从x开始遍历它的所有历代祖先，算法的复杂度正比于x节点的深度，即$O(n=depth(x))$。 1234567891011#define stature(p) ( (p) ? (p)-&gt;height : -1 ) //节点高度—约定空树高度为-1template &lt;typename T&gt; int BinTree&lt;T&gt;::updateHeight(BinNodePosi(T) x) //更新节点x高度&#123; return x-&gt;height = 1 + __max(stature(x-&gt;lc), stature(x-&gt;rc));&#125; //具体规则，因树而异template &lt;typename T&gt; void BinTree&lt;T&gt;::updateHeightAbove(BinNodePosi(T) x) //更新节点x及其历代祖先的高度&#123; while (x) &#123; updateHeight(x); x = x-&gt;parent; &#125;&#125; //可优化：一旦高度未变，即可终止。 O(n = depth(x)) 4.5.节点插入insertAsRC接口为原树中一个没有右孩子的节点插入一个右孩子节点，插入后原树的规模会增加1，x节点的高度有可能因为它新加入了一个孩子而发生变化，因此还要调用updateHeightAbove来对x这个节点以及它的历代祖先更新高度。 123456template &lt;typename T&gt; BinNodePosi(T) BinTree&lt;T&gt;::insertAsRC(BinNodePosi(T) x, T const&amp; e)&#123; _size++; x-&gt;insertAsRC(e); updateHeightAbove(x); //x及其祖先的高度可能增加，其余节点必然不变 return x-&gt;rc;&#125; //e插入为x的右孩子，insertAsLC()完全对称","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"数据结构与算法（8）栈应用","slug":"数据结构与算法（8）栈应用","date":"2020-02-17T11:52:34.000Z","updated":"2020-02-18T15:17:01.676Z","comments":true,"path":"2020/02/17/数据结构与算法（8）栈应用/","link":"","permalink":"http://nekomoon404.github.io/2020/02/17/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%888%EF%BC%89%E6%A0%88%E5%BA%94%E7%94%A8/","excerpt":"","text":"栈结构的经典应用场合大致可以分为以下四类： 逆序输出 conversion： 输出次序与处理过程颠倒；递归深度和输出长度不易预知 递归嵌套 stack permutation + parenthesis 具有自相似的问题可递归描述，但分支位置和嵌套深度不固定 延迟缓冲 evaluation 线性扫描算法模式中，在欲读足够长之后，方能确定可处理的前缀 栈式计算 RPN 基于栈结构的特定计算模式 1.进制转换问题描述：给定任一10进制非负整数，将其转换为$\\lambda$进制表示形式。 解法我们都很熟悉，即短除法：对给定的数做除法，留商取余，所得的余数再逆序输出。 这样一个过程在用代码实现时一个问题需要考虑：计算的过程是由上而下的，而输出的过程是由下而上的，如果不借助对数我们很难预测最终会有多少个数位，也就是整个计算的深度到底有多少，那这个问题该如何解决呢？ 回顾上篇文章中介绍的栈结构，我们只需引入一个栈，在计算的过程中，我们每得到一个数位（余数），就通过push()使它入栈，那么这些数位入栈的次序恰好就是它们被计算出来的次序（在图中是自上而下）。而栈的特性是后进先出（LIFO），一旦计算终止，我们就可以通过一系列pop()操作将这些数位按刚才的数位的逆序（在图中是自下而上）输出出来，从而得到所需要的结果。 123456789void convert(Stack&lt;char&gt; &amp; S, _int64 n, int base) &#123; //薪进制下的数位符号，可视bae取值范围适当扩充 static char digit[] = &#123; '0','1','2','3','4','5','6','7','8','9','A','B','C','D','E','F' &#125;; while (n &gt; 0) &#123; //由低到高，逐一计算出薪进制下的各数位 S.push( digit[ n % base ] ); //余数（对应的数位）入栈 n /= base; //n更新为其对base的除商 &#125;&#125; 2.括号匹配括号匹配这个问题可以归为递归嵌套式问题，以括号匹配为代表的这类问题它们的共同特点是，具有某种意义上的自相似性，即它们的某一个局部往往和整体具有某种共性，同时这种局部作为分支它的位置以及嵌套的深度却难以在事先固定或者确定。 括号匹配是合法表达式的必要条件之一，下面是一个实例： 我们的任务是给定任意一个含有括号的表达式如何来判定它是否是匹配的，为了简化问题不妨将括号之外的其余符号都暂时地忽略掉，只需要做一次线性扫描的预处理即可，这样就可以集中注意处理括号匹配的情况。 2.1.构思为了得到这个问题的有效解法，我们或许应该继续沿用不断将问题简化的总体策略。为此需要首先来考察最基本的平凡情况：如果一个表达式不含任何的括号，从而实质上等价于一个空串，那么它自然是匹配的。 接下来我们注意到这样一个事实：如果某一个表达式E已经是括号匹配的，那么在它的左侧和右侧添加一对匹配的括号，整体依然将是匹配的；如果两个表达式E和F已经是各自匹配的，那么只要将它们串接起来就可以得到一个更大的匹配表达式。 然而很遗憾这两个性质都不能使得我们很好地运用此前所学的减而治之或者分而治之的策略。因此这两条性质只能作为括号匹配判断的必要条件而非充要条件，从逻辑推理的方向来看它是由左侧的前命题推向右侧的后命题，与简化问题的方向背道而驰。 而且它们也存在反例，这就让减而治之和分而治之都是行不通的。因此为了真正使这个问题能够得到有效的简化，必须发现并且借助这个问题所蕴含的某种充分性。 一种可行的方法是将以上的减而治之的策略颠倒过来，不是去试图减除一个表达式最外围的一对括号，而是试图去减除其中相互紧邻的一对左右括号。如果在一个表达式中能够发现一对彼此紧邻的左右括号，那么在将这对括号减除之后，剩余的部分是否匹配与此前的表达式是否匹配必然是互为充要条件的。 那么如何找到这样一对括号呢？另外，更重要的是如何使得这种简化能够持续地进行下去呢？ 实际针对上面的情况，正是栈可以大显身手的时候，借助栈结构的算法过程原理可以表示为下图。如果这是我们所使用的栈，那么其中所包含的就是我们已经扫描过但是仍待处理的部分，其中只需保存左括号。在接下来的扫描中一遇到左括号就令它入栈；而一旦遇到右括号不仅不需要令它入栈，反而应该令栈顶的那个左括号出栈。 这样就刚好实现了上述的可行方法，算法的确可以持续地如此往复进行下去。如果最后一个括号被处理之后，整个栈恰好变空，那么就意味着原来的表达式是匹配的；反之，无论到最后栈非空或者在中途某个阶段提前变空，我们都可以判定原来的表达式是不匹配的。 2.2.实现上节的构思可以具体兑现为这样一段代码： 12345678bool paren(const char exp[], int lo, int hi) &#123; //exp[lo, hi) Stack&lt;char&gt; S; //使用栈记录已发现但尚未匹配的左括号 for (int i = lo; i &lt; hi; i++) //逐一检查当前符号 if ('(' == exp[i]) S.push(exp[i]); //若遇左括号，则进栈 else if (!S.empty()) S.pop(); //否则如果遇右括号，且栈非空，则令栈顶的左括号出栈 else return false; //否则（遇右括号是栈已空），必不匹配 return S.empty(); //最终，栈空当且仅当匹配&#125; 为了判别low与high之间的这样一段表达式是否括号匹配，需要引入一个名为S的栈。以下的这个循环逐一地检查每一个字符，如果是左括号就令它入栈，否则就试图弹出栈顶，如果它的确存在的话应该就是与当前这个右括号匹配的那个左括号，而如果此时栈已经提前变空那么就意味着整个表达式失配，可以断定此时的失配是由于某一个右括号缺少与之匹配的左括号。 当处理完所有的字符并退出循环的时候，需要检查栈在当前是否是空的，只有当栈恰好为空时才说明表达式是匹配的，否则可以断定原来那个表达式是不匹配的，且是属于某一左括号缺失与之配对的右括号的情况。 2.3.反思与拓展为更好地理解这个算法，我们不妨来看一个具体的实例： 在理清这个实例的执行过程后，你可能会质疑我们为什么要使用栈呢[・_・?]。就这样一个特定的问题而言，只需借助一个简明的整数计数器就足以完成刚才的算法任务。如果令计算器初始为0，将算法的过程对应过来，若遇左括号计数器就加1；若遇右括号计算器就减1。最终如果计数器为0，则表达式匹配；如果计数器不为0，或在中途出现负数，则表达式不匹配。 实际上这个计数器所反映的就是刚才我们所使用的那个栈在任何时刻的规模，这样就不难理解其中的必然性了。那么既然如此我们为什么不使用更加简明的计数器，而要使用更为复杂的栈结构呢[・_・?] 这背后的原因在于采用栈结构可以便捷地推广至多种括号并存的情况，这时如果想要使用多个计数器也是行不通的，比如一个简单的反例[ ( ] )，因为如果孤立地通过计数器来考察方括号或者是圆括号，两个计数器都是可以正常工作并且在最终复位为0的，而这个表达式显然是不匹配的。 通过下面这个实例来解释这个算法如何扩展到多个括号并存的情况。当遇到一个左括号，无论是什么形式都放入栈中；遇到一个右括号，则判断此时栈顶的左括号是否与它匹配，若匹配则令栈顶的左括号出栈。 同样地在经过了这样一趟线性的扫描之后，只有当栈最终为空，我们才可以断定原来这个包含多种括号的表达式是匹配的。反过来这时括号不匹配的情况相对于此前单括号的情景要多出一种，即当遇到右括号时，发现此时栈顶的左括号与之不匹配，如刚才举的反例[ ( ] )，则表达式必是不匹配的，因为实际上这样每次“消去”的一对括号必是紧邻的一对括号。 扩展后括号匹配算法可以实现为下面一段代码： 123456789101112bool paren(const char exp[], int lo, int hi) &#123; //表达式括号匹配检查，可兼顾三种括号 Stack&lt;char&gt; S; //使用栈记录已发现但尚未匹配的左括号 for (int i = lo; i &lt;= hi; i++) /* 逐一检查当前字符 */ switch (exp[i]) &#123; //左括号直接进栈；右括号若与栈顶失配，则表达式必不匹配 case '(': case '[': case '&#123;': S.push(exp[i]); break; case ')': if ((S.empty()) || ('(' != S.pop())) return false; break; case ']': if ((S.empty()) || ('[' != S.pop())) return false; break; case '&#125;': if ((S.empty()) || ('&#123;' != S.pop())) return false; break; default: break; //非括号字符一律忽略 &#125; return S.empty(); //整个表达式扫描过后，栈中若仍残留（左）括号，则不匹配；否则（栈空）匹配&#125; 最后需要指出的是，实际上这样一种拓展还可以进一步地进行，也就是并不限于某几种特定的括号，甚至不需要对这些括号到底有多少种做出限定，就像在HTML语言中那样只要表达式中能够按照合理的语法，就能够定义任何一种匹配标志，我们都可以来进行这种意义上的匹配检查。 3.栈混洗本节介绍一个与上一节括号匹配问题非常相关的问题：栈混洗问题。栈混洗就是按照某种约定的规则对栈中的元素进行重新的排列。初始情况下所有的元素都存在栈A中，这里约定分别用尖括号和方括号来表示栈顶以及栈底，栈混洗的目标是将所有这些元素都通过某种方式转入到另一个初始为空的栈B中，为此需要借助一个中转栈S。 栈 A = $&lt;a_1,a_2,\\dots,a_n]$、B = S = $\\varnothing$ 只允许的操作： 将A的顶元素弹出并压入S； //S.push(A.pop()) 或将S的顶元素弹出压入B。 //B.push(S.pop()) 若经过一系列以上的操作后，A中元素全部转入B中； 则B = $[a_{k1},a_{k2},\\dots,a_{kn}&gt;$则称之为A的一个栈混洗（stack permutation） 在遵守以上规则的前提下同一输入序列可能导出不同的栈混洗序列，比如 A = &lt;1, 2, 3, 4]就可能得到： B = [1, 2, 3, 4&gt;，[4, 3, 2, 1&gt;，[3, 2, 4, 1&gt;，…… 3.1.计数那么长度为n的序列，可能的栈混洗总数$SP(n)$为多少呢？ 假定输入栈 A = &lt; 1, 2, 3, ……, n ]，关注1号元素，它第一个被推入栈S中，假设它是第K个从S出栈的元素，当它出栈后S已为空，相应地A剩余最靠底的n-k个元素。此时B中最靠底的k-1个元素和A中最靠底的n-k个元素，它们的栈混洗实际上是相互独立的。 因此对应于1号元素作为第k个元素被推入B中的情况，对应的栈混洗总数就应该是这两个相互独立的子序列所各自对应的栈混洗总数的乘积$SP(k-1)\\times SP(n-k)$，又k可以取1到n，因此长度为n的序列，可能的栈混洗总数为： SP(n)=\\sum_{k=1}^n SP(k-1)\\times SP(n-k)=catalan(n)=\\frac{(2n)!}{(n+1)!n!}3.2.甄别对于输入序列&lt; 1, 2, 3, ……, n ] 的任一个排列 [$p_{1},p_{2},\\dots,p_{n}$ &gt; 如何来判断它究竟是不是一个合法的栈混洗？ 先从简单的例子入手，考虑由三个元素所构成的一个输入序列，则$SP(3)=5$，而三个数的全排列为$3!=6$，对于这种情况有一种排列不是栈混洗，显然不是栈混洗的排列是[ 3, 1, 2&gt;。 实际上任意的三个元素能否按照某种相对的次序出现在最终的栈混洗中，与其它的元素实是无关的，因此推而广之对于任何三个互异的整数 $1\\le i &lt; j&lt;k \\le n$，如果在某个排列中出现了 k i j，那么它就必然不是栈混洗。这是栈混洗所必须禁止的一种特征，称之为禁形。 Kunth在他的《The Art of Computer Programming》中证明了 一个排列permutation是一个栈混洗的充要条件就是其中不含禁形。 由此可以导出一个算法：不断地枚举所有的i j k的组合，但其复杂度高达$O(n^3)$。进一步地可以将这样一个判别的依据简化：判断栈混洗的充要条件是，对于每一对互异的i&lt;j，在排列中不会出现 j+1 i j 这样的一个模式，由此导出的枚举算法的复杂度是$O(n^2)$。 而借助栈结构可以实现一个线性时间$O(n)$的甄别算法，其思想是：完全按照栈混洗的定义引入三个栈，并且通过对栈混洗过程的模拟以一种验证的方式来判别某一个排列是否的确为栈混洗。具体地对于输出序列中的任何一个元素，都采用一种贪心算法的原则以S为中介 将其从A转移至B中，只要这个贪心的过程能够持续进行并最终将所有的元素顺利地从A转入B中，那么就可以判断它是一个栈混洗。反之每次通过pop操作试图从S中弹出当前的元素时，如果S已经变空，或者要弹出的元素不是S栈顶的元素，就可以立即判断这个栈混洗是非法的。 12345678910111213//参照LeetCode面试题31：栈的压入，弹出序列bool validateStackSequences(vector&lt;int&gt;&amp; pushed, vector&lt;int&gt;&amp; popped) &#123; stack&lt;int&gt; S; auto i=pushed.begin(); for(auto j=popped.begin();j!=popped.end();j++)&#123; while(S.empty()||S.top()!=*j) if(i==pushed.end()) return false; else S.push(*i++); S.pop(); &#125; return true;&#125; 3.3.栈混洗与括号匹配的关系n个元素的任何一个栈混洗都对应于中转栈S的n次push操作以及n次pop操作所构成的一个序列。如果将每push都换成一个左括号，而将每次pop对应一个右括号，就会发现同一元素所对应的那对push、pop操作都恰好对应于一对彼此匹配的括号。反过来由n对括号所构成的任何一个合法的表达式，实际上也可以解释为对n个元素进行栈混洗的一个合法的过程，也相应地对应于某一个输出的栈混洗。 因此合法的栈混洗序列与合法的括号匹配表达式之间存在一个一一对应的关系，n个元素的栈混洗有多少种n对括号所能构成的合法表达式也就有多少种。 4.中缀表达式求值中缀表达式求值问题属于栈结构的另一种典型应用场合：延迟缓冲，在中缀表达式求值这样一类线性扫描算法中我们并不能保证处理的速度和读取的速度同步，而往往是需要预读足够多的信息之后才能够确定足以处理的一个前缀。这节中只考虑语法正确的算术表达式，即给定任一语法正确的表达式S，计算出与之对应的数值。 4.1.表达式求值回顾括号匹配算法：对于任何的一个表达式，在其中找到一对彼此紧邻也因此相互配对的括号，将这样一对括号删去，从而在剩余表达式与原先表达式在是否匹配上互为充要条件的前提下，使得问题的规模也就是括号的对数有所减少。 对于表达式求值，我们依然要在表达式中寻找一个能够优先计算的子串，并且对它进行计算，然后将计算所得的数值重新放置在这个位置上，经过如此的转换之后，新的表达式的数值与原表达式的数值保持一致。如果将一个表达式的复杂度定义为其中包含的运算符的数目，那么这个过程依然是一个减而治之（decrease and conquer）的过程。因此这个算法具有单调性，最终将消除掉所有的运算符，从而得到最终的数值。下面是个具体的例子（计算次序由下到上）： 这样一个计算过程虽然在纸面上简便易行，但是如果面对比较长甚至非常长的表达式就将碰到很大的困难，这个困难就在于我们很难定位当前可以计算的那个运算符。如果以一种线性扫描的次序来处理表达式，每当扫到一个运算符的时候都未必能够确认它已经是可以计算的，也就是说计算次序未必与扫描的次序完全一致。 对于这样的一类问题，一种行之有效的办法就是借助栈结构，只需将所有已经扫描过的部分保存为一个栈，在所有已经扫描过的部分中有一些是能够及时处理的即在局部具有足够高的优先级，并且已经计算过的部分；而已经扫描过但是还不足以判断能够计算的部分将通过这个栈被缓冲起来，而我们的策略就是逐步地将尚未扫描的部分扫描并且处理掉。 求值算法 = 栈 + 线性扫描： 12345自左向由扫描表达式，用栈记录已扫描的部分（含已执行运算的结果） 在每一字符处 while(栈的顶部存在可优先计算的子表达式) 该子表式退栈；计算其数值；计算结果进栈 当前字符进栈，转入下一字符 相对于此前那种纸面操作这样一个过程更加接近于机器能够自动实现的层次，然而离最终的目标还有差距，原因在于我们每次在栈顶检出这个可以计算的子表达式，无论是2乘3还是10除以5都不是那么自然，而诀窍在于将运算符和运算数分别对待，即将运算符和运算数分别存入两个栈中。 4.2.实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103//定义优先级表const char pri[9][9] = &#123; // 当前运算符 // + - * / ^ ! ( ) \\0 /* + */&#123; '&gt;','&gt;','&lt;','&lt;','&lt;','&lt;','&lt;','&gt;','&gt;' &#125;, /* - */&#123; '&gt;','&gt;','&lt;','&lt;','&lt;','&lt;','&lt;','&gt;','&gt;' &#125;, /* * */&#123; '&gt;','&gt;','&gt;','&gt;','&lt;','&lt;','&lt;','&gt;','&gt;' &#125;, /* / */&#123; '&gt;','&gt;','&gt;','&gt;','&lt;','&lt;','&lt;','&gt;','&gt;' &#125;, /* ^ */&#123; '&gt;','&gt;','&gt;','&gt;','&gt;','&lt;','&lt;','&gt;','&gt;' &#125;, /* ! */&#123; '&gt;','&gt;','&gt;','&gt;','&gt;','&gt;',' ','&gt;','&gt;' &#125;, /* ( */&#123; '&lt;','&lt;','&lt;','&lt;','&lt;','&lt;','&lt;','=',' ' &#125;, /* ) */&#123; ' ',' ',' ',' ',' ',' ',' ',' ',' ' &#125;, /* \\0*/&#123; '&lt;','&lt;','&lt;','&lt;','&lt;','&lt;','&lt;',' ','=' &#125; //(栈顶运算符)&#125;;//解析数字void readNumber(char*&amp; p, Stack&lt;float&gt;&amp; stk) &#123; //将起始于p的子串解析为数值，并存入操作数栈 stk.push((float)(*p - '0')); //当前数位对应的数值进栈 while (isdigit(*(++p))) //只要后续还有紧邻的数字（即多位整数的情况），则 stk.push(stk.pop() * 10 + (*p - '0')); //弹出原操作数并追加新数位后，新数值重新入栈 if ('.' != *p) return; //此后非小数点，则意味着当前操作数解析完成 float fraction = 1; //否则，意味着还有小数部分 while (isdigit(*(++p))) //逐位加入 stk.push(stk.pop() + (*p - '0') * (fraction /= 10)); //小数部分&#125;// 由运算符得出编号int optr2rank(char op) &#123; switch (op) &#123; case '+':return 0; case '-':return 1; case '*':return 2; case '/':return 3; case '^':return 4; case '!':return 5; case '(':return 6; case ')':return 7; case '\\0': return 8; default:exit(-1); &#125;&#125;//比较运算符的优先级char orderBetween(char op1, char op2) &#123; return pri[optr2rank(op1)][optr2rank(op2)];&#125;//执行二元运算float calcu(float a, char op, float b) &#123; switch (op) &#123; case '+': return a + b; case '-': return a - b; case '*': return a * b; case '/': if (0 == b) exit(-1); return a / b; //注意：如此判浮点数为零可能不安全 case '^': return pow(a, b); default: exit(-1); &#125;&#125;//执行一元运算float calcu(float b) &#123; if (b &lt; 1.01) return 1; return b*(calcu(b-1)); //目前仅有阶乘，可照此方式添加 &#125;//算法主体部分，中缀表达式求值float evaluate(char* S ) &#123; Stack&lt;float&gt; opnd; //运算数栈 Stack&lt;float&gt; optr; //运算符栈 optr.push('\\0'); //尾哨兵'\\0'也作为头哨兵入栈 while (!optr.empty()) &#123; //逐个处理各字符，直至运算符栈空 if (isdigit(*S)) //若当前字符为操作数，则 readNumber(S, opnd); //读入（可能多位的）操作数 else //若当前字符为运算符，则视其余栈顶运算符之间优先级的高低 switch(orderBetween(optr.top(), *S))&#123; case '&lt;': //栈顶运算符优先级更低时 optr.push(*S); S++; //计算推迟，当前运算符进栈 break; case '=': //优先级相等（当前运算符为右括号或者尾部哨兵'\\0'）时 optr.pop(); S++; //脱括号并接收下一个字符 break; case '&gt;': &#123; //栈顶运算符优先级更高时，可实施相应的计算，并将结果重新入栈 char op = optr.pop(); //栈顶运算符出栈并续接至RPN末尾 if ('!' == op) &#123; //若属于一元运算符 opnd.push( calcu( opnd.pop() )); //实施一元计算，结果入栈 &#125; else &#123; //对于其它（二元）运算符 float pOpnd2 = opnd.pop(), pOpnd1 = opnd.pop(); //取出后、前操作数 opnd.push( calcu( pOpnd1, op, pOpnd2 ) ); //实施二元计算，结果入栈 &#125; break; &#125; &#125; &#125; return opnd.pop();&#125; 5.逆波兰表达式5.1.RPN逆波兰表达式（Reverse Polish notation，RPN），是一种由波兰数学家Jan Łukasiewicz在1920年引入的数学表达式，在逆波兰表达式中，所有操作符置于操作数的后面，不需要括号来标识操作符的优先级。在RPN中我们原先使用的运算符优先级和括号强制指定优先级都不存在了，因此RPN更适合机器来完成表达式的计算。 在由运算符（operator）和操作数（operand）组成的表达式中不使用括号（parenthesis-free）即可表示带优先级的运算关系。 相对于日常使用的中缀式（infix），RPN被称为后缀式（postfix） 作为补偿，须额外引入一个起分隔作用的元字符（比如空格） 相比于上节的中缀表示式计算，RPN的计算显得十分简便，只需引入一个辅助栈： 12345678910111213rpnEvaluate(expr) &#123; //假定RPN表达式epxr的语法正确 引入栈S，用于存放操作数； while(expr尚未扫描完毕)&#123; 读入expr的下一个元素x; if(x是操作数) 将X压入S; else&#123; //x是运算符 从栈中弹出运算符x所需数目的操作数; 对弹出的操作数实施x运算，并将运算结果重新压入S; &#125; &#125; 返回栈顶; //也是栈底&#125; 5.2.中缀表达式向RPN的转换中缀表达式向逆波兰表达式的转换可以按以下过程实现： 主要注意的是：在转换完成后，操作数的次序不会发生变换，而运算符的次序可能发生变换。 转换的具体算法实际上可以直接借助上节计算中缀表达式的算法，它在计算中缀表达式结果的同时，实际上也完成了向RPN的转换，重点关注下面代码的第6行和第12行。 1234567891011121314151617float evaluate(char* S string *RPN) &#123; /*.................................*/ while (!optr.empty()) &#123; //逐个处理各字符，直至运算符栈空 if (isdigit(*S)) &#123; //若当前字符为操作数，则 readNumber(S, opnd); *(RPN++) = to_string(opnd.top()); //将其压入RPN else //若当前字符为运算符 switch(orderBetween(optr.top(), *S))&#123; /*..............................*/ case '&gt;': &#123; //栈顶运算符优先级更高时，可实施相应的计算，则 char op = optr.pop(); *(RPN++) = op;//在执行相应的同时将其压入RPN /*.............................*/ &#125; &#125; return opnd.pop();&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"数据结构与算法（7）栈与队列","slug":"数据结构与算法（7）栈与队列","date":"2020-02-17T09:49:20.000Z","updated":"2020-02-17T11:48:18.235Z","comments":true,"path":"2020/02/17/数据结构与算法（7）栈与队列/","link":"","permalink":"http://nekomoon404.github.io/2020/02/17/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%887%EF%BC%89%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97/","excerpt":"","text":"栈和队列这对孪生兄弟都是线性序列的特例，它们在算法以及应用中都扮演着非常基本而重要的角色，本文主要介绍如何以ADT的形式来定义和规范它们的接口，以及如何借助此前学习过的序列结构（向量与列表）简洁高效地加以实现。 1.栈1.1操作与接口栈结构（Stack）依然是由一组元素组成的线性序列，与一般的序列不同的是，在任何时候我们只能够访问栈中的一个特定元素——最末端那个元素，而其余的元素在当前都是禁止访问的。 通常习惯于将可以访问的开放的这一端，称作顶端top，而不开放的那个盲端称作底部bottom。 一般意义下的栈以及它的操作可以由下面的一组图来表示，如果需要将某一个新的元素插入其中，那么只能将它作为最顶部的元素插入，这个动作我们也形象地称之为push。反过来如果需要从栈中取出某一个元素，那么也只能取出当前这个顶部的元素，这个元素被取出之后其余元素将依次向前递补，将会出现一个新的顶部元素，这样的一个过程称为pop。当然通常的栈还会提供另一个辅助的接口，查询顶部元素的数值而并不需要将它真正的弹出，那么这样一个动作称之为top。 1.2.操作实例接下来通过一个实例来了解栈结构各种接口的准确功能，以及它们组合之后所能达到的效果。 首先通过构造函数Stack()来创建一个空的栈，调用empty()来检查其是否为空，push(5)在栈中插入第一个元素，图中栈顶是在左侧，栈底是在右侧。push(3)即在栈顶也就是左侧插入元素3，pop(3)则将栈顶的元素删除，size()返回栈序列中元素的个数，top()返回栈顶元素的值。 需要留意的是，栈中元素入栈的出栈的次序，即相对而言后入栈的元素会更早出栈，这也是栈结构的一个非常独特的性质，即后进先出（LIFO），正是因为栈的这种特性使得它在很多算法中都有重要作用。 1.3.实现实际上既然栈可以视作是序列的一种受限后的特例，那么自然可以通过此前学过的向量或列表结构直接派生而得，即我们完全可以利用向量或者列表来模拟栈以及它的接口行为。 以向量为例，栈中有多少元素，向量中也对应地有多少个元素，如果约定首元素是栈的底部盲端，那么末元素也就是可操作的栈顶，按照这样一个思路就可以简洁地写出栈模板类。 123456template &lt;typename T&gt; class Stack : public Vector&lt;T&gt; &#123; //由向量派生public: //size()、empty()以及其它开放接口均可直接沿用 void push(T const &amp; e) &#123; insert(size(), e); &#125; //入栈 T pop() &#123; return remove(size() - 1); &#125; //出栈 T &amp; top() &#123; return (*this)[size() - 1]; &#125; //取顶&#125;; 对于向量结构而言无论是插入操作还是删除操作，所需要的时间都线性正比于插入和删除位置的后继的数目。对于栈结构，无论是插入操作还是删除操作都是在向量的末端进行，因此所有这些操作接口的时间复杂度都是常数的，即为$O(1)$。如果将向量的首元素作为栈顶，那么每次插入和删除操作的时间复杂度就会变成$O(n)$。 2.队列队列（queue）也是一种特殊的线性序列，想想生活中机场安检窗口的队列或者超市等待付款的队列，用数据额结构的语言来说它们都构成一个线性的序列，与栈一样它也是一个受限的序列，不同的是队列的一端只能够出，另一端只能怪进。通常，允许插入的那一端被称作尾部，允许删除的一端被称为头部。 队列也是受限的序列： 只能在队尾插入（查询）：enqueue() + rear() 只能在队头删除（查询）：dequeue() + front() 队列中元素入队和出队的次序与栈相反： 先进先出（FIFO） 后进后出（LILO） 下面是一个队列的操作具体事例： 2.2.实现与栈同理既然队列也是属于序列，自然可以利用此前已经实现的最基本的向量以及列表结构直接派生而得，这里选用列表来实现队列。 123456template &lt;typename T&gt; class Queue : public List&lt;T&gt; &#123; //由列表派生public: //size()与empty()以及其它开放接口均可直接沿用 void enqueue(T const &amp; e) &#123; insertAsLast(e); &#125; //入队 T dequeue() &#123; return remove( first() ); &#125; //出队 T &amp; front() &#123; return first()-&gt;date; &#125; //查询队首元素&#125;; 由于列表结构的特性，无论是enqueue() ，dequeue()还是front()这些接口都能够在常数的时间内完成，即$O(1)$的时间。 这种基于以前的工作来进一步完成新的任务的思路，不仅使得我们的工作可以快速推进，而且使得整个工作的系统性和安全性都能得到保障(ง •_•)ง 队列结构在此后的图算法以及其它的场合都有广泛的应用，所以这里只是首先简要地介绍它的接口定义，以及它在C++中的实现方式。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"Cpp基础（10）继承和派生","slug":"Cpp基础（10）继承和派生","date":"2020-02-17T03:57:42.000Z","updated":"2020-02-26T10:40:19.695Z","comments":true,"path":"2020/02/17/Cpp基础（10）继承和派生/","link":"","permalink":"http://nekomoon404.github.io/2020/02/17/Cpp%E5%9F%BA%E7%A1%80%EF%BC%8810%EF%BC%89%E7%BB%A7%E6%89%BF%E5%92%8C%E6%B4%BE%E7%94%9F/","excerpt":"","text":"1.继承和派生 继承的概念：在定义一个新的类B时，如果该类与某个已有的类A相似（指的是B拥有A的全部特点），那么就可以把A作为一个基类，而把B作为基类的一个派生类（也称为子类）。 派生类是通过对基类进行修改和扩充得到的。在派生类中，可以扩充新的成员变量和成员函数。 派生类一经定义后，可以独立使用，不依赖与基类。 派生类拥有基类的全部成员变量和成员函数，包括private，public，protected 派生类的写法：class 派生类名: public 基类名 { }; 派生类的内存空间：派生类对象的体积，等于基类对象的体积，再加上派生类对象自己的成员变量的体积。在派生类对象中，包含着基类对象，而且基类对象的存储位置位于派生类对象新增的成员变量之前。 123456class CBase&#123; int v1, v2;&#125;;class CDerived:public CBase&#123; int v3;&#125; 示例：写了一个学生的类Student，再写一个Student类的派生类UndergraduateStudent，补充和修改一些功能。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;class Student &#123; //学生类private: string name; string id; char gender; int age;public: void PrintInfo() &#123; cout &lt;&lt; \"Name: \"&lt;&lt;name&lt;&lt; endl; cout &lt;&lt; \"ID: \" &lt;&lt; id &lt;&lt; endl; cout &lt;&lt; \"Age: \" &lt;&lt; age &lt;&lt; endl; cout &lt;&lt; \"Gender: \" &lt;&lt; gender &lt;&lt; endl; &#125; void SetInfo(const string &amp;_name, const string &amp;_id, int _age, char _gender) &#123; name = _name; id = _id; age = _age; gender = _gender; &#125; string GetName() &#123; return name; &#125; &#125;;class UndergraduateStudent :public Student &#123; //本科生类，继承了Student类的派生类private: string department;public: void QualifiedForBaoyan() &#123; cout &lt;&lt; \"qualified for baoyan\" &lt;&lt; endl; &#125; void PrintInfo() &#123; //派生类中修改基类中的PrintInfo Student::PrintInfo(); //调用基类的PrintInfo cout &lt;&lt; \"Department:\" &lt;&lt; department &lt;&lt; endl; &#125; void SetInfo(const string &amp;_name, const string &amp;_id, int _age, char _gender, const string &amp;_department) &#123; Student::SetInfo(_name, _id, _age, _gender); department = _department; &#125; &#125;; int main() &#123; UndergraduateStudent S; S.SetInfo(\"Harry Potter\", \"20200217\", 19, 'M', \"Machine Learning\"); cout &lt;&lt; S.GetName() &lt;&lt; \" \"; S.QualifiedForBaoyan(); S.PrintInfo(); return 0;&#125; 程序运行结果： 2.继承关系和复合关系类与类之间有两种关系： 继承：“是”关系 基类A，B是基类A的派生类 逻辑上要求：“一个B对象也是一个A对象”，比如上节中Student类和UndergraduateStudent类。 复合：“有”关系 类C中“有”成员变量k，k是类D的对象，则C和D是复合关系 一般逻辑上要求：“D对象是C对象的固有属性或组成部分”。 举一个简单的例子：如果要写一个小区养狗管理程序，需要写一个“业主”类和“狗”类，狗的主人即是业主，规定狗只能有一个主人，而一个业主最多可以有5条狗。 正确的写法：为”狗“类设一个”业主“类的对象指针；为”业主“类设一个”狗“类的对象指针数组。 1234567class Master;class Dog&#123; Master *m;&#125;;class Master&#123; Dog dogs[10];&#125;; 而以下的做法都是错误的或者不好的： 1234567class Dog;class Master&#123; Dog dogs[10];&#125;;class Dog&#123; Master m;&#125;; //会造成循环定义，不定确定Master和Dog所需的内存空间 1234567class Dog;class Master&#123; Dog *dogs[10];&#125;;class Dog&#123; Master m;&#125;; //如何维护不同的狗所属的相同的主人的信息的一致性？改了一条狗，其他狗也要更着改，十分麻烦 1234567class Master;class Dog&#123; Master *m;&#125;;class Master&#123; Dog dogs[10];&#125;; //这样狗对象都包含在业主对象里面，只能通过修改业主信息来修改狗的信息 3.基类和派生类有同名成员的情况 基类和派生类有时会拥有相同名称的成员变量或者成员函数。 12345678910111213141516171819202122232425class base&#123; int j;public: int i; void func();&#125;;class derived:public base&#123;public: int i; //与基类相同的成员对象 void access(); void func(); //与基类相同的成员函数&#125;;void derived::access()&#123; j = 5; //错误的，j是基类的私有成员 i = 5; //引用的是派生类的i base::i = 5; //引用的是基类的i func(); //派生类的成员函数 base::func(); //基类的成员函数 &#125;int main()&#123; derived obj; obj.i = 3; //对派生类的成员变量赋值 obj.base::i = 3; //对派生类对应的基类部分的成员变量赋值&#125; obj对象占用的存储空间： 注意：一般来说，基类和派生类不定义同名成员变量。 4.访问范围说明符 基类的private成员，可以被下列函数访问： 基类的成员函数 基类的友元函数 基类的public成员，可以被下列函数访问： 基类的成员函数 基类的友元函数 派生类的成员函数 派生类的友元函数 其他的函数 基类的protected成员，可以被下列函数访问： 基类的成员函数 基类的友元函数 派生类的成员函数可以访问当前对象的基本的protected成员 1234567891011121314151617181920212223242526class Father &#123;private: int nPrivate;public: int nPublic;protected:int nProtected;&#125;;class Son:public Father&#123; void AccseeFather() &#123; nPublic = 1; //Ok nPrivate = 1; //Wrong nProtected = 1; //Ok，访问从基类基础的protected成员 Son f; f.nProtected = 1; //Wrong,f不是AccseeFather作用的当前对象 &#125;&#125;;int main() &#123; Father f; Son s; f.nPublic = 1; //Ok s.nPublic = 1; //Ok f.nProtected = 1; //Wrong f.nPublic = 1; //Wrong s.nProtected = 1; //Wrong s.nPublic = 1; //Wrong return 0;&#125; 5.派生类的构造函数 派生类对象包含基类对象 执行派生类构造函数之前，先执行基类的构造函数 派生类交代基类初始化，具体形式： 1234构造函数名(形参表):基类名(基类构造函数实参表)&#123; &#125; 看一个具体的例子 ： 1234567891011121314151617181920212223242526272829303132class Bug &#123;private: int nLegs; int nColor;public: int nType; Bug(int _legs, int _color); void PrintBug() &#123; &#125;;&#125;;class FlyBug :public Bug &#123; int nWings;public: FlyBug(int _legs, int _color, int _wings);&#125;;Bug::Bug(int _legs, int _color) &#123; nLegs = _legs; nColor = _color;&#125;//错误的FlyBug构造函数FlyBug::FlyBug(int _legs, int _color, int _wings) &#123; nLegs = _legs; //Wrong，不能访问基类的私有成员 nColor = _color; //Wrong，不能访问基类的私有成员 nType = 1; //OK nWings = _wings;&#125;//正确的FlyBug构造函数FlyBug::FlyBug(int _legs, int _color, int _wings) :Bug(_legs, _color) &#123; nWings = _wings;&#125; 在创建派生类的对象时： 需要调用基类的构造函数：初始化派生类对象中从基类继承的成员 在执行一个派生类的构造函数之前，总是先执行基类的构造函数 调用基类构造函数的两种方式： 显式方式：派生类的构造函数中 $\\to$ 基类的构造函数提供参数 FlyBug::FlyBug(int _legs, int _color, int _wings) :Bug(_legs, _color) 隐式方式：派生类的构造函数中，省略基类构造函数时，会自动调用基类的默认构造函数 派生类的析构函数被执行时，执行完派生类的析构函数后，自动调用基类的析构函数 1234567891011121314151617181920212223class Base &#123;public: int n; Base(int i):n(i)&#123; cout &lt;&lt; \"Base\" &lt;&lt; n &lt;&lt; \" constructed\" &lt;&lt; endl; &#125; ~Base() &#123; cout &lt;&lt; \"Base\" &lt;&lt; n &lt;&lt; \" destructed\" &lt;&lt; endl; &#125;&#125;;class Derived:public Base &#123;public: Derived(int i) :Base(i) &#123; cout &lt;&lt; \"Derived constructed\" &lt;&lt; endl; &#125; ~Derived()&#123; cout &lt;&lt; \"Derived destructed\" &lt;&lt; endl; &#125;&#125;;int main() &#123; Derived Obj(3); return 0;&#125; 程序运行结果为： 对于包含成员对象的派生类的构造函数，创建派生类的对象时： 调用基类的构造函数 $\\to$ 初始化派生类对象中从基类继承的成员 调用成员对象类的构造函数 $\\to$ 初始化派生类对象中成员对象 执行派生类的构造函数 析构时： 执行派生类的析构函数 调用成员对象类的析构函数 调用基类的析构函函数（仍然遵循先构造的后析构的规则） 6.public继承的赋值兼容规则1234class base&#123; &#125;;class derived:public base&#123; &#125;;base b;derived d; 派生类的对象可以赋值给基类对象：b = d; 派生类的对象可以初始化基类引用：base &amp; br = d; 派生类的对象的地址可以赋给基类指针：base * bp = &amp; d; 注意上述规则不能颠倒，且如果派生方式是private或者protected，上述三条都不可行 7.直接基类与间接基类C++中类的派生可以是很多层的。 如类A派生类B，类B派生类C，类C派生类D： 类A是类B的直接基类； 类B是类C的直接基类，类A是类C的间接基类； 类C是类D的直接基类，类A、B是类D的间接基类； 在声明派生类时，只需要列出它的直接基类；派生类沿用着类的层次自动向上继承它的间接基类。 派生类的成员包括：派生类自己定义的成员，直接基类中的所有成员，所有间接基类的全部成员 当执行构造函数时，从顶层基类开始，依次往下执行基类的构造函数，最后执行自己的构造函数。 下面看一个例子：Base $\\to$ Derived $\\to$ MoreDerived 1234567891011121314151617181920212223242526272829303132333435#include&lt;iostream&gt;using namespace std;class Base &#123;public: int n; Base(int i) :n(i) &#123; cout &lt;&lt; \"Base\" &lt;&lt; n &lt;&lt; \" constructed\" &lt;&lt; endl; &#125; ~Base() &#123; cout &lt;&lt; \"Base\" &lt;&lt; n &lt;&lt; \" destructed\" &lt;&lt; endl; &#125;&#125;;class Derived :public Base &#123;public: Derived(int i) :Base(i) &#123; cout &lt;&lt; \"Derived constructed\" &lt;&lt; endl; &#125; ~Derived() &#123; cout &lt;&lt; \"Derived destructed\" &lt;&lt; endl; &#125;&#125;;class MoreDerived :public Derived &#123;public: MoreDerived(int i) :Derived(i) &#123; cout &lt;&lt; \"MoreDerived constructed\" &lt;&lt; endl; &#125; ~MoreDerived() &#123; cout &lt;&lt; \"MoreDerived destructed\" &lt;&lt; endl; &#125;&#125;;int main() &#123; MoreDerived Obj(3); return 0;&#125; 程序运行结果为：","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"Cpp基础（9）运算符重载","slug":"Cpp基础（9）运算符重载","date":"2020-02-16T01:55:33.000Z","updated":"2020-02-16T07:55:33.000Z","comments":true,"path":"2020/02/16/Cpp基础（9）运算符重载/","link":"","permalink":"http://nekomoon404.github.io/2020/02/16/Cpp%E5%9F%BA%E7%A1%80%EF%BC%889%EF%BC%89%E8%BF%90%E7%AE%97%E7%AC%A6%E9%87%8D%E8%BD%BD/","excerpt":"","text":"1.基本概念 运算符：C++预定义表示对数据的运算 +, - , *, /, %, ^, &amp;, ~, !, |, =, &lt;&lt;, &gt;&gt;, != ……. 只能用于基本的数据类型：整型，实型，字符型，逻辑型，…… C++提供了数据抽象的手段：用户自己定义数据类型——类 调用类的成员函数 $\\to$ 操作它的对象，有时会不方便 比如在数学上，两个复数可以直接进行+, -等运算；而在C++中，直接将+，-作用于复数（我们定义的类）是不允许的 运算符重载：对抽象数据类型也能够直接使用C++提供的运算符 程序更简洁 代码更容易理解 运算符重对已有的运算符赋予多重的含义 在使用同一运算符作用于不同类型的数据时 $\\to$ 不同类型的行为 目的是扩展C++中提供的运算符的适用范围，以用于类所表示的抽象数据类型 运算符重载的实质是函数重载 ​ 返回值类型 operator 运算符(形参表) { …… } 在程序编译时： 把含运算符的表达式 $\\to$ 对运算符函数的调用 把运算符的操作数 $\\to$ 运算符函数的参数 运算符被多次重载时，根据实参的类型决定调用哪个运算符函数 运算符可以被重载为普通函数，也可以被重载为成员函数 运算符重载为普通函数时，参数个数为运算符目数（如+为2） 1234567891011121314151617class Complex &#123;public: Complex(double r = 0.0, double i = 0.0) &#123; real = r; imaginary = i; &#125; double real; double imaginary;&#125;;Complex operator+(const Complex &amp;a, const Complex &amp;b) &#123; return Complex(a.real+b.real,a.imaginary+b.imaginary)&#125;int main() &#123; Complex a(1, 2), b(2, 3), c; c = a + b; //相当于operator+(a,b)，会传递两个参数a,b return 0;&#125; 运算符重载为成员函数时，参数个数为运算符目数减一 123456789101112131415161718192021222324class Complex &#123;public: Complex(double r = 0.0, double i = 0.0):real(r),imaginary(i) &#123; &#125; //constructor Complex operator+(const Complex &amp;); //addition Complex operator-(const Complex &amp;); //subtractionprivate: double real; double imaginary;&#125;;//Overload addition operatorComplex Complex::operator+(const Complex &amp;operand2) &#123; return Complex(real + operand2.real, imaginary + operand2.imaginary)&#125;//Overload subtraction operatorComplex Complex::operator-(const Complex &amp;operand2) &#123; return Complex(real - operand2.real, imaginary - operand2.imaginary)&#125;int main() &#123; Complex x(4.3,8.2),y(3.3,1.1),z; z = x + y; //相当于z = x.operator(y)，即只传入一个参数 z = x - y; //相当于z = x.operator(y) return 0;&#125; 2.赋值运算符’=’重载2.1.基本实现当类和对象这个新概念对引入的时候，原先一些传统的运算符并不能直接作用在我们自己定义的类型的对象上，但是唯有赋值运算符’=’是可以直接使用的，它会会按对象的数据成员一一完成赋值。当我们对’=’有更多的要求时，比如两边类型可以不匹配，或者除了完成普通的赋值外还要实现其他功能，就需要重载运算符’=’。 赋值运算符’=’只能重载为成员函数 例子：编写一个长度可变的字符串类String，包含一个char *类型的成员变量 $\\to$ 指向动态分配的存储空间，该存储空间用于存放&#39;\\0&#39;结尾的字符串。 123456789101112131415161718192021222324252627282930313233343536#include&lt;iostream&gt;using namespace std;class String &#123;private: char *str;public: String():str(NULL)&#123;&#125; //构造函数，初始化str为NULL const char * c_str() &#123; return str; &#125; char * operator=(const char * s); ~String();&#125;;char * String::operator=(const char *s) &#123; if (str) delete[] str; if (s) &#123; str = new char[strlen(s) + 1]; strcpy(str, s); &#125; else str = NULL; return str;&#125;String::~String() &#123; if (str) delete[] str;&#125;int main() &#123; String s; s = \"Good Luck\"; //相当于s.operator = \"Good Luck\"; cout &lt;&lt; s.c_str() &lt;&lt; endl; //String s2 = \"hello!\"; 这条语句是错误的，我们并没有定义这样的构造函数 s = \"C++\"; cout &lt;&lt; s.c_str &lt;&lt; endl; return 0;&#125; 2.2.重载赋值运算符的意义—浅复制和深复制 浅复制/浅拷贝：执行逐个字节的复制工作 比如利用我们上节定义的String类，它有两个对象S1和S2，利用我们已经重载的复制运算符可以实现直接将一个字符串赋值给一个String对象，如果想进一步将S2直接赋值给S1，在语法上也是没问题的，它会实现浅复制，即将S2对象中的内容逐字节地复制给S1，实际上就是S1.str = S2.str，两个指针就指向了同一块地址，但这会引发一些问题。 1234String S1, S2;S1 = \"this\";S2 = \"that\";S1 = S2; 当执行了S1 = S2 后，S1.str和S2.str两个指针指向了同一块地址，这引发了两个问题：第一个是存放”this“字符串的内存没有任何指针来对它进行控制，成为了一个内存垃圾；当S1和S2同时消亡的时候，存放”that“的内存会被释放两次，这会导致严重的内存错误。 深复制/深拷贝：将一个对象中指针变量指向的内容 $\\to$ 复制到另一个对象中指针成员对象指向的地方 123456String &amp; operator=(const String &amp;s) &#123; if (str) delete[] str; str = new char[strlen(s.str) + 1]; strcpy(str, s.str); return *this; //注意这里返回的是this指针&#125; 通过上面的赋值运算符深拷贝的实现，我们是否已经完全实现了String对象的赋值呢？仔细考虑一下，还会一点小疏漏，就是当执行s = s，即把当前对象赋值给其自身，那么在刚才的重载函数中就会出现一些小问题，当执行strcpy(str, s.str)时会发现我们已经把原来s.str所指的内存空间中的内容删掉了，所以我们要在原来代码基础上加一条if语句。 1234567String &amp; operator=(const String &amp;s) &#123; if (str == s.str) return *this; if (str) delete[] str; str = new char[strlen(s.str) + 1]; strcpy(str, s.str); return *this;&#125; 那么上面定义的String类还有其他问题嘛？ 需要注意的是：为String类编写复制构造函数时，会面临和’=’同样的问题，如果采用浅拷贝或者调用默认的复制构造函数，就会出现问题，为此我们也要采用深拷贝的方式 12345678String(String &amp;s) &#123; if (s.str) &#123; str = new char[strlen(s.str) + 1]; strcpy(str, s.str); &#125; else str = NULL;&#125; 3.运算符重载为友元函数 通常，将运算符重载为类的成员函数 重载为友元函数的情况： 成员函数不能满足要求 普通函数又不能访问类的私有成员 123456789101112131415class Complex &#123;public: Complex(double r,double i):real(r),imag(i) &#123;&#125; Complex operator+(double r);private: double real, imag;&#125;;Complex Complex::operator+(double r) &#123; return Complex(real + r, imag);&#125;int main()&#123; Complex c; c = c + 5; return 0;&#125; 上面的例子中，能实现c = c + 5，相当于c = c.operator +(5)，但不能实现c = 5 + c，如果要实现后者，就需要将’+’重载为普通函数，且要能访问Complex类的私有成员real，因此要将’+’重载为友元函数。 1234567891011121314class Complex &#123;public: Complex(double r,double i):real(r),imag(i) &#123;&#125; Complex operator+(double r); friend Complex operator+(double r, const Complex &amp;c);private: double real, imag;&#125;;Complex Complex::operator+(double r) &#123; return Complex(real + r, imag);&#125;Complex operator+(double r, const Complex &amp;c) &#123; return Complex(c.real + r, c.imag);&#125; 4.实例-长度可变的整型数组类C++中的数组的大小（size）是固定的，不能按存放元素的多少自动调整容量，为此我们想自己定义一个长度可变的整型数组类，可以实现下面的功能： 12345678910111213141516171819int main() &#123; //要编写可变长度的整型数组类，使用如下功能 CArray a; //开始里的数值是空的 for (int i = 0; i &lt; 5; ++i) a.push_back(i); CArray a2, a3; for (int i = 0; i &lt; a.length(); ++i) cout &lt;&lt; a[i] &lt;&lt; \" \"; cout &lt;&lt; endl; a2 = a3; //a2这时为空 for (int i = 0; i &lt; a2.length(); ++i) //a2.length()为0，所以没有输出 cout &lt;&lt; a2[i] &lt;&lt; \" \"; cout &lt;&lt; endl; a[3] = 100; CArray a4(a); //复制构造 for (int i = 0; i &lt; a4.length(); i++) cout &lt;&lt; a4[i] &lt;&lt; \" \"; cout &lt;&lt; endl; return 0;&#125; 程序的输出结果是： 分析一下需要实现的功能： 要用动态分配的内存来存放数组元素，需要一个指针成员变量 要重载’=’ 要重载’[]’，即实现a2[i]，取下标 有复制构造函数 这个长度可变的整型数组类可以有下面的代码实现，还是有一些需要注意的点的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970class CArray &#123; int size; //数组元素的个数 int *ptr; //指向动态分配的数组public: CArray(int s = 0); //s代表数组元素的个数 CArray(CArray &amp;a); ~CArray(); void push_back(int v); //用于在数组尾部添加一个元素v int length() &#123; return size; &#125; //返回数组元素个数 CArray &amp; operator=(const CArray &amp; a); //用于数组对象间的赋值 int &amp; operator[](int i) &#123; //用于支持根据下标访问数组元素，如n = a[i] 和 a[i] = 4 //若返回类型为int，则不能实现a[i] = 4，因为非引用的函数返回值不能作为左值 return ptr[i]; &#125;&#125;;CArray::CArray(int s) :size(s) &#123; if (s == 0) ptr = NULL; else ptr = new int[s];&#125;CArray::CArray(CArray &amp;a) &#123; if (!a.ptr) &#123; ptr = NULL; size = 0; return; &#125; ptr = new int[a.size]; memcpy(ptr, a.ptr, sizeof(int)*a.size); size = a.size;&#125;CArray::~CArray() &#123; if (ptr) delete[] ptr;&#125;CArray &amp; CArray::operator=(const CArray &amp;a) &#123; //重载后的赋值号的作用是使'='左边对象里存放的数组，大小和内容都和右边的对象一样 if (ptr == a.ptr) return *this; if (a.ptr == NULL) &#123; if (ptr) delete[] ptr; ptr = NULL; size = 0; return *this; &#125; if (size &lt; a.size) &#123; if (ptr) delete[] ptr; ptr = new int[a.size]; &#125; memcpy(ptr, a.ptr, sizeof(int)*a.size); size = a.size; return *this;&#125;void CArray::push_back(int v) &#123; if (ptr) &#123; int *tmpPtr = new int[size + 1]; memcpy(tmpPtr, ptr, sizeof(int)*size); delete[] ptr; ptr = tmpPtr; &#125; else ptr = new int[1]; ptr[size++] = v;&#125; 5.流插入和流提取运算符的重载 cout是在iostream中定义的，ostream类的对象。 “&lt;&lt;” 能用在cout上是因为在iostream里对”&lt;&lt;”进行了重载 cin是在iostream中定义的，istream类的对象。 “&gt;&gt;” 能用在cin上是因为在istream里对”&gt;&gt;”进行了重载 例子：假设c是Complex复数类的对象，实现cout&lt;&lt;c; 能输出”a+bi”形式，cin&gt;&gt;c 能从键盘接受”a+bi”形式的输入。 为此我们需要把&lt;&lt;和&gt;&gt;重载成全局函数，因为它们已经是ostream和istream的成员函数了；又因为它们需要访问Complex类的私有成员，因此要声明成Complex类的友元函数。 123456789101112131415161718192021222324252627282930313233343536#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;cstdlib&gt;using namespace std;class Complex &#123; double real, imag;public: Complex(double r = 0, double i = 0) :real(r), imag(i) &#123; &#125;; friend ostream &amp; operator&lt;&lt;(ostream &amp;os, const Complex &amp;c); friend istream &amp; operator&gt;&gt;(istream &amp;is, Complex &amp;c);&#125;;ostream &amp; operator&lt;&lt;(ostream &amp; os, const Complex &amp;c) &#123; os &lt;&lt; c.real &lt;&lt; \"+\" &lt;&lt; c.imag &lt;&lt; \"i\"; //以“a+bi”的形式输出 return os;&#125;istream &amp; operator&gt;&gt;(istream &amp; is, Complex &amp;c) &#123; string s; is &gt;&gt; s; //将\"a+bi\"作为字符串读入，\"a+bi\"中间不能有空格 int pos = s.find(\"+\", 0); string sTmp = s.substr(0, pos); //分离出代表实部的字符串 c.real = atof(sTmp.c_str()); //atof库函数能将const char* 指针指向的内容转换成float sTmp = s.substr(pos + 1, s.length() - pos - 2); //分离出代表虚部的字符串 c.imag = atof(sTmp.c_str()); return is;&#125;int main() &#123; Complex c; int n; cin &gt;&gt; c &gt;&gt; n; cout &lt;&lt; c &lt;&lt; \",\" &lt;&lt; n; return 0;&#125; 运行结果示例： 6.自加/自减运算符的重载 自加++/自减—运算符有前置/后置之分： 前置++（++a）：先加1，再执行语句 后置++（a++）：先执行语句，再加1 前置运算符作为一元运算符重载： 重载为成员函数：T operator++(); ， T operator--(); 重载为全局函数：T operator++(T); ， T operator--(T); 如++obj，obj.operator++()，operator++(obj)都是调用上述前置运算符 后置运算符作为二元运算符重载：（多写的参数只是标记重载的运算符为后置，并无具体意见） 重载为成员函数：T operator++(int); ， T operator--(int); 重载为全局函数：T operator++(T, int); ， T operator--(T, int); 如obj++，obj.operator++(0)，operator++(obj, 0)都是调用上述前置运算符 例子：我们希望设计一个CDemo对象来实现下面的功能： 123456789101112int main() &#123; CDemo d(5); cout &lt;&lt; (d++) &lt;&lt; \",\"; //等价于d.operator++(0); cout &lt;&lt; d &lt;&lt; \",\"; cout &lt;&lt; (++d) &lt;&lt; \",\"; //等价于d.operator++(); cout &lt;&lt; d &lt;&lt; endl; cout &lt;&lt; (d--) &lt;&lt; \",\"; //等价于operator--(d,0); cout &lt;&lt; d &lt;&lt; \",\"; cout &lt;&lt; (--d) &lt;&lt; \",\"; //等价于operator--(0); cout &lt;&lt; d &lt;&lt; endl; return 0;&#125; 程序运行结果： 我们首先需要设计CDemo对象的自加自减运算符，再注意cout&lt;&lt;d语句它会将CDemo对象直接输出为整型数，而cout并没有这样的功能，它不支持任意自定义类型的输出，因此要设计一个强制类型转换符的一个运算符重载。 123456789101112131415161718192021222324252627282930class CDemo &#123;private: int n;public: CDemo(int i=0):n(i)&#123; &#125; CDemo operator++(); //前置++，重载为成员函数 CDemo operator++(int); //后置++ operator int() &#123; return n; &#125; friend CDemo operator--(CDemo &amp;); //前置--，重载为全局函数(仅做示例，重载为成员函数亦可） friend CDemo operator--(CDemo &amp;, int); //后置--&#125;;CDemo CDemo::operator++() &#123; //前置++ n++; return *this;&#125;CDemo CDemo::operator++(int k) &#123; //后置++ CDemo tmp(*this); n++; return tmp; //返回修改前的对象&#125;CDemo operator--(CDemo &amp; d) &#123; //前置++ d.n--; return d;&#125;CDemo operator--(CDemo &amp;d,int k) &#123; //后置++ CDemo tmp(d); d.n--; return tmp; //返回修改前的对象&#125; 注意语句operator int() { return n; } ： 此时Int作为一个类型强制转换运算符被重载（而不是整型类型了） 12CDemo s;(int)s; //等效于s.int(); 类型强制转换运算符重载时： 不能写返回值类型 实际上返回值类型——类型强制转换运算符代表的类型 运算符重载的注意事项： C++不允许定义新的运算符 重载后运算符的含义应该符合日常习惯 运算符重载不改变运算符的优先级 一下运算符不能被重载：’.‘，’.*‘，’::‘，’?:‘，sizeof 重载运算符()，[]，-&gt;，=时，重载函数必须声明为类的成员函数","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"数据结构与算法（6）选择排序与插入排序","slug":"数据结构与算法（6）选择排序与插入排序","date":"2020-02-15T11:22:47.000Z","updated":"2020-02-16T02:22:47.000Z","comments":true,"path":"2020/02/15/数据结构与算法（6）选择排序与插入排序/","link":"","permalink":"http://nekomoon404.github.io/2020/02/15/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%886%EF%BC%89%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%E4%B8%8E%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/","excerpt":"","text":"1.选择排序与向量那一章的做法一样列表这一章的接下来几节，也将针对列表这种结构的特点介绍几种典型的排序算法，这一节要介绍的是选择排序。 1.1.构思选择排序在生活中其实很常见，举个例子，假设有一篮子苹果或大或小，如果你需要从小到大把它们排成一个序列，那你会怎么做呢？常用的一种方法应该是这样的：首先在其中挑选出最大的，接下来在剩下的中挑选出最大的，即整体中次大的，如此反复，直到篮子中只剩下最后那只苹果。 可以看出在这个过程中我们所采用的策略，原则非常的简单，就是每一次只需在篮子中找出当前最大的那只苹果，并且随即将它转移到桌子上。这里每一步所做的实质的关键动作就是所谓的选择select，而基于这样一种选择过程和策略的排序算法就叫作选择排序（selection sort）。 回顾一下此前所介绍过的起泡排序bubble sort，其实它也是selection sort。在起泡算法的过程中它的不变性可以表述为整个序列总是可以分成前后两个部分，其中后面的一个部分是由一系列已经就位的元素组成的，它们构成了一个有序的sorted的部分；而前半部分的元素数值的分布是随机的或大或小，但是就数值而言它们都绝对不会超过图中的黄线。这个算法是由一趟又一趟的扫描交换构成的，在接下来的一趟扫描交换中最实质的操作实际上是当前最大的那个元素和其后的元素不断地交换，直到它最终被移送到黄色区域的末尾。于是这个有序的S部分就可以向左侧拓展，这也就是起泡排序算法的单调性。 纵观这个过程并且与上面的选择排序做一对比，我们会发现二者之间的相似之处：也就是每一次扫描交换的作用其实实质上都等价于找到这个最大节点，并且随即将它转移至有序和无序子序列的分界点。从这个角度来看起泡排序确实就是一个不折不扣的选择排序。 那反过来既然如此还有什么必要去专门讨论选择排序呢？原因在于起泡排序的效率太低，在最坏情况下它需要$O(n^2)$的时间，而借助列表结构这个效率是完全可以改进的。在起泡排序的过程中所执行的计算无非两类，一类就是相邻元素的比较，另一类才是元素位置的交换。然而很遗憾在这里将最大元素转移至合适的位置这样一个任务，是由一系列的短距离实际上是相邻元素之间的移动构成的，这种“小步慢跑式”的移动正是低效率的来源。 所以既然我们最终无非就是要将这个最大的元素挪到最终的位置，为何不直接一次性地来完成这项工作呢？这正是我们改进的思路。 1.2.实现选择排序selection sort可以实现为下面一段代码： 123456789//列表的选择排序算法：对起始于位置p的n个元素排序，valid(p) &amp;&amp; rank(p) + n &lt;= sizetemplate &lt; typename T&gt; void List&lt;T&gt;::selectionSort(Posi(T) p, int n) &#123; Posi(T) head = p-&gt;pred; ePosi(T) tail = p; //待排序区间为(head, tail) for (int i = 0; i &lt; n; i++) tail = tail-&gt;succ; //head/tail可能是头/尾哨兵 while (1 &lt; n) &#123; //反复从（非平凡的）待排序区间内找出最大者，并将其移至无序区间末尾 insertBefore(tail, remove(selectMax(head-&gt;succ, n))); tail = tail-&gt;pred; n--; //待排序区间、有序区间的范围、均同步更新 &#125;&#125; 注意一下这个接口的语义，待排序的元素实际上是在列表中起始于位置p的连续n个元素，在下面的图示中，如果L是整个的列表那么待排序的区间，从节点p开始第n个节点是用虚线表示的，言下之意这里依然采用了此前左闭右开的区间定义习惯。相应地 这里引入了两个界桩head和tail。 经过入口处预处理的两步，head和tail分别对应的位置就是在这个图中的h以及p+n，在接下来的循环中 head始终不变，而tail会每次向前移动一个节点。而从tail开始到最初的界桩之间的范围正是已经排序的区间，而尚未排序的的前缀U是从p一直到T左闭右开的区间，这也是这个算法的不变性。 从算法可以看出每次我们都调用selectMax的接口，从前缀U这个区间中找到当前的最大者，n会随着迭代地进行相应地下降，从记录前缀U的宽度。在这个图中被选取出来的最大节点以M来表示，我们将M节点摘出来插入至S区间的首节点，tail的前端，相当于将M移动到此前的T的紧邻左侧，并且随即将T移动到新的这个节点处（M处），从而使有序的部分向左拓展一个单元。这个算法将持续地迭代下去，直到n最终缩减到平凡的情况，从而完成整个指定区域的排序。 1.3.推敲现在来重新审视一下这个算法并且对其中的几个细节来做一推敲，第一个问题是算法中套用了此前所实现的remove和insert这两个标准的操作接口，使整个代码实现更加简洁，但从效率而言还是值得推敲的。这其中的原因在于，这两个操作都要使用到动态空间分配，也就是insert的时候必须要用new，remove的时候需要delete。 虽然这两个操作都可以大致认为依然是常数的时间复杂度，但是从实际的时间消耗而言它大致是通常的基本操作的一百倍，也就是说要高出两个数量级，因此在实际中这一对操作应该尽可能少的使用。就这个意义而言，或许应该改用另外一种实现方式，比如可以只通过对M处和T处局部引用的修改，来实现同样的功能；另外一种可行的方式就是只需将M与T当前的前驱直接交换它们的数据域即可。 另一问题是有些情况下，对M的搬动操作其实是不必进行的，如果这个最大节点M恰好正是tail的直接前驱那么它自然已经就位，当然也就无需搬动了。基于这个观察，或许你会倾向于去做这样一种优化：在搬运M前加一条if语句。这样一种改进的方式本身的确是可行的，但是我们并不认为这是一个优化，其中的原因在于，在通常的随机分布下这种情况出现的概率极低（$\\ln n/n \\to 0$），以致于我们这里所做的这种所谓的优化会得不偿失。 1.4.实现：selectMax()selectMax()接口的实现很朴实，从首节点出发逐一地进行比对，并且在这样的一个过程中记录下当前最大的元素，当我们抵达终点时，这个最终的记录就是我们所要找的最大节点。这个过程可以实现为下面的代码： 12345678template &lt; typename T&gt; //从起始于位置p的n个元素中选出最大者Posi(T) List&lt;T&gt;::selectMax(Posi(T) p, int n) &#123; Posi(T) max = p; //最大者暂定为首节点p for (Posi(T) cur = p; 1 &lt; n; n--) //从首节点p出发，将后续节点逐一与max比较 if (!lt((cur = cur-&gt;succ)-&gt;data, max-&gt;data)) //若当前元素不小于max，则 max = cur; //更新最大元素位置记录 return max; //返回最大节点位置&#125; 注意到我们这里所采用的比较器是not less than ，也就是不小于（当前元素不小于max），只要当前的这个节点大于或等于目前的max，就需要做相应的更新。这是接口语义要求的，如果序列有最大的元素有多个的话，这样可以返回其中最靠后的那个（秩最大的那个），而这个元素也相应地会被优先地转移到sorted的那部分，就总体效果而言所有的这些重复元素都会依次地转移到相应的位置上去，从而保证算法的稳定性。 1.5性能选择排序的性能如何呢？这个算法仍是由n次迭代完成的，在第k次迭代中： selectMax()为$\\Theta(n-k)$，算术级数 remove()和insertBefore()均为$O(1)$ 故总体复杂度为$\\Theta(n^2)$。（无论最大元素在哪都需要$n^2$，不分最好最坏情况，因此用$\\Theta$） 选择排序居然和起泡排序的效果一样嘛？其实还是有区别的，就这里所涉及的两类操作：节点的移动以及比较大小而言，前者在实际效果也就是常系数的意义下要复杂的多得多，需要花费远远更多的时间，因此这里能够对移动次数做改进，使得每一趟扫描所需要的移动操作从原来的n降至1，实际上是一个非常了不起的改进。那么比较操作同样可以进一步地改进，等到后面的第十章优先级队列中，将会借助精巧的数据结构，使selectMax()可以$\\log n$而不是$n$的时间内完成。 2.插入排序2.1经验插入排序也是一种生活中很常见的排序算法，比如在发牌时整理手上的牌[・_・?]，我们可能会这样做：将手牌先按大小整理好，接下来拿到新牌后再按照大小插入到手牌中相应的位置，这其实就是插入排序。 在每次这样的过程中，我们所做的工作无非两步：第一步就是由这条绿色的线所标明的去做一个定位，即寻找到要插入的位置；接下来的动作实际上是两个更小的步骤的组合，首先将更小的牌向左移动以腾出一个空余位置，然后将新牌插入到那个位置。 2.2构思在插入排序算法的整个过程中我们始终将输入序列视作两个部分：有序部分+无序部分 Sorted + Unsorted L[0, r) + L[r, n) 接下来是迭代的过程，在每一次迭代中将当前秩为r的元素e插入到前面的Sorted部分的合适位置，使之保持有序，这样待解决问题的规模就减一，这也是这个算法的不变性。 【初始化】置$S_0$为空序列（长度为0） //空序列自然有序 【迭代】在有序的$S_r$ = S[0, r)中确定适当位置 //有序序列的查找 ​ 插入S[r]，得到有序的$S_{r+1}$ = S[0, r] //有序序列的插入 如此，可逐步得到：$S_0,S_1,\\dots,S_n$，最终，$S_n$ = S[0, n)即排序序列 正确性基于以下不变性：$\\forall$ 0&lt;= r &lt;= n，$S_r$ = 前r个元素组成的有序序列 2.3对比介绍到这你可能会有疑问，排序排序和选择排序看起来是一回事啊？但其实它们是截然不同的两种算法，这里先来看一下两者在整体策略上的区别。 第一个区别：insertion sort有序部分和无序部分的次序左右颠倒，而在selection sort中 有序部分是后缀，无序的部分才是前缀，当然这不是很重要φ(&gt;ω&lt;*) 。 第二个区别：在选择排序中，无序的前缀部分始终保持着一个不变性，也就是无序部分的所有元素都不会超过有序部分的最小元素（第一个）；反观插入排序，并没有这样的规定，有序部分的k个元素仅仅是序列中的前k个元素，要插入的元素可能是任意大小。 2.4.实现将插入排序的过程实现为下面一段代码： 1234567//列表的插入排序算法：对起始于位置p的n个元素排序，valid(p) &amp;&amp; rank(p) + n &lt;= sizetemplate &lt; typename T&gt; void List&lt;T&gt;::insertionSort(Posi(T) p, int n) &#123; for (int r = 0; r &lt; n; r++) &#123; //逐一为各节点 insertAfter(search(p-&gt;data, r, p), p-&gt;data); //查找适当的位置并插入 p = p-&gt;succ; remove(p-&gt;pred); //转向下一节点 &#125; //n次迭代&#125; //仅使用O(1)辅助空间，属于就地算法 p一开始指向序列的起点，经过一次迭代p后移一位，它始终是有序部分和无序部分的界点。search(p-&gt;data, r, p)返回有序部分中不大于p的最大元素（最靠后），接着将值为p-&gt;data的节点插入其后（注意这里并不是将p直接插入到那位置），然后将p转向它的后继p-&gt;succ，并删除原来的p。 整个算法过程中，除了输入的列表自己本身以外，只需要$O(1)$的额外辅助空间，这种算法被称为就地算法（in-place algorithm ）。 2.5.性能 最好情况：完全（或几乎）有序 每次迭代，只需1次比较，0次交换；累计$O(n)$时间 ！（而选择排序无论好坏都是$\\Theta(n^2)$） 最坏情况：完全（或几乎）逆序 第k次迭代，需要$O(k)$次比较，1次交换；累计$O(n^2)$时间 你可能会想，既然是在一个有序部分中查找，那为什么不用二分查找呢[・_・?]，可以把比较次数降到$O(\\log k)$。其实是不行的，因为列表不支持这种循秩访问的方式，而向量是可以的。 那为啥不用向量实现插入排序呢[・ヘ・?]。如果用向量的话，在插入操作时，我们必须将所插入元素之后的元素整体向后移一个单位，这样每次迭代仍然是需要$O(n)$时间，所以就最坏的情况而言，改用向量结构对于插入排序的改进于事无补。 2.6.平均性能上节分析插入排序在最好和最坏情况下的时间复杂度，那么在一般情况下呢？ 假定各元素的取值遵守均匀，独立分布，那么平均要做多少次比较呢？ 为此需要采用一种方法：后向分析（backward analysis），我们把时间拉回到某个元素，比如第r个元素刚刚完成插入的那个时刻，在此之前有序前缀长度为r，现在长度为r+1。那么插入这个元素需要花费多少时间呢？由于插入元素的大小是不确定的，而且它插在不同位置上对应的成本也不同。 但是由于我们采用的是均匀独立分布的假设，那么对于现在有序前缀中的r+1个元素中，每个元素都可能是刚刚插入的那个元素，其概率是相等的，为$1/(r+1)$。为了估算刚才那一步迭代的时间成本，我们将每一个元素作为刚插入元素完成插入所对应的成本累计起来，求期望： \\left[ r+(r-1)+\\dots+3+2+1+0\\right]/(r+1)+1=r/2+1于是，整个过程的总体期望是： \\left[ 0+1+\\dots+(n-1)\\right]/2+1=O(n^2)所以插入排序的平均复杂度依然是$O(n^2)$，与它在最坏情况下是同阶的，换而言之，虽然它有最好情况复杂度为$O(n)$，但这种情况发生的概率极低。 2.7.逆序对最后来看一个与插入排序非常相关的一个概念：逆序对（inversion）。实际上在一个由n个元素构成的序列中，任何两个元素都有可能构成逆序对，其逆序对的总数接近$n^2$。逆序对涉及到两个元素，我们不妨把逆序对这个标记记到后边元素上，对应一个节点p，可以用$i(p)$来表示节点p对应的逆序对数的总和，那么整个序列的逆序对总数为： I=\\sum_p i(p) 在插入排序中，当要把节点p插入到前面的有序序列S的适当位置时，p所对应的逆序对的个数就是p要经过的比较的次数，$i(p)$其实就是p所对应的查找长度。因此$I$就对应着整个插入排序算法所需要的比较次数的总和，这是算法所消耗时间的主要部分，再加上每n步插入所需要的时间一共是$n$，那么插入排序的复杂度就是$O(I+n)$。 如果把整个序列的逆序对总数$I$作为序列无序程度的度量尺度，那么插入排序insertion sort就可以理解为是通过一次一次的努力去修复这种无序性。因此它的算法复杂度其实不光是取决于问题的规模，而更多的是取决于输入序列本身所具有的特性即它的无序程度，所以这样一种算法也称作输入敏感的（input-sensitive）。在排序算法家族中并非每一种算法都具有这样的一个特性，而插入排序也正因为它具有这样一个特性，而显得非常的独特。在之后要介绍的希尔排序（Shell sort）中，我们将会看到这种输入敏感的特性对于希尔排序整体的性能，乃至这个算法的有效性都是至关重要的。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"数据结构与算法（5）列表","slug":"数据结构与算法（5）列表","date":"2020-02-14T12:33:23.000Z","updated":"2020-02-15T12:33:23.000Z","comments":true,"path":"2020/02/14/数据结构与算法（5）列表/","link":"","permalink":"http://nekomoon404.github.io/2020/02/14/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%885%EF%BC%89%E5%88%97%E8%A1%A8/","excerpt":"","text":"本章的主题是列表，与向量一样列表也是典型的最基本的一类线性结构，但是列表结构与向量结构在几乎所有的方面都是对称的、互补的，因此它的特点也十分的鲜明。本文主要介绍列表的接口与实现，无序列表和有序列表。 1.接口与实现1.1.从静态到动态 根据是否修改数据结构，所有操作大致分为两类方式： 静态：仅读取，数据的内容及组成一般不变：get、search 动态：需写入，数据结构的局部或整体将改变：insert、remove 与操作方式相对应地，数据元素的存储于组织方式也分为两种 静态：数据空间整体创建或销毁 ​ 数据元素的物理存储次序与其逻辑次序严格一致 ​ 可支持高效的静态操作 比如向量，元素的物理地址与其逻辑次序线性对应 动态：为各数据元素动态地分配和回收的物理空间 ​ 逻辑上相邻的元素记录彼此的物理地址 ​ 形成一个整体可支持高效的动态操作 这里的代表就是我们这一章的主题：列表 1.2.从向量到列表 列表（list）是采用动态存储策略的典型结构 其中的元素称作节点（node） 各节点通过指针或引用彼此联接，构成一个逻辑上的线性序列：$L=\\{a_0,a_1,\\dots,a_{n-1}\\}$ 相邻节点彼此互称前驱（predecessor）或后继（successor） 前驱或后继若存在，则必然唯一 一个序列中的第一个元素称为首节点（没有前驱），最后一个元素称为末节点（没有后继） 以下图为例，对于任何一个列表而言，首先都有一个入口的位置，所有的元素确实可以从入口开始沿着它们之间的引用，依次地从相对的前驱转向后继以及后继的后继，直到最终的末节点。虽然在逻辑上它们是这样的一个排列的次序，但是在物理上却远远不是。但是这样不妨碍它们定义并且实现这样的一个次序，比如说从某一个位置出发，我们可以找到它的物理位置并且访问它，接下来可以顺着它的后继的引用找到它的后继，以及再顺着后继的引用找到后继的后继，诸如此类直到最终抵达末节点，从而退出这个列表。 1.3.从秩到位置 向量支持循秩访问（call-by-rank）的方式，根据数据元素的秩，可在$O(1)$时间内直接确定其物理地址， V[i] 的物理地址 = V + i × s，s为单个单元占用的空间量 既然同属线性序列，列表固然也可通过秩找到对应的元素 为找到秩为i的元素，须从头（尾）端出发，沿引用前进（后退）i步 然而因为成本过高，此时的循秩访问已不合时宜 以平均分布为例，单次访问的期望复杂度为$(n+1)/2=O(n)$ 因此，应改用循位置访问（call-by-position）的方式访问列表元素，也就是说，应转而利用结点之间的相互引用，找到特定的节点 1.4.实现1.4.1列表节点：ADT接口 作为列表的基本元素，列表节点首先需要独立地“封装”实现。为此，可设置并约定若干基本的操作接口 1.4.2.列表节点：ListNode模板类 123456789101112131415#define Posi(T) ListNode&lt;T&gt;* //列表节点位置（ISO C++.0x，template alias）template &lt;typename T&gt; //简洁起见，完全开放而不再过度封装struct ListNode &#123; //列表节点模板类（以双向链表形式实现） T data; //数值 Posi(T) pred; //前驱 Posi(T) succ; //后继 // 构造函数 ListNode() &#123;&#125; //针对header和trailer的构造 ListNode(T e, Posi(T) p = NULL, Posi(T) s = NULL) : data(e), pred(p), succ(s) &#123;&#125; //默认构造器 // 操作接口 Posi(T) insertAsPred(T const&amp; e); //紧靠当前节点之前插入新节点 Posi(T) insertAsSucc(T const&amp; e); //紧随当前节点之后插入新节点&#125;; 1.4.3列表：ADT接口在给出列表结构的具体实现之前，首先定义一组它所应该提供的操作接口，仔细看会发现它的接口的形式以及对应的功能与第二章中所学过的向量Vector结构颇为类似，这里逐一再展开了，在后边相应的各节将对它们的功能和实现再做详细的介绍。 1.4.4.列表：List模板类接下来介绍列表也就是List这种模板类的具体定义，首先要引入刚才所实现的列表节点类，可以看到List这种模板类也是分为三个层次，其中private 私有的层次与向量类似，记录的都是那些对外不可见的部分，具体包括规模、引入两个哨兵节点。另外也包括一些内部的功能函数，以及刚才我们所定义的那些对外开放的标准ADT接口。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include \"listNode.h\" //引入列表节点类template &lt;typename T&gt; class List &#123; //列表模板类private: int _size; Posi(T) header; Posi(T) trailer; //规模、头哨兵、尾哨兵 protected: void init(); //列表创建时的初始化 int clear(); //清除所有节点 void copyNodes(Posi(T), int); //复制列表中自位置p起的n项 void merge(Posi(T)&amp;, int, List&lt;T&gt;&amp;, Posi(T), int); //归并 void mergeSort(Posi(T)&amp;, int); //对从p开始连续的n个节点归并排序 void selectionSort(Posi(T), int); //对从p开始连续的n个节点选择排序 void insertionSort(Posi(T), int); //对从p开始连续的n个节点插入排序public:// 构造函数 List() &#123; init(); &#125; //默认 List(List&lt;T&gt; const&amp; L); //整体复制列表L List(List&lt;T&gt; const&amp; L, Rank r, int n); //复制列表L中自第r项起的n项 List(Posi(T) p, int n); //复制列表中自位置p起的n项// 析构函数 ~List(); //释放（包含头、尾哨兵在内的）所有节点// 只读访问接口 Rank size() const &#123; return _size; &#125; //规模 bool empty() const &#123; return _size &lt;= 0; &#125; //判空 T&amp; operator[] (Rank r) const; //重载，支持循秩访问（效率低） Posi(T) first() const &#123; return header-&gt;succ; &#125; //首节点位置 Posi(T) last() const &#123; return trailer-&gt;pred; &#125; //末节点位置 bool valid(Posi(T) p) //判断位置p是否对外合法 &#123; return p &amp;&amp; (trailer != p) &amp;&amp; (header != p); &#125; //将头、尾节点等同于NULL Posi(T) find(T const&amp; e) const //无序列表查找 &#123; return find(e, _size, trailer); &#125; Posi(T) find(T const&amp; e, int n, Posi(T) p) const; //无序区间查找 Posi(T) search(T const&amp; e) const //有序列表查找 &#123; return search(e, _size, trailer); &#125; Posi(T) search(T const&amp; e, int n, Posi(T) p) const; //有序区间查找 Posi(T) selectMax(Posi(T) p, int n); //在p及其n-1个后继中选出最大者 Posi(T) selectMax() &#123; return selectMax(header-&gt;succ, _size); &#125; //整体最大者// 可写访问接口 Posi(T) insertAsFirst(T const&amp; e); //将e当作首节点插入 Posi(T) insertAsLast(T const&amp; e); //将e当作末节点插入 Posi(T) insertA(Posi(T) p, T const&amp; e); //将e当作p的后继插入（After） Posi(T) insertB(Posi(T) p, T const&amp; e); //将e当作p的前驱插入（Before） T remove(Posi(T) p); //删除合法位置p处的节点,返回被删除节点 void merge(List&lt;T&gt;&amp; L) &#123;merge(first(), size, L, L.first(), L._size);&#125;//全列表归并 void sort(Posi(T) p, int n); //列表区间排序 void sort() &#123; sort(first(), _size); &#125; //列表整体排序 int deduplicate(); //无序去重 int uniquify(); //有序去重 void reverse(); //前后倒置（习题）// 遍历 void traverse(void(*) (T&amp;)); //遍历，依次实施visit操作（函数指针，只读或局部性修改） template &lt; typename VST&gt; //操作器 void traverse(VST&amp;); //遍历，依次实施visit操作（函数对象，可全局性修改）&#125;; //List 这样的一个宏观结构可以用下面的图来表示，任何一个List结构都会拥有一个叫作header，以及另一个叫作trailer的哨兵节点，header和trailer对外是不可见的，当然我们后面会看到它们的作用非常巨大。而对外可见的部分主要是介乎header和trailer之间的这样的一系列的元素，其中如果存在的话，第一个元素也就是firstNode，我们称作首元素，而最后一个last我们称作末元素。那么相应的也把名字规范一下，称header叫作头元素，称trailer是尾元素。 等效地，头、首、末和尾这四个节点的“ 秩 ”可以分别理解为是等于-1、0、n-1以及n。 那么它们之间的联系是：头节点和尾节点是与生俱来的，而且二者并不相同，first和last并不见得不相同，甚至不能保证它们存在，但是对外而言first、last是可见的，而trailer和header这两个哨兵都是invisible不可见的。当然从秩的角度来看一个长度为n的列表中，头、首、末和尾这四个节点的秩可以分别理解为是等于-1、0、n-1以及n。 1.4.5.构造如此定义的一个列表结构可以按照这样的过程来创建：首先要为header和trailer分别地分配一个节点使它们真实的存在，接下来要将它们的后继以及前驱引用分别指向对方，从而实现这样一个互联的效果。当然逻辑上看这个时候对外可见的那个列表实际上是没有任何元素的，对应的就是一个空列表。而在接下来的几节里会介绍如何实现在其中插入一些元素，以及再插入一些元素，也可能时不常地从中删除或者是修改某一个元素。总而言之这个列表将有可能会包含一些实在的、对外可见的节点，我们在后面几节再来看这些操作是如何具体实现的。 1234567template &lt;typename T&gt; void List&lt;T&gt;::init() &#123; //列表初始化，在创建列表对象时统一调用 header = new ListNode&lt;T&gt;; //创建头哨兵节点 trailer = new ListNode&lt;T&gt;; //创建尾哨兵节点 header-&gt;succ = trailer; header-&gt;pred = NULL; //互联 trailer-&gt;pred = header; trailer-&gt;succ = NULL; //互联 _size = 0; //记录规模&#125; 2.无序列表2.1.秩到位置这一节讨论无序列表的相关算法，首先关心的一个问题是既然列表和向量同属于线性的序列结构那么是否可以继续沿用向量那种，十分便捷也是我们十分习惯的循序秩访问的方式呢？具体说来，对于任何一个名字叫L的列表，每当我们指定其中一个合法的秩r，都可以以这样的一个形式来直接引用并且访问到对应的这个节点。 答案是可以的，因为我们可以仿照向量的做法对下标操作符进行适当的重载，具体的方法如下： 123456template &lt; typename T&gt; //重载下标操作符，以通过秩直接访问列表节点（虽方便，效率低，需慎用）T List&lt;T&gt;::operator[] (Rank r) const &#123; //assert: 0 &lt;= r &lt; size Posi(T) p = first(); //从首节点出发 while (0 &lt; r--) p = p-&gt;succ; //顺数第r个节点即是 return p-&gt;data; //目标节点，返回其中所存元素&#125; 由此也可以看出这个算法的复杂度是取决于所指定的那个秩r的，即$O(r)$，这个是十分低下的。实际上这种用法虽然很方便，但是我们只能偶尔为之而不能常用。估算它的平均性能为$O(n)$，需要线性的时间，这样一个性能，无论如何我们都是无法接受的。 2.2.查找接下来考虑无序列表的查找算法，这里将这个算法的接口语义定义为在当前的列表L中以位置为p的某一个特定节点为基准，在它的n个真前驱中（不包括它自己在内的n个前驱中）找到某个可能存在的数值为特定值e的节点。 仿照向量的查找算法我们从p这个位置出发，从后向前将每个节点逐一取出并与目标元素进行比对，一旦发现相等也就是命中，即可停止。 这样一个过程可以准确地描述为下面的代码： 123456template &lt; typename T&gt; //在无序列表内节点p（可能是trailer）的n个真前驱中，找到等于e的最后者 Posi(T) List&lt;T&gt;::find(T const&amp; e, int n, Posi(T) p) const &#123; while (0 &lt; n--) //（0 &lt;= n &lt;= rank(p) &lt; _size）对于p的最近的n个前驱，从右向左 if (e == (p = p-&gt;pred)-&gt;data) return p; //逐个比对，直至命中或范围越界 return NULL; //p越出左边界意味着区间内不含e，查找失败&#125; //失败时，返回NULL 注意无论是成功的情况所返回的p，还是失败时返回的NULL，都是我们此前所定义的一个节点位置Position。还要注意一种特殊的情况，即目标节点e不仅存在而且可能有多个，那么在这时根据这个算法，它会首先停止于相对而言最靠后的那个节点，因为这正是我们的语义所要求的一个细节。 那么在最坏的情况下，当然这个算法必须一直迭代到最末尾这个位置，累计的宽度是n，所以相应的复杂度也就是最坏情况下$O(n)$。 还需要留意的是，我们这里的三个参数的次序find(T const&amp; e, int n, Posi(T) p)，为什么这里将n放在p的前端呢？实际上这是为了让我们更方便地了解这个算法的功能语义，当使用find(e, n ,p)这样一个方式来调用这个接口的时候，你就很容易理解它是在p的n个前驱中去进行查找。换而言之我们完全可以重载另一个接口find(e, p, n)，它的不同之处就在于p和n的位置恰好交换，这就意味着是在p的n个后继中去查找特定的某一个元素。 2.3.插入本节以insertBefore（前插入）为例介绍列表插入功能的实现，所谓的insert Before就是在当前的列表中以某个位置p为基准，在它的前方也就是作为它的前驱插入一个新的节点，而且这个节点的数值应该是我们指定的e。可以看到实际上它是通过转而由p作为一个节点的对象所支持的一个叫作insert as predecessor这样一个接口来实现的。 123456789101112template &lt; typename T&gt; Posi(T) List&lt;T&gt;::insertBefore(Posi(t) p, T const&amp; e)&#123; _size++; return p-&gt;insertAsPred(e); //e当作p的前驱插入&#125;//前插入算法（后插入算法完全对称）template &lt; typename T&gt; Posi(T) ListNode&lt;T&gt;::insertAsPred(T const&amp; e) &#123; Posi(T) x = new ListNode(e, pred, this); //创建（耗时100倍） pred-&gt;succ = x; pred = x; return x; //建立链接，返回新节点的位置&#125; insertAsPred首先生成一个节点，它的数值为e，这个节点的前驱就是节点this的前驱，而这个新生成出来的节点的后继恰好就是this。所以Posi(T) x = new ListNode(e, pred, this);这样一句不仅创建了一个数值为e的节点，而且完成了这个节点到当前这个列表的正确的连接。 接下来做进一步的调整，也就是既然新的这个节点已经以节点this作为后继，那么当前节点this也就应该以新的这个节点作为前驱。反之新的这个节点既然以原来的那个前驱为前驱，那么原来的这个前驱也就应该以新的这个节点为后继，至此完成了新节点的插入操作。（类似于Cpp基础6中的链表的创建过程） 需要注意的是这样一种操作只是在局部进行，只牵涉到局部的三个节点，与其它的节点没有关系，它的复杂度是常数的，而这一点与向量是迥然不同的（向量插入元素后，原位置其后的元素要整体后移）。 考虑一些特殊的情况，比如说如果当前这个节点this是首节点，以至于它的前驱根本就不存在，那么这个时候这种操作难道不会出问题吗？其实是不会出问题的，因为我们在此前设立过哨兵，即便当前这个节点是首节点，它的前驱在内部依然是存在的，也就是header，那么这个时候的这种操作依然是安全的。 2.4.基于复制的构造有的时候我们也需要以某个已有的列表为蓝本，通过复制来创建一个新的列表，这样的过程可以由下面的代码来实现。 12345678910111213template &lt; typename T&gt; //列表内部方法：复制列表中自位置p起的n项 void List&lt;T&gt;::copyNodes(ListNodePosi(T) p, int n) &#123; //p合法，且至少有n-1个真后继节点 init(); //创建头、尾哨兵节点并做初始化 while (n--) &#123; insertAsLast(p-&gt;data); p = p-&gt;succ; &#125; //将起自p的n项依次作为末节点插入 &#125;template &lt;typename T&gt; ListNodePosi(T) List&lt;T&gt;::insertAsLast ( T const&amp; e )&#123; _size++; return trailer-&gt;insertAsPred ( e ); &#125; //e当作末节点插入 解读下这个算法：可见这里被复制的节点范围是从某一个列表的位置p开始随后地连续n个节点，因此首先我们创建一个空的列表，它只包含内部的头尾哨兵节点，不包含任何实质的节点。接下来我们不断地将当前这个p处所指的那个节点的元素取出来，把它作为当前列表的末节点插入其中，每做完这样一个新节点的引入，我们都会将注意力转向当前这个节点的后继，如此往复直到这个区间中的所有n个节点，都被复制过来。 那么其中的insertAsLast怎么来实现呢？回顾我们所有对外可见的最后那个节点叫作last 即末节点，但是我们还同样配备了一个哨兵节点叫作trailer。所以所谓的insertAsLast其实就是insertBefore trailer。 2.5.删除接下来介绍如何从列表中删除指定的节点。可以通过下面的代码来实现。 12345678template &lt;typename T&gt; T List&lt;T&gt;::remove(ListNodePosi(T) p) &#123; //删除合法节点p，返回其数值 O(1) T e = p-&gt;data; //备份待删除节点的数值（假定T类型可直接赋值） p-&gt;pred-&gt;succ = p-&gt;succ; p-&gt;succ-&gt;pred = p-&gt;pred; //后继、前驱 delete p; _size--; //释放节点，更新规模 return e; //返回备份的数值 &#125; 首先我们需要将这个节点的数据域取出作一备份，以便在最终能够顺利地将它返回，而删除节点之后整个列表在这个局部的拓扑连接关系的调整则是由第4、5行来完成的，通过下面的图便于理解一下这个过程。 p-&gt;pred-&gt;succ = p-&gt;succ的效果是将p现在的后继当作p现在的前驱的后继，这样等同于将后继的这个引用直接跳过了p。而p-&gt;succ-&gt;pred = p-&gt;pred的过程以及效果与前一句完全对称，具体来说就是将p现在的前驱作为p的现在后继的前驱。这样两句的实质的作用可以理解为是将这个局部的后继引用，以及前驱的引用都跳过节点p，也就是说 p可以认为与原来的列表已经在拓扑上隔离开了。所以当我们将p直接地通过delete操作删除并归还系统之后就实现了指定节点p的删除。 整个这个过程只牵涉到局部的三个节点其余的节点没有任何的影响，由此也可以确认删除算法的复杂度与插入算法一样，都是常数的。 2.6.析构如何销毁一个已有的列表呢？整个过程分为两步，首先要将对外可见的所有节点逐一删除，当所有对外可见的节点都被删除之后，再将内部的两个哨兵也就是header以及trailer给释放了。 所以整个这个算法分为两级：析构方法首先调用一个名为clear的函数，将所有的可见节点删除，接下来再将header和trailer删除掉，那么clear的实质的工作也就是反复的删除header的后继。 1234567891011template &lt;typename T&gt; List&lt;T&gt;::~List() &#123; //列表析构 clear(); //清空列表 delete header; delete trailer; //释放头、尾哨兵节点&#125;template &lt;typename T&gt; List&lt;T&gt;::clean() &#123; //清空列表 int oldSize = _size; while (0 &lt; _size) //反复删除首节点，直至列表变空 remove(header-&gt;succ); return oldSize;&#125; //O(n)，线性正比于列表规模 2.7.唯一化本节介绍列表的唯一化问题，即如何将列表中可能存在的重复元素剔除掉。在接下来要介绍的这个算法中，我们始终将整个列表视作由三部分组成，这个前缀部分作为这个算法的一条不变性，已经能够保证不包含任何重复的节点，而我们当前所要考察的就是接下来的这个节点p，当然此时可能还存在一个非空后缀。 这个算法的关键部分是如果当前那个节点的数值为e，就以e为目标在前缀中对它进行查找，无论查找成功与否我们都可以将问题的规模，有效地降低至少一个单位，具体的算法可以实现为这样一段代码： 12345678910template &lt;typename T&gt; int List&lt;T&gt;::deduplicate() &#123; //剔除无序列表中的重复节点 if (_size &lt; 2) return 0; //平凡列表自然无重复 int oldSize = _size; //记录原规模 LPosi(T) p = first(); Rank r = 1; //p从首节点起 while (trailer != (p = p-&gt;succ)) &#123; //依次直到末节点 Posi(T) q = find(p-&gt;data, r, p); //在p的r个（真）前驱中，查找与之相同者 q ? remove(q) : r++; //若的确存在，则删除之；否则秩递增 &#125; //assert: 循环过程中的任意时刻，p的所有前驱互不相同 return oldSize - _size; //返回列表规模的变化量，即被删除元素总数 &#125; //正确性及效率分析的方法与结论，与Vector::deduplicate()相同 解读一下这段代码：首先是处理平凡情况，确保这个列表中至少包含两个节点，接下来这一步可以认为是初始化，可以看到p最初的时候是指向首节点，即这时的前缀实际上可以认为是空的，所以不变性自然也就满足。接下来算法的主体部分是这个循环，可以看到每一次我们都将p中所存的那个元素在以p为基准的r个真前驱中查找。 在前缀中进行查找无非两种情况：一种就是find操作返回了一个非空的元素，它的数值当然就等于e，这是就把q删除掉了；否则如果q是一个null，那就意味着查找失败，即在这样的一个前缀中根本就不存在语义相同的元素，所以在这之后p这个节点可以归入到前缀中去，使得这个前缀拓展一个单位，在这种情况下就可以将r加1，即前缀的长度增加一个单位，同时p转入它的后继元素继续迭代，直到抵达哨兵trailer也就意味着整个列表全部扫描并且处理完毕。 3.有序列表本节介绍有序列表，我们假设其中的元素不仅可以比较大小而且已经按照这种大小关系有序地排列。在这样的一个条件下很多问题都存在更加高效的解法。 3.1.唯一化：原理以唯一化问题为例，回顾有序向量的唯一化可以比无序向量的唯一化更快地完成，那么有序列表的唯一化是否也能够比无序列表的唯一化完成地更快呢？在介绍出具体的算法之前，我们先来分析一下算法的思路。 回顾在有序向量的去重算法中，曾经介绍过这样一个事实：在任何一个有序的序列中，如果存在重复元素，那么每一组重复元素必然会彼此紧邻地排列成一个又一个的分组，每一个分组都由彼此相同的一组元素构成。而唯一化的任务可以等效地理解成是从每一组中挑选出一个代表，而将其余的元素都剔除掉。于是仿照有序向量的唯一化算法也可以得到一个迭代式的算法。 具体来讲每一次迭代我们都将注意力关注于当前以p指示的那个节点，同时还要考虑p的直接后继q，在经过一次比对之后如果发现p和q相等，就可以将后者剔除掉，这个可以通过列表所提供的remove标准接口来实现。此后q将指向新的后继节点。同样地在接下来的一步迭代中，依然会发现q与p相等，所以可以继续将它删除。 在某一步接下来的迭代中情况可能发生变化，即首次发现一个与p不同的节点，这时就预示着一个新的区段出现了，作为这个新的区段的首节点，我们将保留这个节点。而为了这个算法能够继续计算下去，不妨将p由原来的位置转向这个新发现的不同的节点（图中p由红色转向绿色）。可以看到经过这样一轮迭代之后，刚才相同的一组元素中，除了第一个其余的后继都被删除掉了。 3.2.唯一化：实现上小节的算法思路可以实现为下面一段具体的代码： 12345678910template &lt;typename T&gt; int List&lt;T&gt;::uniquify() &#123; //成批剔除重复元素，效率更高 if (_size &lt; 2) return 0; //平凡列表自然无重复 int oldSize = _size; //记录原规模 Posi(T) p = first(); Posi(T) q; //p为各区段起点，q为其后继 while (trailer != (q = p-&gt;succ)) //反复考查紧邻的节点对(p, q) if (p-&gt;data != q-&gt;data) p = q; //若互异，则转向下一区段 else remove(q); //否则（雷同），删除后者 return oldSize - _size; //列表规模变化量，即被删除元素总数 &#125; 解读一下这段代码：首先是处理平凡的情况，即要确保这个列表至少包含两个元素。我们所关注的总是当前节点p，这个节点从首节点开始，同时还有另一个辅助的引用q指向p的后继。以下是一个循环，每一次我们都令q指向p的直接后继，随后将二者作一比对，如果相同就调用remove接口将q删除掉；反过来 一旦遇到一个与p不同的后继节点，那么就意味着抵达了下一个区段的首节点，这个时候我们就可以直接将注意力转向这个下一个区段的首节点也就是q。此后发生的情况将与刚才那样一轮迭代完全相同，迭代持续进行直到最终q试图越过边界，这时整个算法也就宣告结束。 整个算法过程主体的复杂度来源是while，可以看到每经过一步迭代p都会转入一个新的节点，所以整个迭代至多经过线性步就会停止，所有这个算法的时间复杂度为$O(n)$，这相对于无序列表也是一个很大的改进。 3.3.查找有序列表的去重操作相对于无序列表而言可以更快地进行，那么其它的操作呢？比如我们最最关心的查找呢？ 有序的列表的查找操作可以由下面一段代码实现： 12345678template &lt; typename T&gt; //在有序列表内节点p的n个（真）前驱中，找到不大于e的最后者Posi(T) List&lt;T&gt;::search(T const&amp; e, int n, Posi(T) p) const &#123;// assert: 0 &lt;= n &lt;= rank(p) &lt; _size while (0 &lt;= n--) //对于p的最近的n个前驱，从右向左 if (((p = p-&gt;pred)-&gt;data) &lt;= e) //逐个命中 break; return p; //直至命中，数值越界或范围越界后，返回查找终止的位置&#125; //最好O(1)，最坏O(n)；等概率时平均O(n)，正比于区间宽度 我们发现这个算法与无序列表居然没有太多的差异，同样它在最好情况下也是常数，比如在开始的位置就发现命中目标；反过来最坏也可能多达$O(n)$，即一直查找到最后才得出是否命中的结论。在整个查找范围也就是p之前的n个前驱中，每一个元素对应的查找成本将呈算数级数趋势变化，总体而言是线性的。 这样的结论多少会令人失望，因为尽管我们这里已经将列表中的元素按顺序进行了重新组织，但是查找的效率居然没有实质的改进。这并不是我们实现不当，而根本的原因在于列表的循位置访问这种方式。这种访问方式与向量的循秩访问有着本质的差异。 Vector这种结构在访问数据的时候，所依据的实际上是秩rank，而列表结构所依据的是位置position。在此前所介绍过的计算模型，实际上对于RAM模型来说，它所对应的其实就是循秩访问的方式。对于RAM模型来说每给出一个编号i，都可以在$O(1)$的时间内找到对应的寄存器，读取其中的数值或者对其进行修改。 再来看图灵机模型，虽然“纸带”长度无限，但在任何时候我们所能够操纵的只是其中某一个特定的单元，而且更重要的是在接下来只可能向左或者向右移动一格。如果的确需要访问一个相距很远的单元，我们将不得不亦步亦趋地逐步地通过多步的迭代才能够抵达目标，这正是我call-by-position，而不是RAM那种call-by-rank。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"Cpp基础（8）数据的共享与保护","slug":"Cpp基础（8）数据的共享与保护","date":"2020-02-13T12:58:24.000Z","updated":"2020-02-15T12:58:24.000Z","comments":true,"path":"2020/02/13/Cpp基础（8）数据的共享与保护/","link":"","permalink":"http://nekomoon404.github.io/2020/02/13/Cpp%E5%9F%BA%E7%A1%80%EF%BC%888%EF%BC%89%E6%95%B0%E6%8D%AE%E7%9A%84%E5%85%B1%E4%BA%AB%E4%B8%8E%E4%BF%9D%E6%8A%A4/","excerpt":"","text":"标识符的作用域与可见性作用域是一个标识符在程序正文中的有效区域。作用域可以分为以下几类： 函数原型作用域：函数原型中的参数，其作用域始于”(“，结束语”)”，如： 1double area(double radius) 局部作用域： 函数的形参、在块中声明的标识符 其作用域自声明处起，限于块中 类作用域： 类的成员具有类作用域，其范围包括类体和非内联成员函数的函数体。 如果在类作用域以外访问类的成员，要通过类名（访问静态成员），或者该类的对象名、对象引用、对象指针（访问非静态成员） 文件作用域： 不在前述各个作用域中出现的声明，就具有文件作用域，这样声明的标识符其作用域开始于声明点，结束语文件尾。 可见性： 可见性是从对标识符的引用的角度来谈的概念 可见性表示从内层作用域向外层作用域“看”时能看见什么 如果标识在某处可见，就可以在该处引用此标识符 如果某个标识符在外层中声明，且在内层中没有同一标识符的声明，则该标识符在内层可见 对于两个嵌套的作用域，如果在内层作用域内声明了与外层作用域中同名的标识符，则外层作用域的标识符在内层不可见。 12345678910111213#include&lt;iostream&gt;using namespace std;int i; //全局变量，文件作用域int main()&#123; i = 5; &#123; int i; //局部变量，局部作用域 i = 7; cout &lt;&lt; \"i = \" &lt;&lt; i &lt;&lt; endl; //输出7 &#125; cout &lt;&lt; \"i = \" &lt;&lt; i &lt;&lt; endl; //输出5 return 0;&#125; 对象的生存期静态生存期： 静态生存期与程序的运行期间相同; 在文件作用于中声明的对象具有这种生存期 在函数内部声明静态生存期对象，要冠以关键字static 动态生存期： 开始于程序执行到声明点时，结束语命名该标识符的作用域结束处 块作用域中声明的，没有用static修饰的对象时动态生存期的对象（习惯称局部生存期对象） 12345678910111213141516171819202122232425262728#include&lt;iostream&gt;using namespace std;int i = 1; //i为全局变量，具有静态生存期void other() &#123; static int a = 2; static int b; //a, b为静态局部变量，具有全局寿命，局部可见 //只第一次进入函数时被初始化 int c = 10; //C为局部变量，具有动态生存期， //每次进入函数时都初始化 a += 2; i += 32; c += 5; cout &lt;&lt; \"---OTHER---\\n\"; cout &lt;&lt; \"i: \" &lt;&lt; i &lt;&lt; \" a; \" &lt;&lt; a &lt;&lt; \" b: \" &lt;&lt; b &lt;&lt; \" c: \" &lt;&lt; c &lt;&lt; endl; b = a;&#125;int main() &#123; static int a; //静态局部变量，有全局寿命，局部可见（实际中尽量不使用重名的变量） int b = -10; //b,c为局部变量，具有动态生存期 int c = 0; cout &lt;&lt; \"---MAIN---\\n\"; cout &lt;&lt; \"i: \" &lt;&lt; i &lt;&lt; \" a; \" &lt;&lt; a &lt;&lt; \" b: \" &lt;&lt; b &lt;&lt; \" c: \" &lt;&lt; c &lt;&lt; endl; c += 8; other(); cout &lt;&lt; \"---MAIN---\\n\"; cout &lt;&lt; \"i: \" &lt;&lt; i &lt;&lt; \" a; \" &lt;&lt; a &lt;&lt; \" b: \" &lt;&lt; b &lt;&lt; \" c: \" &lt;&lt; c &lt;&lt; endl; i += 10; other(); return 0;&#125; 类的静态成员静态成员：在定义前面加了static关键字的成员 1234567891011class CRectangle&#123; private: int w,h; static int nTotalArea; static int nTotalNUmber; //静态成员变量 public: CRectangle(int _w,int _h); ~CRectangle(); static void PrintTotal(); //静态成员函数&#125;； 基本概念 普通成员变量每个对象有各自的一份，而静态成员变量一共就一份，为所有对象共享 ​ 注意：sizeof运算符不会计算静态成员变量，如下例中sizeof(Myclass)等于4 1234class Myclass&#123; int n; static int s&#125; 普通成员函数必须具体作用于某个对象，而静态成员函数并不具体作用于某个对象。 因此静态成员不需要通过对象就能访问 如何访问静态成员： 类名::成员名 1CRectangle::PrintTotal(); 对象名.成员名（并不意味着静态成员或静态成员函数只作用于该对象,，它们是被所有的CRentangle对象所共享的） 1CRectangle r; r.PrintTotal(); 指针-&gt;成员名 1CRectangle *p = &amp;r; p-&gt;PrintTotal(); 引用.成员名 1CRectangle &amp;ref = r; int n = ref.nTotalNumber; 静态成员变量本质上是全局变量，哪怕一个对象都不存在，类的静态成员变量也存在。 静态成员函数本质上是全局函数 设置静态成员这种机制的目的是将和某些类紧密相关的全局变量和函数写到类里面，看上去像一个整体，易于维护和理解。 示例：考虑一个需要随时知道矩阵总数和总面积的图像处理程序，可以用全局变量来记录总数和总面积，同静态成员将这两个变量封装进类中，就更容易理解和维护。 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include&lt;iostream&gt;using namespace std;class CRectangle&#123;private: int w, h; static int nTotalArea; static int nTotalNUmber; //静态成员变量public: CRectangle(int _w, int _h); ~CRectangle(); static void PrintTotal(); //静态成员函数&#125;;CRectangle::CRectangle(int _w, int _h)&#123; w = _w; h = _h; nTotalNUmber++; nTotalArea += w * h;&#125;CRectangle::~CRectangle()&#123; nTotalNUmber--; nTotalArea -= w * h;&#125;void CRectangle::PrintTotal()&#123; cout &lt;&lt; nTotalNUmber &lt;&lt; \", \"&lt;&lt;nTotalArea &lt;&lt; endl;&#125;int CRectangle::nTotalNUmber = 0;int CRectangle::nTotalArea = 0;//必须在定义类的文件中对静态成员变量进行一次说明或初始化。//否则编译能通过，链接不能通过int main()&#123; CRectangle r1(3, 3), r2(2, 2); //cout&lt;&lt;CRectangle::nTotalNUmber; //Wrong,私有成员不能在类外访问 CRectangle::PrintTotal(); r1.PrintTotal(); //与上一句等价 return 0;&#125; 注意事项： 在静态成员函数中，不能访问非静态成员变量，也不能调用非静态成员函数（因为其可能访问非静态成员变量），例如下面的定义就是错误的。 1234void CRectangle::PrintTotal()&#123; cout &lt;&lt; w &lt;&lt;\", \"&lt;&lt; nTotalNUmber &lt;&lt; \", \"&lt;&lt;nTotalArea &lt;&lt; endl; &#125; //Wrong，因为PrintTotal是静态成员函数，而w属于非静态成员变量 我们回过头来再看之前的CRectangle类的写法，其实它是有严重缺陷的，那么这个缺陷是如何产生的呢？ 问题就出在我们忽略了复制构造函数，在程序需要它时，会调用自动生成的复制构造函数，自然就不会对nTotalNUmber和nTotalArea作相应的增加。 在使用CRectangle类时，有时会调用复制构造函数生成临时的隐藏的CRectangle对象 调用一个以CRectangle类对象作为参数的函数时 调用一个以CRectangle类对象作为返回值的函数时 临时对象在消亡时会调用析构函数，减少nTotalNUmber和nTotalArea的值，可是这些临时对象在生成时却没有增加nTotalNUmber和nTotalArea的值。 解决办法：为CRectangle类写一个复制构造函数 123456CRectangle::CRectangle(CRectangle &amp;r)&#123; w=r.w; h=r.h; nTotalNUmber++; nTotalArea += w * h;&#125; 类的友元 友元是C++提供的一种破坏数据封装和数据隐藏的机制 通过将一个模块声明为另一个模块的友元，一个模块能引用到另一个模块中很是被隐藏的信息 可以使用友元函数和友元类 为了确保数据的完整性，及数据封装与隐藏的原则，建议尽量不使用或少使用友元 友元函数 友元函数是在类声明中由关键字friend修饰说明的非成员函数，在它的函数体重能够通过对象名访问private和protected成员。 作用：增加灵活性，时程序员可以在封装和快速性方面做合理的选择。 访问对象中的成员必须通过对象名 123456789101112131415161718192021222324252627#include&lt;iostram&gt;using namespace std;class CCar; //提前声明CCar类，因为后面CDriver类要前向引用class CDriver &#123; public: void ModifyCar(CCar *pCar); //改装汽车&#125;;class CCar&#123; private: int price; friend int MostExpensiveCar(CCar cars[], int total); //类外函数声明为友元函数 friend void CDriver::ModifyCar(CCar *pCar); //其他类的成员函数声明为友元函数&#125;;void CDriver::ModifyCar(CCar *pCar) //求最贵汽车的价格&#123; pCar-&gt;price += 1000; //汽车改装后价格增加&#125;int MostExpensiveCar(CCar cars[], int total)&#123; int tmpMax = -1; for (int i = 0; i &lt; total; ++i) if (cars[i].price &gt; tmpMax) tmpMax = cars[i].price; return tmpMax;&#125; 友元类 若一个类为另一个类的友元，则此类的所有成员都能访问对方类的私有成员 声明语法：将友元类名在另一个类中使用friend修饰说明 1234567891011121314class CCar&#123; private: int price; friend class CDriver; //声明CDriver为CCar的友元类&#125;;class CDriver &#123; public: CCar myCar; void ModifyCar()&#123; //改装汽车 myCar.price += 1000; //CDriver是CCar的友元类-&gt;可以访问其私有成price &#125; &#125;; 主要注意的是：类的友元关系是单向的： 如果声明B类是A类的友元，B类的成员函数就可以访问A类的私有和保护数据，但A类的成员函数不能访问B类的私有、保护数据，即友元类的关系不能传递，不能继承。 this指针在C++刚推出的时候，编译器在编译C++程序课程时先把一段C++程序翻译成C程序，然后再用C的编译去编译。比如说我们把下面的CCar的类的C++程序翻译成C程序，class对应C中的struct结构体，而C中没有成员函数，因此需要借助this指针来实现SetPrice的功能。 C++代码： 12345678910111213class CCar &#123;public: int price; void SetPrice(int p);&#125;;void CCar::SetPrice(int p) &#123; price = p;&#125;int main() &#123; CCar car; car.SetPrice(20000); return 0;&#125; C代码： 1234567891011struct CCar &#123; int price;&#125;;void SetPrice(struct CCar *this, int p) &#123; this-&gt;price = p;&#125;int main() &#123; struct CCar car; SetPrice( &amp;car, 20000 ); return 0;&#125; this指针作用：非静态成员函数中可以直接使用this来代表指向该函数作用的对象的指针。 12345678910111213141516class Complex &#123;public: double real, imag; void Print() &#123; cout &lt;&lt; real &lt;&lt; \"+\" &lt;&lt; imag &lt;&lt; \"i\"; &#125; Complex(double r,double i):real(r),imag(i) &#123;&#125; Complex AddOne() &#123; this-&gt;real++; this-&gt;Print(); return *this &#125;&#125;;int main() &#123; Complex c1(1, 1), c2(0, 0); c2 = c1.AddOne(); //输出 2+1i return 0;&#125; 注意静态成员函数不能使用this指针，因为静态成员函数并不具体作用于某个对象。 常量const 常量对象：如果不希望某个对象的植被改变，则定义该对象时在其前面加const关键字。 常量成员函数：在类的成员函数说明后面加const关键字 常量成员函数执行期间不应该修改其作用的对象。因此，在常量成员函数中不能修改成员变量的值（静态成员变量除外），也不能调用同类的非常量成员函数（静态成员函数除外）。 若有两个成员函数，名字和参数表都一样，但是一个是const，一个不是，算重载，而不是冲突定义。 常引用：引用前加const，不能通过常引用，修改其引用的变量；常引用经常作为函数的参数 当传递一个对象的效率较低（因为需要调用复制构造函数生成一个新的对象），又要确保实际参数的值不能在函数内部被修改时，可以将参数的类型声明为常引用 12345678class Rectangle&#123;public: int w, h;&#125;;int getArea(const Rectangle &amp;rect)&#123; //rect.w = rect.h + 2; 非法 return rect.w * rect.h;&#125;","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"ML:Week2","slug":"ML-Week2","date":"2020-02-13T09:43:58.000Z","updated":"2020-02-15T09:43:58.000Z","comments":true,"path":"2020/02/13/ML-Week2/","link":"","permalink":"http://nekomoon404.github.io/2020/02/13/ML-Week2/","excerpt":"","text":"Multivariate Linear RegressionMultiple FeaturesLinear regression with multiple variables is also known as “multivariate linear regression”. We now introduce notation for equations where we can have any number of input variables. \\begin{align*}x_j^{(i)} &= \\text{value of feature } j \\text{ in the }i^{th}\\text{ training example} \\newline x^{(i)}& = \\text{the input (features) of the }i^{th}\\text{ training example} \\newline m &= \\text{the number of training examples} \\newline n &= \\text{the number of features} \\end{align*}The multivariable form of the hypothesis function accommodating these multiple features is as follows: h_θ(x)=θ_0+θ_1x_1+θ_2x_2+θ_3x_3+⋯+θ_nx_nIn order to develop intuition about this function, we can think about $\\theta_0$ as the basic price of a house, $\\theta_1$ as the price per square meter, $\\theta_2$ as the price per floor, etc. $x_1$ will be the number of square meters in the house, $x_2$ the number of floors, etc. Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as: \\begin{align*}h_\\theta(x) =\\begin{bmatrix}\\theta_0 \\hspace{2em} \\theta_1 \\hspace{2em} ... \\hspace{2em} \\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\newline x_1 \\newline \\vdots \\newline x_n\\end{bmatrix}= \\theta^T x\\end{align*}This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more. Remark: Note that for convenience reasons in this course we assume $x^{(i)}_0=1 $ for $(i\\in 1,\\dots,m)$. This allows us to do matrix operations with theta and x. Hence making the two vectors $’\\thetaθ’$ and $x^{(i)}$match each other element-wise (that is, have the same number of elements: n+1).] Gradient Descent for Multiple VariablesThe gradient descent equation itself is generally the same form; we just have to repeat it for our ‘n’ features: \\begin{align} & \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)}\\newline \\; & \\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_1^{(i)} \\newline \\; & \\theta_2 := \\theta_2 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_2^{(i)} \\newline & \\cdots \\newline \\rbrace \\end{align}In other words: \\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \\; & \\text{for j := 0...n}\\newline \\rbrace\\end{align*}The following image compares gradient descent with one variable to gradient descent with multiple variables: Python代码示例： 计算代价函数：$J\\left( \\theta \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{\\left( h_{\\theta}\\left( x^{(i)} \\right)-y^{(i)} \\right)}^2$其中：$h_{\\theta}\\left( x \\right)=\\theta^{T}X=\\theta_{0}x_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+…+\\theta_{n}x_{n}$ 123def computeCost(X, y, theta): inner = np.power(((X * theta.T) - y), 2) return np.sum(inner) / (2 * len(X)) Gradient Descent in Practice I -Feature ScalingWe can speed up gradient descent by having each of our input values in roughly the same range. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven. The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally: −1 ≤ $x_{(i)}$≤ 1 or −0.5 ≤ $x_{(i)}$ ≤ 0.5 Two techniques to help with this are feature scaling(特征缩放) and mean normalization(均值归一化). Feature scaling involves dividing the input values by the range (maximum value - minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula: x_i:=\\frac{x_i-\\mu_i}{s_i}Where $\\mu_i$ is the average of all the values for feature (i) and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation(标准差). （量化后的特征将分布在[-1, 1]，服从标准正态分布） Gradient Descent in Practice II - Learning RateDebugging gradient descent(调试梯度下降): Make a plot with number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α. Automatic convergence test(自动收敛测试): Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as 10−3. However in practice it’s difficult to choose this threshold value It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration. To summarize: If $\\alpha$ is too small: slow convergence. If $\\alpha$ is too large: ￼may not decrease on every iteration and thus may not converge. Features and Polynomial RegressionWe can improve our features and the form of our hypothesis function in a couple different ways. We can combine multiple features into one. For example, we can combine $x_1$ and $x_2$ into a new feature $x_3$ by taking $x_1⋅x_2$. Our hypothesis function need not be linear (a straight line) if that does not fit the data well. We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form). For example, if our hypothesis function is $h_\\theta (x) = \\theta_0 + \\theta_1 x_1$ then we can create additional features based on $x_1$, to get the quadratic function $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2$ or the cubic function $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1^3$ In the cubic version, we have created new features $x_2$ and $x_3$ where $x_2 = x_1^2$ and $x_3 = x_1^3$. To make it a square root function, we could do: $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 \\sqrt{x_1}$. One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important. eg. if $x_1$ has range 1 - 1000 then range of $x_1^2$ becomes 1 - 1000000 and that of $x_1^3$ becomes 1 - 1000000000 Normal EquationGradient descent gives one way of minimizing J. Let’s discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In the “Normal Equation“ (正规方程) method, we will minimize J by explicitly taking its derivatives with respect to the θj ’s, and setting them to zero: $\\frac{\\partial J\\left( \\theta \\right)}{\\partial \\theta }=0$. This allows us to find the optimum theta without iteration. The normal equation formula is given below: \\theta = (X^T X)^{-1}X^T y There is no need to do feature scaling with the normal equation. The following is a comparison of gradient descent and the normal equation: 总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\\theta $的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。 随着学习算法越来越复杂，例如，分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及特征变量的数量，这两种算法都是值得学习的。 正规方程的python实现： 1234567import numpy as np def normalEqn(X, y): theta = np.linalg.inv(X.T@X)@X.T@y #X.T@X等价于X.T.dot(X) return theta Noninvertibility(不可逆性)When implementing the normal equation in octave we want to use the pinv function rather than inv. The ‘pinv‘ function will give you a value of $\\theta$ even if $X^TX$ is not invertible. If $X^TX$ is noninvertible, the common causes might be having : Redundant features, where two features are very closely related (i.e. they are linearly dependent) Too many features (e.g. m ≤ n). In this case, delete some features or use “regularization“（正则化） (to be explained in a later lesson). Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features. 补充：$\\theta = (X^T X)^{-1}X^T y$的 证明： 代价函数： J\\left( \\theta \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{\\left( h_{\\theta}\\left( x^{(i)} \\right)-y^{(i)} \\right)}^2其中：$h_{\\theta}\\left( x \\right)=\\theta^{T}X=\\theta_{0}x_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+…+\\theta_{n}x_{n}$ 将向量表达形式转为矩阵表达形式，则有$J(\\theta )=\\frac{1}{2}{\\left( X\\theta -y\\right)}^2$ ，其中$X$为$m$行$n$列的矩阵（$m$为样本个数，$n$为特征个数），$\\theta$为$n$行1列的矩阵，$y$为$m$行1列的矩阵，对$J(\\theta )$进行如下变换 \\begin{align}J(\\theta )&=\\frac{1}{2}{\\left( X\\theta -y\\right)}^{T}\\left( X\\theta -y \\right) \\\\ &=\\frac{1}{2}\\left( {\\theta }^{T}{X^T}-{y}^{T} \\right)\\left(X\\theta -y \\right)\\\\&=\\frac{1}{2}\\left( {\\theta }^T{X}^{T}X\\theta -{\\theta}^{T}{X}^{T}y-{y}^{T}X\\theta -{y}^{T}y \\right) \\end{align}接下来对$J(\\theta )$偏导，需要用到以下几个矩阵的求导法则 \\begin{align} \\frac{dAB}{dB}&={A^T}\\\\\\frac{dX^TAX}{dX}&=2AX \\end{align}所以有: \\begin{align} \\frac{\\partial J\\left( \\theta \\right)}{\\partial \\theta }&=\\frac{1}{2}\\left(2{X^T}X\\theta -{X^T}y -({y^T}X )^{T}-0 \\right)\\\\&=\\frac{1}{2}\\left(2{X^T}X\\theta -{X^T}y -{X^T}y -0 \\right)\\\\&={X^T}X\\theta -{X^T}y \\end{align}令：$\\frac{\\partial J\\left( \\theta \\right)}{\\partial \\theta }=0$ 则有：$\\theta =\\left( X^{T}X \\right)^{-1}{X^T}y$。 ———————————————————————————————————————————————— PS：Week2的第三部分是Octave语言教程，看了一遍视频发现Octave的语法和Matlab基本是一样的，有很方便的矩阵运算，而且Octave是完全开源的，但正版的Matlab确实很贵，这大概也是老师这门课用Octave的原因之一吧。（Andrew Ng 永远滴神！）","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://nekomoon404.github.io/categories/Machine-Learning/"}],"tags":[]},{"title":"数据结构与算法（4）气泡排序与归并排序","slug":"数据结构与算法（4）气泡排序与归并排序","date":"2020-02-12T12:45:52.000Z","updated":"2020-02-13T12:45:52.000Z","comments":true,"path":"2020/02/12/数据结构与算法（4）气泡排序与归并排序/","link":"","permalink":"http://nekomoon404.github.io/2020/02/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%884%EF%BC%89%E6%B0%94%E6%B3%A1%E6%8E%92%E5%BA%8F%E4%B8%8E%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/","excerpt":"","text":"通过之前的两篇文章我们可以知道有序向量相对于无序向量有着更多的优势，比如它的去重操作以及查找操作都可以更快速地完成，然而我们遗留下一个问题，就是如何将一个无序的向量转化为有序的向量，这就需要用到排序算法，本文针对向量介绍两种典型的排序算法，即起泡算法与归并算法。 排序器：统一接口1234567891011template &lt;typename T&gt; void Vector&lt;T&gt;::sort ( Rank lo, Rank hi ) &#123; //向量区间[lo, hi)排序 switch ( rand() % 6 ) &#123; case 1: bubbleSort ( lo, hi ); break; //起泡排序 case 2: selectionSort ( lo, hi ); break; //选择排序（习题） case 3: mergeSort ( lo, hi ); break; //归并排序 case 4: heapSort ( lo, hi ); break; //堆排序（第12章） case 5: quickSort ( lo, hi ); break; //快速排序（第14章） default: shellSort ( lo, hi ); break; //希尔排序（第14章） &#125; //随机选择算法以充分测试。实用时可视具体问题的特点灵活确定或扩充&#125; 起泡排序1234567template &lt;typename T&gt; //向量的起泡排序（基本版）void Vector&lt;T&gt;::bubbleSort( Rank lo, Rank hi ) &#123; //assert: 0 &lt;= lo &lt; hi &lt;= size while( lo &lt; --hi ) //反复起泡扫描 for( Rank i = lo; i &lt; hi; i++ ) //逐个检查相邻元素 if( _elem[i] &gt; _elem[i + 1] ) //若逆序，则 swap( _elem[i], _elem[i + 1] ); //经交换使局部有序&#125; 在第一章曾以这个算法为例介绍过如何证明算法的正确性，这里按照刚才统一定义的形式将它整理为一个名为bubbleSort的算法接口。这个算法实际上可以认为是通过调用一个名为bubble的过程迭代地来进行，在每一迭代过程中都会考察当前介于lo和hi之间的所有相邻元素，只要有一对相邻元素是逆序的，就将它们交换，所以整个这样的一个过程也称作扫描交换。 这个算法的不变法具体来说，如果最初的这个向量是一个无序向量的话，那么每经过这样一趟对bubble的调用都会有一个新的元素就位，比如对于第一次而言就是全局最大的那个元素，这里用红色来表示就位的元素，那么当然互补地其它的部分也就是接下来要考察的问题的范围，就会相应地缩小一个单元，这也是减而治之。再接下来有序的部分会继续地拓展，而无序的部分会继续地缩减，整个呈现为一个不断此消绿色的这部分，和彼涨红色的这部分这样一个过程，直到无序的部分只剩下一个元素。 不难看出每一趟对bubble的调用所需要的时间都线性正比于绿色无序部分的宽度，整体地呈现为一个算术级数的形式，所以它的总体量与它的末项成平方关系，即$O(n^2)$。然而我们并不满足于这样的结果，至少在很多情况下都是有可能改进的。 改进可以看到这里的红色部分确实必然是有序的，但是绿色的部分未必都是无序的，事实上比如这个时候有可能其中会有一部分元素，甚至所有的元素都是有序的。那么如何尽早地判定出这种情况，从而提前结束这个算法呢？这里依赖的准则与算法最初的判定准则是一样的，也就是一个向量包括一个区间如果是完全有序的，当且仅当其中任何一对相邻的元素都是彼此顺序的，而实际上在刚刚进行完的前一次迭代中我们在某种意义上已经做过这种类似的检查了。 由此可以得出一个改进的策略：在每一次扫描交换的过程中不妨记录一下是否曾经真的存在逆序元素，如果存在的话它的充要条件是在此前做过一次交换，所以我们只要来记录一下在当下这趟扫描交换过程中是否曾经做过至少一次扫描交换，如果没有做过那么后续的各趟其实都可以省略掉，从而在实际的运行时间上有可能会有所减少，甚至大大减少。这是一个很好的策略，我们不妨把这个策略整理为下面的一段代码。 123456789101112template&lt;typename T&gt; void Vector&lt;T&gt;::bubbleSort(Rank lo,Rank hi)&#123; while (!bubble(lo, hi--));&#125; //逐趟做扫描交换，直至全序template&lt;typename T&gt; void Vector&lt;T&gt;::bubble(Rank lo, Rank hi) &#123; bool sorted = ture; //整体有序标志 while(++lo &lt; hi)&#123; //自左向右，逐一检查各相邻元素 if (_elem[lo - 1] &gt; _elem[lo]) &#123; //若逆序，则 sorted = false; //意味着尚未整体有序，并需要 swap(_elem[lo - 1], _elem[lo]); //交换 &#125; return sorted; //返回有序标志&#125; 原算法整体运行时间确实可以度量为一个三角形的面积，那么对于新的改进的这个算法，它固然要做第一趟扫描交换也许还需进行若干次扫描交换，但是在某些情况下它有可能会发现不光此后的部分已经有序了，而且这个前缀也已经完全有序了，所以这时它就会及时地跳转到最后，聪明地绕过这些完全可以绕过的计算量。因此与刚才那样对比新的这个算法所执行的计算量可以度量为这样一个梯形，而不是原来的三角形，也就是说很多情况下都可以节省一定的甚至是相当多的时间。不过我们对这个算法的改进并不满足于此因为我们发现在一些其它或者说在更多的情况下，这个算法依然存在继续改进的空间。 再改进考察这样一个向量，假设它可以分为长度相差悬殊的一个前缀以及后缀，而且后缀中的元素都已按顺序排列并严格地就位，当然相应地所有的乱序元素都集中分布于这样一个相对更短的前缀中。对于这样的一个实例，上节中已经做过优化的起泡排序算法会如何表现呢？ 首先它需要做第一趟完整地扫描交换，并且确认在最后这个位置有一个元素就位，虽然它原本就是就位的。请注意虽然这个时候在这个后缀中，存在着大量的就位元素，但因为在前缀中刚才存在交换，bubble算法会返回false，那么算法接下来还会继续下去。尽管能够判定的就位元素数目会继续增加，但是与刚才同理，我们依然不能确认可以提前退出，接下来还需要进行若干次的扫描交换。那么对于这样的一个例子，总体而言需要的扫描交换的趟数不会超过这个前缀的长度r。 因为此前所做的各趟扫描交换，与其说是在对绿色的范围做处理，不如说实际影响的是这个前缀中的倒数第一个倒数第二个 以及倒数第三个，即是在这个前缀中后面的那些元素。每一趟扫描交换所起的实质作用无非是在这样一个前缀中，令其中的一个一个的后缀元素依次就位，直到整个这个前缀中的元素完全就位。 因此这个算法总体消耗的时间应该是n乘以r，如果r取作根号n，相应地也就是n的1.5次方，即$O(n^{1.5})$。但如果能及时地检测出这样一种情况，也就是实质需要排序的元素集中在一个宽度仅为$\\sqrt{n}$的区间中，而不是整个向量。那么即使套用最原始的起泡排序算法，所需要的时间也无非是$O((\\sqrt{n})^2)=O(n)$。问题是如何才能够完成从1.5次方到一次方的优化转换呢？ 重新审视上面的例子，所多余出来的时间消耗无非是在后缀中，对这些已就位元素的反复扫描交换，不难理解这些元素都是不必扫描交换的，可惜此前的算法版本未能及时地将它们分解出来，但它们实际上是可以分解出来的。 比如说如果我们通过某一种方法记录在上一趟扫描交换过程中所进行的最后一次交换，就很容易确定在上一趟扫描的区间中有一个多长的后缀实际上没有做过任何交换，也就是说它们中的元素都是已经就位了的。如果能这样只需要将原先的右侧标志hi直接地指向这个新的位置，而不是像刚才那样亦步亦趋地、逐个地收缩。 基于以上的分析不难得到下面的新的改进的方法，从结构上看跟刚才大体类似，依然是逐个地检查所有的相邻对，如果是逆序的就做交换，不同之处在于这里我们所记录的不再只是一个逻辑性变量，而是一个名为last的整型或者说是秩，它的初值是取作lo，而每当需要交换就将这个last更新为新的位置。在整个算法的过程中lo这个变量是持续递增的，所以当它在返回的时候，last确实名副其实地记录了最右侧也就是最后一对逆序对的位置。 123456789101112template&lt;typename T&gt; void Vector&lt;T&gt;::bubbleSort(Rank lo,Rank hi)&#123; while (!bubble(lo, hi--));&#125; //逐趟做扫描交换，直至全序template&lt;typename T&gt; void Vector&lt;T&gt;::bubble(Rank lo, Rank hi) &#123; Rank last = lo; //最右侧的逆序对初始化为[lo-1, lo] while(++lo &lt; hi)&#123; //自左向右，逐一检查各相邻元素 if (_elem[lo - 1] &gt; _elem[lo]) &#123; //若逆序，则 last = lo; //更新最右侧逆序对位置记录，并 swap(_elem[lo - 1], _elem[lo]); //交换 &#125; return last; //返回有序标志&#125; //前一版本中的逻辑型标志sorted，改为秩last 这样我们就可以有效地来处理刚才那种情况，回到刚才那个实例，我们构造了一个足够短的乱序前缀再加一个非常长但是已经就绪了的后缀。新的算法首先也会做一趟扫描交换，当然为此花费的时间是$O(n)$。但是与刚才那个版本的不同，在这个时候它会检测出发生的最后一次扫描交换绝对不会超过绿色末尾的位置，将扫描交换的右侧界桩hi一次性地挪到那里，这等效于判断出了此后的这些元素包括最后那个元素都是已经就位的。 从算法的流程来说我们的下一趟扫描交换的区间，就不再是原先整个那个绿色的区间，而是相对要短很多的一个区间。接下来等效于只是对这样一段区间做扫描交换，因此需要花费的时间除了刚才的$O(n)$以外，主要是对应于这样的一个更小的三角形，如果边长是$\\sqrt{n}$，累计也不过是再加上一个$O(n)$，与刚才的$O(n)$合并，总体不过是$O(n)$，更有意思的是这种情况在整个排序过程中有可能会多次出现。 我们也可以通过图形的方式，形象地将新的这个算法版本与之前的原始版本在时间效率上做一个对比。这个三角形 代表的是原始的起泡排序算法所需要的时间。新版本的算法所需要执行的扫描交换将会呈现为连续的一段。然后再间或地跳跃到下面一段以及再间或地有可能会跳跃到下面一段（深色部分）。换而言之这个算法的时间成本将取决于这样一个一个若干个梯形的面积总和，相对于此前那个梯形来说这种梯形的划分更加的精细，所以它节省下来的时间也会在通常的情况下相对更多。 当然在最坏的情况下这个算法依然是于事无补的，起泡排序依然注定需要$O(n^2)$的时间。 综合评价 三种起泡排序在最好和最坏情况下的效率相同：最好$O(n)$，最坏$O(n^2)$ 输入含重复元素时，算法的稳定性（stability）是更为细致的要求 重复元素在输入，输出序列中的相对次序，是否保持不变？（在某些问题中很敏感） ​ 输入：$6,7_a,3,2,7_b,1,5,8,7_c,4$ ​ 输出：$1,2,3,4,5,6,7_a,7_b,7_c,8$ //stable ​ $1,2,3,4,5,6,7_a,7_c,7_b,8$ //unstable 三种起泡排序算法都是稳定的，因为在起泡排序中，元素$7_a$和$7_b$的相对位置发生变化，只有一种可能： ​ 经分别与其他元素的交换，二者相互接近直至相邻 ​ 在接下来一轮扫描交换中，二者因逆序而交换位置 而起泡排序中交换，即if的判断条件是_elem[lo - 1] &gt; _elem[lo])，严格大于，因此不会出现上面的情况 虽然起泡排序可以做大量的改进，但从最坏情况而言它依然是注定也需要$O(n^2)$的时间，所以我们非常希望能够得到一个即便在最坏情况下也能够效率更高的排序算法，这也就是下一节所要介绍的内容。 归并排序采用包括Bubble sort在内的常规的基于比较式的算法（Comparison Based Algorithm），求解排序问题都存在一个下界$nlogn$。那么在$n^2$的上界到$nlogn$的下界之间是否存在一些其它的，相对于$n^2$而言更好的算法,甚至于是否有一个算法即使在最坏的情况下也只需要$nlogn$的时间就能完成排序呢？答案就蕴含在这一节的主题里也就是归并排序（Merge Sort）。 归并排序算法是分治策略在算法设计中应用的又一个典型，这个算法最初是由冯·诺依曼编码实现的，所谓的分治策略在这里就是说将待排序的那个序列（向量或者列表）一分为二，这种分法很快捷只需要$O(1)$的时间，接下来 对于划分出的两个子序列分别去做递归地求解，也就是递归地排序。而当两个子序列已经分别有序之后，我们接下来要解决的一个问题就是将它们合并准确地讲是归并merge，从而构成一个完整的有序序列。 对于上面这样一个由8个元素组成的向量，首先是分沿左右划分为左和右两个子序列，这两个子序列递归地求解的过程中依然还是相对比较大，所以它们会继续递归地、各自地进行划分继续分为左左、左右以及右左和右右四个子序列。同样 它们还是不够平凡所以我们最后还要对这四个子序列继续地一分为二，最终八个元素各自成为一个独立的序列，这个时候从递归地角度讲就抵达了递归基，所有这些元素都已经不需要再继续划分下去了，因为它们各自有序了。 所以如果说前面半层是做无序向量的递归分解，接下来就要通过逐层的合并使之逐渐地变成一个大一点的，更大一点的，直到最后那个有序的序列。我们可以看到每一次都是将两个已经是有序的子序列合并为一个有序的子序列，然后再继续相邻的子序列逐对地合并构成再更大的序列，最后左右这两个各自有序的子序列再逐对地合并最终得到整体的序列。 那么如果果真能像这里所说的那样，我们就应该能够得到一个总体是$nlogn$的算法，可由下面的递推式证明，其中$O(n)$是分与并累计的时间。 T(n)=2\\cdot T(n/2)+O(n)可以得到：$T(n)=O(nlogn)$。 接下来的技术细节就是如何来兑现这一点呢？可以看到从这里的划分的过程是非常简单，递归也可以交给递归的机制去做，所以这里核心的任务是在怎么进行合并，或者准确地讲是怎么将两个已经有序的序列归并成一个更大的序列，这也是这个算法最关键的细节和技巧。 主算法把刚才的思路实现为这样一段具体的代码，和所有的递归程序一样首先要处理递归基，接下来开始实质的分也就是除二取到中点，这样的话我们可以将整体的一个序列分成左和右两部分，分别由lo和mi，以及mi和hi来界定。对于这两个序列，分别是递归调用自己，mergeSort前一个序列，mergeSort后一个序列。接下来最重要的实质的工作是在merge，下面不妨来通过一个实例来理解merge算法的原理 12345678template &lt;typename T&gt; //向量归并排序void Vector&lt;T&gt;::mergeSort ( Rank lo, Rank hi ) &#123; //0 &lt;= lo &lt; hi &lt;= size if ( hi - lo &lt; 2 ) return; //单元素区间自然有序，否则... int mi = ( lo + hi ) / 2; //以中点为界 mergeSort ( lo, mi ); mergeSort ( mi, hi ); //分别排序 merge ( lo, mi, hi ); //归并&#125; 原理 2-way merge：将两个有序序列合并为一个有序序列 S[lo, hi) = S[lo, mi) + S[mi, hi) 首先（a）图给出了两个各自有序的子序列，二路归并算法的要诀就是我们只需要把注意力关注在这两个序列的首元素上，这样一个虚线的方框是我们的关注焦点，其余的元素可以暂时不用顾及。那么我们取出这两个序列各自的首元素的时候，都要从中挑选出更小的那个元素，如果是两者相等的话，可以任意取一个。比如 就这个例子而言 就这个例子而言首先取出的是这个2，我们将它择出来，相应地在摘除了首元素以后，后续的元素将逐次递补，也就是关注到新顶替上来的这个首元素上。同样在接下来的一轮比对中，我们考察这两个首元素的大小，并且同样地取出其中的更小的那个，4依然比5小所以4被取出，同样它的后继们会顶替上来对这个例子而言就是10。就这样逐步进行到图（h），直到最终一旦有一个向量已经变成空的，那么另一个向量所剩余的元素无论多少都直接串接在后边（因为剩余那部分必然是有序的）。 按照这样的原理，我们确实可以得到一个更大的单调序列，这种二路归并的算法实际上是非常通用的一个版本，但在这里针对于归并排序而言的，我们实际上用到的是其中的一种特例，在这个时候参与归并的两个序列实际上是来自于同一个更大的向量，只不过是由其中的三个界桩也就是lo、mi和hi来联合定义的。如果左侧的这个向量称作B，右侧的称作C的话，那么合并起来的整体的这个向量就是A。那下一小节介绍针对这样一种特殊情况，二路归并算法应该如何实现。 实现1234567891011121314template &lt;typename T&gt; //有序向量（区间）的归并void Vector&lt;T&gt;::merge ( Rank lo, Rank mi, Rank hi )&#123;//各自有序的子向量[lo, mi)和[mi, hi) T* A = _elem + lo; //合并后的向量A[0, hi - lo) = _elem[lo, hi) int lb = mi - lo; T* B = new T[lb]; //前子向量B[0, lb) = _elem[lo, mi) for ( Rank i = 0; i &lt; lb; i++ ) B[i] = A[i]; //复制前子向量 int lc = hi - mi; T* C = _elem + mi; //后子向量C[0, lc) = _elem[mi, hi) for (Rank i = 0, j = 0, k = 0; (j &lt; lb) || (k &lt; lc); ) &#123; if ((j &lt; lb) &amp;&amp; (lc &lt;= k || (B[j] &lt;= C[k]))) A[i++] = B[j++]; //B更小，C[k]已无或不小 if ((k &lt; lc) &amp;&amp; (lb &lt;= j || (C[k] &lt; B[j]))) A[i++] = C[k++]; //C更小，或B[j]已无或更大 &#125; //该循环实现紧凑；但就效率而言，不如拆分处理 delete [] B; //释放临时空间B&#125; 解读一下上面的代码：首先需要将定义两个向量的三个界桩也就是lo、mi和hi作为参数传入，接下来要定义清楚ABC三个向量：首先A向量在这里将继续地保存在它输入的位置，准确地讲就是在_elem整个数据区中起自于最左侧的界桩lo的一段区间，可以直接令A指向这个区间的起点。 1T* A = _elem + lo; //合并后的向量A[0, hi - lo) = _elem[lo, hi) 接下来是左侧的子向量B，我们需要为这个子向量申请一段空间，它的宽度应该是mi到lo之间的距离，当然还需要将A中对应的那些元素，也就是左半部分的那些元素，逐一地取出来并且复制到新开辟的这段空间中去，从而完成整体的这个子向量B的一个缓冲。 12int lb = mi - lo; T* B = new T[lb]; //前子向量B[0, lb) = _elem[lo, mi)for ( Rank i = 0; i &lt; lb; i++ ) B[i] = A[i]; //复制前子向量 最后是C，C非常的简单，实际上定义的就是在_elem数据区中，起始于mi的这段数据，那么不同的在于右侧的子向量C并不需要另辟空间进行缓存，尽管在这里为了说明的方便，还是将它画在了上边作为一个单独的子向量。 1int lc = hi - mi; T* C = _elem + mi; //后子向量C[0, lc) = _elem[mi, hi) 接下来就是最主要的这个循环，这也就是上节实例子所给的过程，具体来讲就是每一次我们都比较两个子向量当前的首元素取出其中更小的那个，比如说在for循环体中上面一句的情况下B更小，而在下面一句的情况下C更小，无论谁更小都把它转入到A中去。B和C首元素是由j和k这两个秩来标定的，在最初始的情况下它们都是0，分别指向B和C的第一个元素，在随后 每当有一个元素转移到A中，它们各自都会自加，从而指向下一个替补的新的首元素。而A每次纳入新元素由i指示，其初值也是0。 123456for (Rank i = 0, j = 0, k = 0; (j &lt; lb) || (k &lt; lc); ) &#123; if ((j &lt; lb) &amp;&amp; (lc &lt;= k || (B[j] &lt;= C[k]))) A[i++] = B[j++]; //B更小，C[k]已无或不小 if ((k &lt; lc) &amp;&amp; (lb &lt;= j || (C[k] &lt; B[j]))) A[i++] = C[k++]; //C更小，或B[j]已无或更大 &#125; 当B更小的情况：严格来讲是由一系列的逻辑判断构成的，首先是一个and，我们要确定j &lt; lb，即B中的首元素的秩应该至少没有越过它的右侧的边界，它还是合法的，也就是B[j]指向的还是一个实在的而不是虚拟的元素，接下来地有两种情况，要么C中的k已经越界，要么就是k没有越界，但是B[j]更小B[j] &lt;= C[k]，这里我们运用了C++语言里头的“短路求值”的语法特性，否则在不满足的情况下还去进行比较求值，实际上这个k因为已经越界就会造成程序运行过程中的错误。当C更小的情况也是同理。 当然整个这个循环的退出条件也值得揣摩的，这里的条件(j &lt; lb) || (k &lt; lc)可以理解为是这两个位置j和k同时越界之后算法才会退出，而在这个时候无论是B还是C中的元素都已经完整地归入到了A中，成为了一个整体的序列。 正确性为了更好地理解算法的过程，我们不妨分几种情况来给出具体的图示作进一步解释。 首先来考虑第一种情况（a)，i还是介于lo和mi之间没有越过mi这个界线，还没有进入到C这个子向量的范围，这种情况显然i不可能居于j的左侧，顶多是平齐，所以每次迭代中如果需要发生数据转移的话，无论是B[j]转移到A[i]，还是C[k]转移到A[i]，整个数据从内容来讲都不会发生覆盖，是安全的，功能上讲也是正确的。 再来看相对复杂一点的情况(b)，也就是当i在持续增加之后，终会越过mi，进入C的区域。表面看这样会侵犯到C的区域，但实际上不要紧，因为在这个时候k绝对不会位于i的左侧，所以介于mi和i之间的这些元素，其实作为C中原来的元素必然已经归入到A中，当然是它的左侧在i之前的这部分中的某一个适当的位置。所以这种情况依然是安全的，无论是C[k]、还是B[j]转移到A[i]中去，都不会导致C中已有的元素被无意中覆盖掉，从而导致错误。 再来看最后两种更为复杂的情况，如图（c），B这个子向量已经提前耗尽，它其中的元素已经完全地归入到A中当然也是就位了，而在C中还残存有部分的元素没有转移和就位。这种情况下我们的逻辑其实相当于等效地是在B的最右侧，就是lb这个位置上增加了一个哨兵节点，而且它的数值就是正无穷。因此即便C的右侧还残存有若干个元素它们也会在接下来的各次迭代中，因为是与这样一个正无穷相比而被认为是更小，从而顺利地转移到A中适当的位置，直到两个子向量都同时耗尽。 反过来另一种对称的情况（d）就是C也可能会提前耗尽，也相当于等效地 在C的最右侧增加了一个数值为正无穷的哨兵，它的秩是lc，所以即便在B的尾部 还残存有部分的元素也不要紧，它们也等效于和这样一个数值为正无穷的哨兵相比，总是会被认为是更小，所以按照算法的逻辑会等效地转移到A中剩余的对应区域中去，整个这个过程也是会顺利地进行，不会出现我们所说的数据遗漏或者数据被无意中覆盖。 需要注意的是（c）和（d）这两种情况其实并不对等，因为按照这里的设计，其实向量C和B地位本来就是不等的。B是完全复制出来的一个缓冲部分，而C虽然是独立的绘制出来但实际上它就在A中，占据右端，换而言之如果是C提前耗尽，我们确实需要把B尾部的这些元素悉数转移到A的尾部，但如果是B提前耗尽那么对C尾部这些元素的转移其实都是多余的，因为它们原来就在那，完全没有必要。注意到这样一个现象的话，我们就不难对刚才表面上很规范的逻辑进一步的精简： 123456for (Rank i = 0, j = 0, k = 0; j &lt; lb; ) &#123; if ( lc &lt;= k || (B[j] &lt;= C[k]) ) A[i++] = B[j++]; //B更小，C[k]已无或不小 if ((k &lt; lc) &amp;&amp; (C[k] &lt; B[j]) ) A[i++] = C[k++]; //C更小，或B[j]已无或更大 &#125; 这里最重要的改进就是并不需要考虑C提前耗尽的那种情况，我们只需要考虑B提前耗尽的情况，一旦B提前耗尽我们就可以直接终止这个循环包括这个算法，这样可以使这个算法效率进一步的提高，尽管不是从渐进角度而言的一种实质的提高。 那么这个算法在原来以及包括这样精简之后，从渐进意义上讲 复杂度是多少呢？是否能像我们最初所预期的那样能够有大幅度的提高呢？ 复杂度 算法的运行时间主要消耗于for循环，共有两个控制变量 ​ 初始：j = 0, k = 0 ​ 最终：j = lb, k = lc ​ 亦即：j + k = lb + lc =hi - lo = n 观察：每经过一次迭代，j和k中至少有一个会加一（j + k 也至少加一） 故知：merge()总体迭代不过$O(n)$次，累计只需线性时间 这一结论与排序算法的$\\Omega(nlogn)$下界并不矛盾——毕竟这里的B和C均已各自有序 归并算法在最坏情况下的复杂度：$T(n)=2\\cdot T(n/2)+O(n)$ ——&gt;$T(n)=O(nlogn)$ 注意：待归并子序列不必等长 亦即：允许lb $\\ne$ lc，mi $\\ne$ (lo + hi) / 2 实际上，这一算法及结论也适用于另一类序列——列表","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"ML:Week1","slug":"ML-Week1","date":"2020-02-11T02:15:57.000Z","updated":"2020-02-13T02:15:57.000Z","comments":true,"path":"2020/02/11/ML-Week1/","link":"","permalink":"http://nekomoon404.github.io/2020/02/11/ML-Week1/","excerpt":"","text":"IntroductionWhat is Machine LearningTwo definitions of Machine Learning are offered. Arthur Samuel described it as: “the field of study that gives computers the ability to learn without being explicitly programmed.” This is an older, informal definition. Tom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” Example: playing checkers. E = the experience of playing many games of checkers T = the task of playing checkers. P = the probability that the program will win the next game. In general, any machine learning problem can be assigned to one of two broad classifications: Supervised learning and Unsupervised learning. Supervised LearningIn supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised learning problems are categorized into “regression“ and “classification“ problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. Example 1: Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. We could turn this example into a classification problem by instead making our output about whether the house “sells for more or less than the asking price.” Here we are classifying the houses based on price into two discrete categories. Example 2: (a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture (b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign. Unsupervised LearningUnsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data. With unsupervised learning there is no feedback based on the prediction results. Example: Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering: The “Cocktail Party Algorithm”, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party). Model and Cost FunctionModel RepresentationTo establish notation for future use, we’ll use $x^{(i)}$to denote the “input” variables (living area in this example), also called input features, and $y^{(i)}$ to denote the “output” or target variable that we are trying to predict (price). A pair $(x^{(i)} , y^{(i)} )$ is called a training example, and the dataset that we’ll be using to learn—a list of m training examples $(x^{(i)},y^{(i)});i=1,…, m$—is called a training set. Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation. We will also use X to denote the space of input values, and Y to denote the space of output values. In this example, X = Y = ℝ. To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis. Seen pictorially, the process is therefore like this: When the target variable that we’re trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem. When y can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem. Cost FunctionWe can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x’s and the actual output y’s. J(\\theta_0, \\theta_1) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left ( \\hat{y}_{i}- y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)^2To break it apart, it is $\\frac{1}{2} \\bar{x}$， where\\bar{x}$$ is the mean of the squares of $h_\\theta (x_{i}) - y_{i}$, or the difference between the predicted value and the actual value. This function is otherwise called the “Squared error function“, or “Mean squared error“. The mean is halved $\\left(\\frac{1}{2}\\right) $as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the$ \\frac{1}{2}$ term. The following image summarizes what the cost function does: Intuition 1If we try to think of it in visual terms, our training data set is scattered on the x-y plane. We are trying to make a straight line (defined by $h_\\theta(x)$) which passes through these scattered data points. Our objective is to get the best possible line. The best possible line will be such so that the average squared vertical distances of the scattered points from the line will be the least. Ideally, the line should pass through all the points of our training data set. In such a case, the value of $J(\\theta_0, \\theta_1)$will be 0. The following example shows the ideal situation where we have a cost function of 0. When $\\theta_1 = 1$, we get a slope of 1 which goes through every single data point in our model. Conversely, when $\\theta_1 = 0.5$, we see the vertical distance from our fit to the data points increase. This increases our cost function to 0.58. Plotting several other points yields to the following graph: Thus as a goal, we should try to minimize the cost function. In this case, $\\theta_1 = 1$=1 is our global minimum. Intuition 2A contour plot（等高线，轮廓线） is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line. An example of such a graph is the one to the right below. Taking any color and going along the ‘circle’, one would expect to get the same value of the cost function. For example, the three green points found on the green line above have the same value for $J(\\theta_0,\\theta_1)$ and as a result, they are found along the same line. The circled x displays the value of the cost function for the graph on the left when $\\theta_0$ = 800 and $\\theta_1$= -0.15. Taking another h(x) and plotting its contour plot, one gets the following graphs: When $\\theta_0$ = 360 and $\\theta_1$ = 0, the value of $J(\\theta_0,\\theta_1)$ in the contour plot gets closer to the center thus reducing the cost function error. Now giving our hypothesis function a slightly positive slope results in a better fit of the data. The graph above minimizes the cost function as much as possible and consequently, the result of $\\theta_1$ and $\\theta_0$ tend to be around 0.12 and 250 respectively. Plotting those values on our graph to the right seems to put our point in the center of the inner most ‘circle’. Parameter LearningGradient DescentSo we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That’s where gradient descent comes in. Imagine that we graph our hypothesis function based on its fields $\\theta_0$ and $\\theta_1$ (actually we are graphing the cost function as a function of the parameter estimates). We are not graphing x and y itself, but the parameter range of our hypothesis function and the cost resulting from selecting a particular set of parameters. We put $\\theta_0$ on the x axis and $\\theta_1$ on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup. We will know that we have succeeded when our cost function is at the very bottom of the pits in our graph, i.e. when its value is the minimum. The red arrows show the minimum points in the graph. The way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter α, which is called the learning rate. For example, the distance between each ‘star’ in the graph above represents a step determined by our parameter α. A smaller α would result in a smaller step and a larger α results in a larger step. The direction in which the step is taken is determined by the partial derivative of J(\\theta_0,\\theta_1)J(θ0,θ1). Depending on where one starts on the graph, one could end up at different points. The image above shows us two different starting points that end up in two different places. The gradient descent algorithm is: repeat until convergence: \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)where $j=0,1$ represents the feature index number. At each iteration j, one should simultaneously update the parameters $\\theta_1, \\theta_2,…,\\theta_n$. Updating a specific parameter prior to calculating another one on the $j^{(th)}$ iteration would yield to a wrong implementation. IntuitionIn this part we explored the scenario where we used one parameter $\\theta_1$ and plotted its cost function to implement a gradient descent. Our formula for a single parameter was : Repeat until convergence: \\theta_1 := \\theta_1 - \\alpha \\frac{\\partial}{\\partial \\theta_1} J(\\theta_1)Regardless of the slope’s sign for $\\frac{d}{d\\theta_1} J(\\theta_1)$, $\\theta_1$ eventually converges to its minimum value. The following graph shows that when the slope is negative, the value of $\\theta_1$ increases and when it is positive, the value of $\\theta_1$ decreases. On a side note, we should adjust our parameter $\\alpha$ to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong. How does gradient descent converge with a fixed step size $\\alpha$? The intuition behind the convergence is that $\\frac{d}{d\\theta_1} J(\\theta_1)$ approaches 0 as we approach the bottom of our convex function. At the minimum, the derivative will always be 0 and thus we get: \\theta_1 := \\theta_1 - \\alpha*0 Gradient Descent For Linear RegressionWhen specifically applied to the case of linear regression, a new form of the gradient descent equation can be derived. We can substitute our actual cost function and our actual hypothesis function and modify the equation to : Repeat until convergence: \\begin{align*} & \\newline \\theta_0 := & \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}(h_\\theta(x_{i}) - y_{i}) \\newline \\theta_1 := & \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m}\\left((h_\\theta(x_{i}) - y_{i}) x_{i}\\right) \\newline & \\end{align*}where m is the size of the training set, $\\theta_0$ a constant that will be changing simultaneously with $\\theta_1$ and $x_i$, $y_i$ are values of the given training set (data). The point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate. So, this is simply gradient descent on the original cost function J. This method looks at every example in the entire training set on every step, and is called batch gradient descent. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. Indeed, J is a convex quadratic function. Here is an example of gradient descent as it is run to minimize a quadratic function. The ellipses shown above are the contours of a quadratic function. Also shown is the trajectory taken by gradient descent, which was initialized at (48,30). The x’s in the figure (joined by straight lines) mark the successive values of θ that gradient descent went through as it converged to its minimum.","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://nekomoon404.github.io/categories/Machine-Learning/"}],"tags":[]},{"title":"数据结构与算法（3）有序向量","slug":"数据结构与算法（3）有序向量","date":"2020-02-10T02:58:38.000Z","updated":"2020-02-12T02:58:38.000Z","comments":true,"path":"2020/02/10/数据结构与算法（3）有序向量/","link":"","permalink":"http://nekomoon404.github.io/2020/02/10/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%883%EF%BC%89%E6%9C%89%E5%BA%8F%E5%90%91%E9%87%8F/","excerpt":"","text":"唯一化有序向量是相对于无序向量而言，无序向量要求元素之间至少应该能比较是否相等，我们称作比对操作；而有序向量更为复杂，它需要能够判定任何一对元素孰大孰小，这叫作比较操作。元素之间可以相互比较只是有序向量的一个必要条件，如果要成为一个真正的有序向量，还必须要求其中的元素确实是按照顺序排列的，因此就存在一个如何甄别一个向量是否有序的问题。 有序性及其甄别 与起泡排序算法的理解相同： ​ 有序序列中，任意一对相邻元素顺序；无序序列中，总有一对相邻元素逆序。 因此，逆序相邻元素的数目，可用以度量向量的逆序程度。 无序向量经预处理转换为有序向量之后，相关算法多可优化。 1234567template &lt;typename T&gt; //返回逆序相邻元素对的总数int Vector&lt;T&gt;::disordered() const &#123; int n = 0; //计数器 for (int i = 1; i &lt; _sizei++) //逐一检查各对相邻元素 n += (_elem[i - 1] &gt; _elem[i]); //逆序则计数 return n; //向量有序当且仅当 n = 0&#125; //若只需判断是否有序，则首次遇到逆序对之后，即可立即终止 根据上面的分析可以知道，一个向量是有序的，当且仅当经过disordered()判断以后返回的值是零。实际上只要向量中的元素本身是支持大小比较的，就有一定的办法将它转化为有序向量。其中的原因在于经过这样的一个转换以后虽然我们花费了一定的成本，但此后涉及到的很多操作也就是相关算法，大多都可以优化，相应地所得要远远比转换时所花费的成本大的多。 低效算法上一篇文章介绍了无序向量的去重操作，现在我们希望把这种去重操作推广到有序向量，即将一个有序向量中的重复元素（如果存在）全部剔除掉，同样地每一组重复元素只保留一个副本。有序向量其实相对于无序向量而言，具有更好的规范性。这种规范性是指在有序向量中，彼此重复的元素必然会依次相互紧邻地构成一个一个的区间，比如就下图中的例子而言，这些元素相互重复，它们彼此紧邻，会紧密地排列成一个区间，其它元素也有这种规律。所以既然我们需要从每一组元素中保留一个副本，等价于从其中找出一个代表并且保留下来。 具体到一个算法，可以大致用一个线性扫描过程来描述：每次都观察并比对一对相邻的元素，如果二者相等就将后者删除掉，并且继续比较，如果后者还相等就把它继续删除掉，直到遇到一个不相重复的元素，这个时候我们才把注意力后移，再去考虑下一对紧邻的元素，如果依然出现这种情况再删除，直到又转到下一对。这样确实可以顺利地把所有重复的元素都剔除掉，但是不倾向与使用，因为其效率低。 12345678template &lt;typename T&gt;int Vector&lt;T&gt;::uniquify() &#123; int oldSize = _size; int i = 0; //从首元素开始 while (i &lt; _size - 1) //从前向后，逐一比对各相邻元素 //若雷同，则删除后者；否则，转至后一个元素 (_elem[i] == _elem[i + 1]) ? remove(i + 1) : i++; return oldSize - _size; //返回向量规模变化量，即删除元素总数&#125; //注意：其中_size的减小，由remove()内隐式地完成 低效算法的复杂度 算法的运行时间主要取决去while循环，次数共计: _size - 1 = n -1 最坏情况下：每次都需调用remove()，耗时$O(n-1)\\sim O(1)$，累计$O(n^2)$ ​ 尽管省去fine()，总体竟与无序向量的deduplicate()相同。 高效算法需要首先对原有的算法进行反思，我们发现造成低效率的根源在于：其中的同一个元素有可能会作为被删除元素的后继，而多次地参与前移操作，对于这样的一个元素来说虽然它每次都是向前移动，但是很可惜它的每一次移动只会移动一个单元，而不是一次性地一步到达它最终的位置。 反过来这就启示我们，如果能够将每一个重复的区间作为一个整体来考虑，成批地删除雷同的元素而不是像刚才那样逐个地去删除，并且逐个地移动，就有可能实现这种一步到位式的移动，从而使得整体的性能大大地改进。 这个新算法的思路可以由上面的图来表示，在任何时刻我们关注的都是i和j两个元素，而且这里有一个不变性，也就是在i之后 j之前的所有这些元素都与i重复，这个算法一直扫描直到发现第一个与i不同的元素。如果它确实是不同的话我们就只需将j向前移到与i紧邻于右侧的这个位置，这是一个很高明的删除算法，因为在这样的一个过程中虽然没有显式地去做这些重复元素的删除，但是实际上已经无形中将它们忽略掉了，等效于做删除。 1234567891011template &lt;typename T&gt;int Vector&lt;T&gt;::uniquify() &#123; Rank i = 0, j = 0; //各对互异“相邻”元素的秩 while (++j &lt; _size) //逐一扫描，直至末元素 //跳过雷同者；发现不同元素时，向前移至紧邻于前者右侧 if (_elem[i] != _elem[j]) _elem[++i] = _elem[j]; _size = ++i; shrink(); //直接截除尾部多余元素 return j - i; //向量规模变化量，即被删除元素总数&#125; //注意：通过remove(lo,hi)批量删除，依然不能达到高效率 高效算法的复杂度下面通过一个例子来分析新算法的复杂度： 共计n-1次迭代，每次常数时间，累计$O(n)$时间。 算法首先考虑的i和j元素，其实就是0和1号元素，对这个例子而言它们是彼此重复的元素，所以在那个循环中将会通过那个隐藏着看不见的else直接将它忽略掉，并且使得j进而转向下一个单元，以及在接下来的一个循环中再下一个单元，以及再下一个单元。执行到3和5出现了第一次的不同，按照刚才算法的逻辑会把i++到1号位置，然后把第j号元素取出来复制到对应的1号位置上，这就是为什么变成了3和5相邻。注意，在这个过程中我们并没有做显式的删除操作。 接下来的操作与之类似，直到j第一次越过右侧的边界的时候循环退出，算法也就终止。这个时候我们已经无形中将后边的这些元素统一地给删除掉了，这种删除非常的高明，因为我们没有做任何的一次显式的删除操作，而只是通过合理的计算得知了最终的向量规模之后，对_size这个量重新进行了一次设置。 通过这个例子可以得出，算法过程中只是经过了i+1次的迭代，每次移动j必然总是会往后移动一位。而且在每一次过程中，所做的操作无非就是一次比对，只有在比对不同的情况下才会做一次复制，即便是最坏的情况下既比对而且也复制的话，累计起来也不过是常数的时间。所以换而言之，整个这个新的算法只需要$O(n)$线性的时间。 二分查找（版本A）在上一篇文章中介绍了无序向量的查找算法，它的格式为Vector::find(e, lo, hi)，第一个参数指明查找的对象，第二和第三个参数lo和hi指示查找的区间范围。这种算法从思路上来说大体是从一端出发不断地逐个比对，直到发现某一个特定的元素就是e，或者一直到lo-1这个位置在左侧越界，即是查找失败。所以最好情况它只需$O(1)$的时间，但是从最坏的情况以及从一般e的概率分布的平均情况而言，都不得不需要线性的时间。 那么在进入有序向量之后，我们应该可以得到更快的一种解决方案，不妨重新起一个名字叫search()，以示与无序向量的那个find()的区别。当然从操作的参数以及接口的语义来说都是类似的，即我们同样要在lo到hi这样一个左闭右开的区间里找到一个特定的元素。 统一接口123456template &lt;typename T&gt; //查找算法统一接口，0 &lt;= lo &lt; hi &lt;= _sizeRank Vector&lt;T&gt;::search(T const&amp; e, Rank lo, Rank hi) const&#123; return(rand() % 2) ? //按各50%的概率随机选用 binSearch(_elem, e, lo, hi) //二分查找算法，或者 : fibSearch(_elem, e, lo, hi); //Fibonacci查找算法&#125; 这里所提供的search()接口从形式上看是统一的，即ADT。从内部讲，它的具体实现算法却不见得完全一样，后面的的各节将会分别介绍二分查找算法以及Fibonacci查找算法，而且对每一种算法都有不同的版本。 为了做测试这里采用了一个随机的方法，也就是在0和1之间随机地取一个数，从而随机地调用这两个算法。在实际应用中可以针对不同的情况在这几种算法中选择其一。 seach()的简要的操作语义就是在lo和hi所确定的这个区间找出目标元素e（如果它确实存在的话）。这里需要处理很多特殊的情况，比如，目标元素并不存在与规定的区间中，这就叫失败。在此前学习的无序向量的find的接口中我们只是简单地返回了一个标志-1，但严格地说这样做是不够的。反过来有可能目标元素存在多个，既然作为有序向量，一旦有多个e的话，那么它肯定会连续地分布构成一个区间。在这种情况下，到底是返回最前边的一个，最后的一个？还是中间的某一个？这些都是我们需要进一步地从语义上予以约定的。 语义约定在语义上的细致约定是非常有必要的，否则search()接口将只能作为一个孤立的功能，而不能有效地、便捷地为其它的算法，作为一个基本的部件而利用。search()接口至少应该使得有序向量自身的动态维护变得非常便利，比如在有序向量不断插入元素过程中，我们希望往往能够采用这样一种形式：当插入某一个元素时，首先要通过search()来确定一个适当的位置，例如查找返回的那个值再加1，然后再将e插入于这个秩所对应位置，并且同时使得这个有序向量继续是一个有序向量。 1V.insert(1 + V.search(e), e); 幸运的是前人已经帮我们设计出了这样的语义约定，比如下面就是其中的一种约定： 在有序向量区间V[lo, hi)中，确定不大于e的最后一个元素 -∞ &lt; e &lt; V[lo] 时，返回 lo-1 （左侧哨兵） V[hi-1] &lt; e &lt; +∞ 时，返回hi-1（右侧哨兵的前一个） 按照这个约定，对于要查找的元素有重复元素的情况，即有多个元素是与目标的元素是重复的，应该返回的所谓的不大于e的最后一个元素，也就是这个区段的右端点。如果我们要做一个插入，把新的元素插入这个位置同加1后的位置，即重复元素区间右端点的后面，正是再合适不过的。 这里的合适是指：第一，它继续保持了整体的有序性；第二，它以及与它雷同的那些元素会保持它们插入到这个向量中的先后的次序。所以这种语义约定是非常好的，它涵盖了我们几乎所有的情况包括特殊情况。所以接下来我们在实现这些具体的算法的时候，必须最终落实到能够符合这种语义的要求。 原理这个版本只是为了说明原理，从严格的意义上讲，它还不能完全地符合刚才的语义要求，在后面的小节就会对它进行改进。 减而治之：以任一元素x = S[mi] 为界，都可将待查找的区间分为三部分 S[lo, mi) &lt;= S[mi] &lt;= S(mi, hi) // S[mi] 称作轴点 只需将目标元素e与x做比较，即可分三种情况进一步处理： e &lt; x：则e若存在，必属于左侧子区间S[lo, mi)，故可递归深入 x &gt; e：则e若存在，必属于右侧子区间S(mi, hi)，亦可递归深入 x = e：已在此处命中，可随即返回 //若有多个，返回哪个？后面会介绍 二分（折半）策略：轴点mi总是取作中点（至少能保证不是最坏情况） 于是每经过至多两次比较，或者能够命中，或者将问题规模减一半 实现12345678910template &lt;typename T&gt; //在有序向量区间[lo, hi)内查找元素estatic Rank binSearch(T* A, T const&amp; e, Rank lo, Rank hi) &#123; while (lo &lt; hi) &#123; Rank mi = (lo + hi) &gt;&gt; 1; //每步迭代可能要做两次比较判断，有三个分支 if (e &lt; A[mi]) hi = mi; //深入前半段[lo, hi)继续查找 else if (A[mi] &lt; e) lo = mi + 1; //深入后半段(mi, hi) else return mi; //在mi处命中 &#125; return -1; //查找失败&#125; Tips：这里有编写程序的一个小的习惯，可以帮助我们更好地思考问题并且写出算法，更重要的是可以让代码更加好理解，同时也减少一些不必要的失误。我们这里统一地都用了小于号，因为小于号的左右的次序和我们通常所画的这样从小到大的次序是吻合的，所以这里e &lt; A[mi]的解读既可以认为是e小于mi，也可以认为是e存在于当前这个分界点mi的左侧。当这样顺着读下来时，当然我们就应该深入到前半段也就是左半段去，相应地呢，我们应该修改右侧的界桩hi = mi。同样接下来A[mi] &lt; e解读也是这样与其说是mi小于e，不如更直观地说是我们的目标e是处于mi这个分界点的右侧，所以我们应该深入到右半段也就是后半段去继续搜索，相应的操作也就是去修改左侧的界桩lo = mi +1。 实例与复杂度 S.search(8, 0, 7)：共经$2+1+2=5$次比较，在S[4]处命中 S.search(3, 0, 7)：共经$1+1+2=4$次比较，在S[1]处失败 线性递归：$T(n)=T(n/2)+O(1)=O(logn)$，大大优于顺序查找 递归跟踪：轴点总取重点，递归深度$O(logn)$；各递归实例均耗时$O(1)$。 查找长度有序向量的查找是一种非常基本的算法，而且它存在多个版本，因此除了上面利用渐近的复杂度能够从总体上把握它的大体性能以外，我们还需要对不同版本算法的性能做更加细微的评定。具体来说就是考察渐近复杂度$logn$前面的那个常系数，而具体地在统计和分析的时候，更多的是考量关键码的比较操作次数，也就是在其中所执行的if语句的次数，我们将此称作是不同的算法在不同的情况下所对应的查找长度。 如何更为精确地评估查找算法的性能？ 考查关键码的比价次数，即查找长度（search length） 通常，需分别针对成功与失败查找，从最好，最坏，平均等角度评估 例如，成功、失败时的平均查找长度均大致为$O(1.50\\cdot logn)$。 下面是一个一个具体的实例，这是一个由七个元素构成的有序向量，其实它的数值是具体是多少我们并不在意，只要它是非降排列的就可以。如果把算法改写成递归的形式，那么整个的不同情况的递归跟踪将构成下面的递归跟踪图，每条虚线旁边的数字代表由上一步执行到下一步所增加的比较操作的次数，具体位置的方框中的数字代表查找到它所需要总的比较操作次数，即查找长度。需要注意的是，每次递归到左子区间，比较操作次数增加1，而递归到右区间，比较操作次数增加2。 n = 7时，各元素对应的成功查找长度为$\\{4,3,5,2,5,4,6\\}$ ​ 在等概率情况下，平均成功查找长度$=29/7=4.14$； 共有8中失败情况，查找长度分别为$\\{3,4,4,5,4,5,5,6\\}$ 在等概率情况下，平均失败查找长度$=36/8=4.50$； 可见，成功和失败的平均查找长度大致是$1.50\\cdot log_28$ Fibonacci查找改进思路及原理在上一节引入了二分查找（Binary search）这样的一个概念，并且给出了一个基本的算法的版本，这个版本的复杂度从渐近意义而言应该是logn量级的，但如果进一步地细微地来考察前面的系数大致是1.5，我们也指出这个1.5是可以改进的。我们现在就来看看，如何通过一种新的算法：fibonacci查找（fibonaccian search）来对此进行改进。 上一节的末尾以一个长度为7的有序向量为例，具体地给出了在成功和失败情况下平均查找长度的估算的过程。实际上通过那个实例的推而广之，如果考虑更一般的情况，不难发现此前所介绍的版本A，确实还有很大地改进余地。这样一个判断是来自于这样一个观察事实，也就是说版本A这个算法实际上从用意上讲，它是试图通过使各种情况的搜索在迭代次数上的平衡来尽可能地回避掉最坏的情况。 具体讲比如所有的失败情况大部分都会失败在同样深度的，也就是最深的这个位置，所以它表面上看是平衡的，但这其中却蕴涵着很大的不平衡。因为在整个这个查找的过程中我们在任何一个位置上，如果要决定是向左或者是向右深入的话，所花费的成本，也就是比较的次数是不等的。准确地说按照版本A，向左侧只需要一次比较，而向右侧却需要两次比较，所以这样一个表面上看是非常公平的一个平衡，实际上在内部却蕴涵着极大的不平衡，所以我们确实有理由怀疑算法的效率是否已经达到最优。 反过来我们也可以得到改进的一个思路，具体讲就是既然我们已经看到目前的机制中，向左侧确实会成本更低，而向右侧更高。那么为什么不把这个搜索的各种情况画成类似下面的这样一个树状图，做成左侧是更深的，而右侧是相对更浅的。这样一个表面上看的不平衡，却因为它恰好和这种成本互相之间能做一个合适的补偿，反过来有可能从整体上会得到更优，也就是说使得整体的查找平均长度反而会缩短。 具体来讲，越是成本低的转向我们就越希望更多地做，越是成本更高的越是希望它能更少地来做，所以这样的话我们就得到了新的算法的改进的思路。那么具体这个思路怎么来兑现呢？非常有意思的是需要用到fibonacci数。不失一般性，假设有序向量的长度N，就是某个fibonacci数减1的形式。 如下图所示有序向量的长度n = fib(k) - 1，那我们就在其中选择这么样一个特定的切分点mi，mi = fib(k-1) - 1，如果以这个点为切分，那么左边子向量的长度就恰好是fib(k-1) - 1，而右边子向量的长度恰巧是fib(k-2) - 1。可见这样一种切分的好处就是，在任何时候只要按照这样来切分，无论是向左还是向右它都会从长度上保持某个fibonacci数再减1的形式，而这种形式实际上恰好是最优的。 实现首先定义一个Fib类，让其提供一些接口。 12345678910class Fib &#123; //Fibonacci数列类private: int f, g; //f = fib(k - 1), g = fib(k)。均为int型，很快就会数值溢出public: Fib ( int n ) //初始化为不小于n的最小Fibonacci项 &#123; f = 1; g = 0; while ( g &lt; n ) next(); &#125; //fib(-1), fib(0)，O(log_phi(n))时间 int get() &#123; return g; &#125; //获取当前Fibonacci项，O(1)时间 int next() &#123; g += f; f = g - f; return g; &#125; //转至下一Fibonacci项，O(1)时间 int prev() &#123; f = g - f; g -= f; return g; &#125; //转至上一Fibonacci项，O(1)时间&#125;; Fibonacci查找可以实现为下面的一段代码，可以注意到它的接口还是完全一样的，而且在其中的这个循环，大致来说也是与版本A类似的，即每次都要来判断以保证当前的lo和hi构成一个合法的区间，如果这个区间能够收缩到非法(lo == hi)，那也就意味着查找是失败的，这跟此前的版本A是一样的。 123456789101112131415#include \"fibonacci/Fib.h\" //引入Fib数列类// Fibonacci查找算法（版本A）：在有序向量的区间[lo, hi)内查找元素e，0 &lt;= lo &lt;= hi &lt;= _sizetemplate &lt;typename T&gt; static Rank fibSearch ( T* S, T const&amp; e, Rank lo, Rank hi ) &#123; Fib fib(hi - lo); //用O(log_phi(n = hi - lo)时间创建Fib数列 while(lo &lt; hi) &#123; while ( hi - lo &lt; fib.get() ) fib.prev(); //自后向前顺序查找（分摊O(1)） Rank mi = lo + fib.get() - 1; //确定形如Fib(k) - 1的轴点 if ( e &lt; S[mi] ) hi = mi; //深入前半段[lo, mi)继续查找 else if ( S[mi] &lt; e ) lo = mi + 1; //深入后半段(mi, hi)继续查找 else return mi; //在mi处命中 &#125; //成功查找可以提前终止 return -1; //查找失败&#125; //有多个命中元素时，不能保证返回秩最大者；失败时，简单地返回-1，而不能指示失败的位置 查找长度 fibonacci查找算法的平均查找长度为$O(1.44 \\cdot logn)$，略优于二分查找 仍以n = fib(6) -1 = 7 为例，在等概率情况下： 平均成功查找长度$=(2+3+4+4+5+5+5)/7=28/7=4.00&lt;4.14$ 平均失败查找长度$=(4+5+4+4+5+4+5+4)/7=35/7=4.38&lt;4.50$ 最优性 通用策略：对于任何的A[0, n)，总是选取A[λn]作为轴点，$0\\le \\lambda &lt;1$: 比如二分查找对应于$\\lambda=0.5$，Fibonacci查找对应于$\\lambda=\\phi=(\\sqrt{5}-1)/2=0.6180339\\dots$（黄金分割比） 在[0, 1)内，$\\lambda$如何取值才能达到最优？设平均查找长度为$\\alpha(\\lambda)\\cdot log_2n$，何时$\\alpha(\\lambda)$最小？ 递推式：$\\alpha(\\lambda)\\cdot log_2 n=\\lambda\\cdot [1+\\alpha(\\lambda)\\cdot log_2 (\\lambda n)]+(1-\\lambda)\\cdot [2+\\alpha(\\lambda)\\cdot log_2 \\left((1-\\lambda) n \\right)]$ 整理后：$\\frac{-ln2}{\\alpha(\\lambda)}=\\frac{\\lambda\\cdot ln\\lambda+(1-\\lambda)\\cdot ln(1-\\lambda)}{2-\\lambda}$，当$\\lambda=\\phi$时，$\\alpha(\\lambda)=1.440420\\dots$达到最小。 相对于我们上一节的二分查找$\\alpha(\\lambda)=1.50$，Fabonacci查找又有了一定的改进，而且从本节的分析可以看出这种改进已经达到了极限，如果我们不再改变这个算法的总体模式和框架的话。 二分查找（改进）这一节将介绍另一种思路的改进，这是一种直截了当的改进思路，既然我们已经注意到了此前的版本A中造成效率略低的原因是因为左右分支的转向代价不平衡，那么可以考虑是否能将二者做成是平衡的。 改进思路 二分查找中左、右分支转向代价不平衡的问题，也可直接解决 比如，每次迭代（或每个递归实例）仅做1次关键码比较，如此，所有分支只有2个方向，而不再是3个 同样地，轴点mi取作中点，则查找每深入一层，问题规模也缩减一半 1）e &lt; x： 则e若存在，必属于左侧子空间S[lo, mi)，故可递归深入 2）x &lt;= e：则e若存在，必属于右侧子空间S[mi, hi)，亦可递归深入 只有当元素数目hi - lo = 1时，才判断该元素是否命中，这是该算法做出的牺牲 版本B：实现主要注意代码中与版本A不同的地方。 123456789// 二分查找算法（改进）：在有序向量的区间[lo, hi)内查找元素e，0 &lt;= lo &lt; hi &lt;= _sizetemplate &lt;typename T&gt; static Rank binSearch ( T* S, T const&amp; e, Rank lo, Rank hi ) &#123; while ( 1 &lt; hi - lo ) &#123; //每步迭代仅需做一次比较判断，有两个分支；成功查找不能提前终止 Rank mi = ( lo + hi ) &gt;&gt; 1; //以中点为轴点（区间宽度的折半，等效于宽度之数值表示的右移） ( e &lt; S[mi] ) ? hi = mi : lo = mi; //经比较后确定深入[lo, mi)或[mi, hi) &#125; //出口时hi = lo + 1，查找区间仅含一个元素A[lo] return (e == A[lo]) ? lo : -1; //返回命中元素的秩或者-1&#125; 这个算法是封闭的，可以运转，而且可以完全实现此前一样的功能。与此前的版本A对比，它在最好情况下反而有所倒退，原因是在与即使是成功的情况它也一直要推迟到最终，只有在经过最终的这次比对之后才会确定是否成功。此前的版本A它的最好情况是非常好的，最最好的情况莫过于在第一次试图做减而治之的时候，所采用的那个切分点就成功命中，只需要$O(1)$的时间。 本节改进的二分查找无论如何都一直要切分到最后，所以最好的情况的时间复杂度是$O(logn)$。但是反过来最坏的情况又会更好，因为我们这里最坏的情况不会出现每一次都是向右，即每次都要花费两次比较的情况，所以最坏的情况会得到抑制。所以从总体而言此前的那个版本A如果说它在性能上好坏情况相差非常大的话，那么本节中改进的版本在整体性能上，它就会趋于更加的稳定，即差异化不是那么大，当然这还不是它的最大的优势所在。 语义约定 以上的二分查找及Fibonacci查找算法，均未严格地兑现search()接口的语义约定： 返回不大于e的最后一个元素 只有兑现这一约定，才可以有效支持相关算法，比如：V.insert(1 + V.search(e), e) 1）当有多个命中元素时，必须返回最靠右（秩最大）者 2）失败时，应返回小于e的最大者（含哨兵lo-1） 版本C：实现在刚才代码的基础上，我们做进一步的调整，得到一个最终的版本，它可以严格地实现上面定义的语义。 12345678template &lt;typename T&gt; static Rank binSearch ( T* S, T const&amp; e, Rank lo, Rank hi ) &#123; while ( lo &lt; hi ) &#123; //不变性：A[0,lo) &lt;= e &lt; A[hi,n) Rank mi = ( lo + hi ) &gt;&gt; 1; //以中点为轴点 ( e &lt; S[mi] ) ? hi = mi : lo = mi + 1; //经比较后确定深入[lo, mi)或(mi, hi) &#125; //出口时，A[lo = hi]为大于e的最小元素 return --lo; //故循环结束时lo - 1即不大于e的元素的最大秩&#125; //有多个命中元素时，总能保证返回秩最大者；查找失败时，能够返回失败的位置 就算法的结构而言，这个新的算法版本C和此前的版本A，尤其是版本B，似乎没有什么太大的区别。解读一下：当这个区间还是合法之前我们就不断地迭代，每一次也照样是取出它的中点作为轴点，并且经过一次比较从而决定到底是向左侧还是向右侧深入，那么直到区间宽度缩小到足够小的时候，才返回最终的值。 需要注意的是版本C和版本B，虽然在功能上是等效的，但是在很多细节上却有着本质的区别： 待查找区间宽度缩短至0而非1时，算法才结束 转入右侧子向量时，左边界取作mi+1，而非mi //A[mi]会被遗漏？下一小节证明 无论成功与否，返回的秩严格符合接口的语义约定 正确性首先通过下面的图例来具体地了解一下版本C的工作过程，其实最主要的是它的每次迭代的过程都是类似的。如图(a)，在整个向量的区间内，我们关注的是某一个特定的从lo到hi的一个查找区间，每次在这个区间里都要考虑middle point，即图中的x。 我们以它为界，经过一次比较以后有可能会发现目标元素更小所以就深入到如图b所示的左侧的这个子区间；或者对称地，因为目标元素更大而深入到右侧的这个区间，如图(c)。版本C的算法中左侧子区间和右侧的子区间都没有覆盖这个middle point，而且对middle point也没有做显式地判断，所以这也是为什么有理由怀疑它有可能是这个算法的一个疏忽。 接下来我们来证明这样一个模式实际上是安全的，为此同样用我们的两种技巧：第一就是给出这个算法的不变性其次要给出它的一个单调性，而单调性是一目了然，就不再说明了，主要是证明它的不变性： 不变性：A[0, lo) &lt;= e &lt; A[hi, n) //A[hi] 总是大于e的最小者 初始时，lo = 0且 hi = n，A[0, lo) = A[hi, n) = $\\varnothing$，自然成立 数学归纳法：假设不变性一直保持至图(a)的状态，下一步无非两种情况： 第一种情况，也就是深入左侧这个分支的情况，即图(b)。那么此前的判断e &lt; A[mi]返回的是True，之后执行 hi = mi，从而使得右侧的这段区间向左拓展是安全的，因为确实可以断定这个整个区间内的这些元素都是严格地大于e的，因为它们其中最小的那个元素也就是A[mi]都大于e。而A[0, lo)保持不变,所以这种情况是没有问题的。 第二种情况，也就是深入右侧这个分支的情况，即图(c)。那么此前的判断e &lt; A[mi]返回的是False，之后执行 lo = mi，此时e是不小于A[mi]的，而A[mi]元素是左段区间中最大的，所以左段区间都是都是不大于e的。这样一个左侧区间向右拓展的动作在刚才不变性的意义上讲，依然是安全的，它使得不变性得到了延续。所以经过一次迭代以后无论是向左还是向右的深入，不变性都是成立的。 单调性：显而易见，直到最后会出现一个情况，就是整个区间的宽度变成零，可以表示为下图。 从整个的原始的搜索空间开始，经过不断地压缩、压缩、压缩之后，将搜索的范围缩小到一个宽度为零的一个区间，其实它就只是一个分界。它严格地将整个区间分为了左右两部分，由不变性左侧这部分依然是不大于e，而右侧这部分是严格地&gt;e。如果查找的结果是命中的，我们只需要返回左侧这个区间的最右端的那个元素就可以了，而这个元素正是A[lo-1]。这也就是为什么我们在算法的最终返回之前要做一次--lo的操作。 这样的话我们就得到了一个从功能上、从语义上、从性能上都近乎完美的算法！ 插值插值插值查找（Interpolation Search）有序向量查找算法的一个另类的变种，此前所介绍的Fibonacci search或binary search包括它们的各种版本对向量只做了一个假定，即其中的元素是单调有序的，对于其中元素的分布情况并没有做任何的假设，也就是可以是完全理想任意随机的。但是在某些情况下也许不是这样，比如我们可能不仅知道向量是有序的，而且其中的元素是按照某种先验规律随机分布的。 在这里我们考虑一种最常见的随机分布：均匀独立的随机分布，比如在从lo一直到hi的秩的范围之内，所有的元素都是互相不影响，各自独立的，然后从取值来看是均匀的取自于某一个区间范围。如果我们确实知道诸如此类的规律的话，就有可能实现优于此前那些算法$O(logn)$的更高的查找效率，以$o(logn)$的效率来完成一次查找。 原理与算法在均匀且独立的随机分布下，所有的元素在排序之后，即组织成一个有序向量之后，必然大体上是按线性增长的趋势分布的，从最小值lo开始大致是线性增长到最高值hi。这就意味着对于其中的任何一个潜在元素mi，都可以写出这样一个近似的线性等式，它们的秩的比与它们的数值比，二者是近似接近的。 \\frac{mi-lo}{hi-lo}\\approx \\frac{e-A[lo]}{A[hi]-A[lo]}实际上这给了我们一个启示，即在每次确定mi的时候，既不需要固定的用1/2，也不需要固定的用小写的φ（黄金分割比），甚至不需要用某一个一般的λ，而是可以动态的来猜测这样一个轴点，就是根据上面的等式。将这个等式稍微整理一下把mi提到左侧，我们就可以知道根据lo、hi以及它们对应的这两个元素的数值，以及每次动态要查找的那个元素的数值e，就可以大致的估算出mi，这样的话如果整个的减而治之的搜索过程可以认为是一个不断收缩包围圈逐步收敛的一个过程，那么它将会使得收敛的速度极大的加快，从而更快速的完成我们整个的查找。 mi\\approx lo+(hi-lo)\\cdot \\frac{e-A[lo]}{A[hi]-A[lo]} 正如这个图所画的是一本英文词典中abcd一直到z开头的单词各自起始的页码,它大致是1300多页，换而言之如果它确实是一个大致平均分布的话，每一个字母大概占50页，所以我们可以大致估算出来从1到50页大概是a，50页到100页大概是b，100页到150页大概是c，诸如此类。比如说去查binary (b)，那么因为它是第二个字母所以它大概会在整书从2/26这个位置开始，而search,s是第19个元素 所以大概它会位于19/26的位置。正因为这种算法在确定切分点也就是轴点的时候，采用的是近似的插值估算的方法，所以我们也称之为Interpolation Search插值查找，下面是一个实例。 性能从刚才的例子我们可以看出，对于这样一个长度为19的有序向量，只用了3次比较就给出了答案，而在通常的二分查找中这是做不到的，所以我们已经看到它在某些情况下确实很快，但是它总是能很快吗？包括这种很快到底定性是多大呢？ 我们需要做一个严格的界定，首先一个不好的消息是插值查找在某些情况下效率会很低，比如说 可能退化为与平凡的顺序查找没有什么区别，我们此前所做的那种假设也就是均匀独立的分布不满足，或者至少在某些部分不满足以致在全局或某些局部出现一些所谓的病态分布。 最坏情况：$O(hi- lo)=O(n)$ 当然 插值查找的最好情况也是不言而喻的，和其他的查找差不多，也就是说有可能我们在某次，甚至在第一次猜测的时候就直接命中，那么这种我们也不再考虑。我们转而再考虑一般的情况，也就是平均而言会怎么样。 这里我们需要用到一个非常基础类似引理的结论这个结论：在插值查找算法中每经过一次迭代，或者说每经过一次比较，都可以将查找的范围也就是减而治之之后剩余的部分由原先的规模n缩减为$\\sqrt{n}$。 平均情况：每经过一次比较，$n$缩减至$\\sqrt{n}$。 于是，待查找区间宽度将按一下趋势缩减： ​ $n,\\quad\\sqrt{n},\\quad \\sqrt{\\sqrt{n}},\\quad \\sqrt{\\sqrt{\\sqrt{n}}},\\dots,\\quad2$ ​ $n,\\quad n^{(1/2)},\\quad n^{(1/2)^2},\\dots,\\quad n^{(1/2)^k},\\dots,\\quad2$ 经多少次比较之后，有$n^{(1/2)^k}&lt;2$？ $k&gt;loglogn$ 插值查找的时间复杂度为：$O(loglogn)$ 我们同样可以来估算：如果向量的长度或者这个区间的宽度是n的话，考虑这个n按照二进制打印出来以后的位宽就是以的2为底 logn，那么每一次将它变为根号n从二进制的打印宽度来看其实就是变成了1/2的原来那么多宽度，换而言之每一次开方其实同步的是使宽度变成了原来的1/2，这样的过程 从n的数位宽度来说是一个不断折半的过程。 回顾此前的二分查找，如果是对的n的数值每次折半的话，那么这里的插值查找实际上就是对n的二进制位宽度来做二分查找。二分查找所需要的迭代次数是与它的初始值呈一个对数关系的，即$O(logn)$，而插值查找的位宽的初值相当于是logn，所以其需要的迭代次数就是$O(loglogn)$。 从今以后也许我们应该学会忘掉这些复杂的，虽然是精确的数学，而改用这种宏观的大趋势的把握本质的习惯。 综合对比现在将插值查找和其他的算法综合起来进行比对和考量，刚才插值查找所实现的这种改进也就是从logn到loglogn虽然从数学上是一个比较大的改进，但从实际效率来看却值得商榷。 从$O(logn)$到$O(loglogn)$，是否值得？ 通常优势不明显，除非查找区间宽度极大，或者比较操作成本极高。 比如，n = 2^(2 ^ 5) = 2 ^ 32 = 4G时，$log_2(n)=32,\\quad log_2(log_2(n))=5$ 易受小扰动的干扰和“蒙骗”，可能在局部花费非常多的时间 须引入乘法、除法运算，相对而言成本更高（二分查找只需加法，Fibonacci查找只需加法和减法） 所以可行的查找算法也许应该将插值查找以及此前的那些查找算法各自的优势综合结合起来，比如说插值查找更善于在比较大的一个宏观的范围内，将问题的关注点尽可能快的缩小到一定的范围，即它比较擅长于处理那种极大的情况，然后一旦到了比较小的情况，这种容易受到干扰包括蒙骗尤其是乘法除法这样的一些overhead额外计算占得比重就会更大成为不可忽略的因素，而在这个时候二分查找的优势就体现出来了。 实际可行的方法： 首先通过插值查找，将插值范围缩小到一定的范围，然后再进行二分查找，或者顺序查找，即： 大规模：插值查找 中规模：折半查找 小规模：顺序查找","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"数据结构与算法（2）向量","slug":"数据结构与算法（2）向量","date":"2020-02-08T02:04:27.000Z","updated":"2020-02-09T02:04:27.000Z","comments":true,"path":"2020/02/08/数据结构与算法（2）向量/","link":"","permalink":"http://nekomoon404.github.io/2020/02/08/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%882%EF%BC%89%E5%90%91%E9%87%8F/","excerpt":"","text":"1.接口与实现我们首先需要辨析一组非常相关但是又非常容易弄混的概念，也就是抽象数据类型以及数据结构。那么什么是Abstract Data Type呢？以及什么是Data Structure呢？可以从字面上给出定义，抽象数据类型就是在一组数据的模型上定义的一组操作。数据结构则是基于某种特定的语言真正实现的一套完整的算法。 Data Type数据类型，比如在高级程序设计语言中int也就是整型，这就是一个数据类型，而float也是，还有char，诸如此类地。这种数据类型能够让我们能定义其中的一个成员，比如n是一个整数，从此以后我们就可以使用它了，我们也可以定义x是一个浮点数，c是一个字符。 123int n;float x;char c; 凡是这样指定了某一个元素是来自于某一个数据类型，或者说属于某一个数据类型，那么它就自然地具有了这种数据类型的特点，包括支持相应地处理方法，比如说运算。那么这里那些操作的运算具体是如何实现的，我们并不知道，我们也并不需要知道，这是最重要的。 把这样的一个概念抽象出来施加到我们所将要实现的数据结构上，比如这一章要介绍的vector。我们希望在使用的时候能够参照数据类型的这种形式，把它等同地当作是一个数据类型，比如可以用类似的方法来定义一个vector结构，包括下一章将要介绍的List。 这种使用方法使得我们可以将数据结构与数据类型等同起来，我们只需要知道它所提供的那些操作，比如说向量的查找、排序，而不需要去关心它其中的细节，比如说这些操作是如何实现的。那么从这个意义上讲，它就是一个经过了抽象以后的数据类型，所以称之为Abstract Data Type。 举个例子：可以将数据结构比喻成某种产品，比如说汽车，相关的有两类人，首先是用户，我们笼统地称之为应用Application，另一类人是汽车这种产品的设计和制造者，称之为实现Implementation。这两类人所关心的以及他们的职责是不同的，作为用户而言，他只关心这种产品的外在特性，能够提供的功能；而实现者则需要对这些功能以及特性具体如何落实负责。在这二者之间实际上是有某种形式的一个协议，也就是使用说明书，产品手册。而这种手册或者说明在数据结构的使用者与数据结构内部算法的设计者之间，达成了这么样一个协议，两类人可能互不见面，互不相识，但是他们通过这样一个规范，可以很好地彼此沟通，并且有效地合作。 1.1.向量ADT1.1.1.从数组到向量向量实际上是C++等高级编程语言中数组这种数据组织形式的一个推广和泛化。实际上在这些高级程序设计语言中所谓的数组实际上就是一段连续的内存空间，它被均匀地划分为若干个单元，而每一个单元都与0到n之间的某一个整数编号相互彼此对应。这里我们也同样延用此前已经约定的习惯，虽然最后这个第n个元素，实际上未必存在，我们还是把它虚拟地放在这儿作为哨兵，以帮助我们对很多问题的思考，并且使得我们很多算法的实现能够得以简化。 C/C++语言中，数组A[ ]中的元素与[0,n)内的编号一一对应。 既然每一个这样的元素都与这些编号是一一对应的，所以反过来我们通过合法区间内的编号都可以唯一地来指代并且访问对应的那个元素。一旦知道这个元素的下标i，就可以从A也就是这段存储区域的首地址出发，再向后以s作为间隔去数出i步，就可以得到某一个特定的单元。正因为所有这些元素的物理地址可以按照这样一个线性的方程来确定。所以我们也称之为线性数组（linear array）。 反之每个元素均由（非负）编号唯一指代，并可直接访问。A[i]的物理地址 = A + i×s，s为单个元素占用的空间量。 向量是数组的抽象与泛化，由一组元素按线性次序封装而成： 各元素与[0, n)内的秩（rank）一一对应 元素的类型不限于基本类型 操作、管理维护更加简化、统一于安全 可更为便捷地参与复杂数据结构的定制与实现 1.1.2.向量ADT接口按照抽象数据类型的规范，向量结构必须提供一系列的操作接口，可以通过这些操作接口对向量做各种操作，同时也只能通过这些操作接口对向量进行操作，这里的接口功能非常的丰富。 比如说与其它的数据结构一样向量也可以看作是一组元素的集合，所以size( )实际上返回的是其中元素的总数，称之为这个数据结构的规模。也可以从中取特定的元素get(r)，也可以修改其中特定的元素put(r, e)，甚至插入insert(r, e)或者是删除某个元素remove(r)。我们也可以判定一下其中的元素是否已经有序排列disordered( )，如果没有有序排列，可以调用相应的接口使之有序排列sort( )。 我们也可以在它尚未有序排列的时候，按某种算法找到其中特定的元素find(e)，也可以在已经有序的前提下按照某种方式，来找到其中的元素search(e)。当然为了展示一些算法的实现我们也附加了一些其它的功能，比如说能够在无序和有序的情况下分别剔除这个数据集中的重复元素：deduplicate( )和uniquify( ) 。最后也是非常重要的一个接口就是如何对这个数据集中的元素逐一地进行枚举，并且访问一遍traverse( )，称之为遍历。 1.1.3.ADT接口操作实例下面举例说明ADT接口的实现。 最开始向量与任何一个数据结构一样，初始化的时候都是不包含任何实质的内容的，我们称它是一个空的向量。接下来调用插入操作insert，它在rank为0的这个位置上插入一个元素9，所以向量的组成将由空变成包含一个元素9。接下来继续调用insert接口，在0号这个位置上rank为0的这个位置上插入一个元素4，原来的元素9将会后移一位。同样地，我们也可以调用插入接口在rank为1的位置上插入5，在这个位置上出现了5，而它的后继统一地向后后移了一位。我们也可以调用put接口，这个接口的意思是修改，它会把当前rank为1的那个位置上的元素数值，由原来的5修改为2。我们也可以通过get这个接口获取秩为某一特定值的元素，比如说秩为2的那个元素，实际上就是2这个位置上的9，因此会返回9。 remove接口的参数是2，这说明它希望在原来这个向量中将rank为2的这个元素，把它剔除掉，剔除之后，会把这个被剔除的元素的值作为输出返回，即返回2，同时它的所有的后继与插入时候的操作的现象相反，会向前平移一个单元。当这个时候我们调用size的时候，因为这里所包含的元素总共是6个，所以它会返回6。 我们可以看到在整个这个操作的过程中向量都确实具有这么样一个特点，就是它在逻辑上，甚至在物理上必然是彼此紧邻的排列的，所有的元素之间没有任何的缝隙。需要注意的是无论是此前所介绍的这些接口，还是后面所要介绍的接口，就目前而言，我们并不关心它的具体实现方法，我们关心的只是它的操作语义。 接下来我们可以通过disordered()这个接口来检测向量的有序性，或者更准确地讲它的无序性。在此前介绍bubble sort算法的原理的时候，曾经指出包括向量在内的序列是否有序，当且仅当其中是否存在紧邻的逆序对。那么这里总共有6个元素，共定义了5组紧邻对，其中有3组，也就是4和3、7和4、和9和6是逆序的，disordered会返回逆序对的个数，即是3，只要这个数值不是0，就说明它尚未构成有序的序列。 对于这样的一个无序向量我们已经可以通过find接口，来查找其中特定的某个元素，比如说9。可以看到9号元素是位于rank为4的位置，因此find会返回4。同样地，也可以查找比如说5，我们发现5并不存在，这个时候我们统一地约定返回一个数值是-1，这个-1肯定不是一个合法的rank，表示查找失败。接着我们可以通过sort这个接口对整个向量排序，接下来再调用disordered()这个接口，它已经没有任何逆序的紧邻对了，所以返回0。 对于有序向量，我们可以通过另一套接口，也就是search来进行查找。比如说可以首先通过search，然后引用9来查找数值为9的元素，这个元素的rank为5，因此返回的是5。那么如果查找8会怎么样呢？向量中并没有8，这里我们采用了另一种约定：如果没有找到这个元素，我们要找的是不超过这个元素的最大的那个元素的值。对这个例子而言不超过8的最大的元素实际上就是7，而7的秩是4，所以search(8)会返回4。同样 我们如果要去查找10的话会返回不超过10的最大的那个元素也就是9的秩5，因此search(10)会返回5。 另一种特殊情况：查找一个全局都没有而且小于全局的最小的那个元素的数比如说1，我们会假设在-1的rank这个位置上有一个假想的哨兵，它的数值是负无穷，所以search(1)返回的是-1。这样一套约定可以使得我们在语义上更加的明确，使得我们在后续的操作过程中可以便利地来搭建不同的算法。还有一点要注意的是：在有些时候，我们要查找的元素尽管有，但是它却有多次出现，比如说这个4 出现了两次，那这个时候会返回什么呢？同样跟这里的语义所定义吻合的是，我们要返回其中不超过4这个目标元素的最后边那个元素，所以如果有两个甚至多个4的话，我们会取其中rank最大的那个元素把它的rank返回，对这个例子而言也就是2号元素，因此search(4)会返回2。 最后，uniquify()对于一个有序的向量把所有的重复的元素，比如说4都剔出掉，只保留一个拷贝。 1.2.vector模板类有上述接口规范之后，我们就可以遵照这种规范来学习如任何具体地在C++语言平台上实现这样一种向量模板类vector结构。首先约定用int来定义这里所说的秩这种概念，接下来会首先采用一种基本的扩容方式，它的初始容量需要设定，这里不妨取它的DEFAULT_CAPACITY取作3，在实际应用中完全可以取更大的一个数。 下面通过template这种方式给一个模板参数T，它的意思可以认为是定义了一个vector这样的模板类。其中的元素类型是什么可以是将来指定的任何名字现在叫作T的类型。所以与其说它写的是一个类，不如说这个模板类给的是一系列的类，我们可以根据实际需要直接地生成相应的vector类。在模板类里面有一些私有的，也就是封装和隐藏起来的变量，比如说其内部会记忆它到底有多少个元素有一个_size ，以及它目前的容量_capacity，还有包括真正存放元素的一个空间_elem。其它的内部函数以及公开的接口函数会在后边陆续学到。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859typedef int Rank; //秩#define DEFAULT_CAPACITY 3 //默认的初始容量（实际应用中可设置为更大） template &lt;typename T&gt; class Vector &#123; //向量模板类protected: Rank _size; int _capacity; T* _elem; //规模、容量、数据区 void copyFrom ( T const* A, Rank lo, Rank hi ); //复制数组区间A[lo, hi) void expand(); //空间不足时扩容 void shrink(); //装填因子过小时压缩 bool bubble ( Rank lo, Rank hi ); //扫描交换 void bubbleSort ( Rank lo, Rank hi ); //起泡排序算法 Rank max ( Rank lo, Rank hi ); //选取最大元素 void selectionSort ( Rank lo, Rank hi ); //选择排序算法 void merge ( Rank lo, Rank mi, Rank hi ); //归并算法 void mergeSort ( Rank lo, Rank hi ); //归并排序算法 void heapSort ( Rank lo, Rank hi ); //堆排序（稍后结合完全堆讲解） Rank partition ( Rank lo, Rank hi ); //轴点构造算法 void quickSort ( Rank lo, Rank hi ); //快速排序算法 void shellSort ( Rank lo, Rank hi ); //希尔排序算法public:// 构造函数 Vector ( int c = DEFAULT_CAPACITY, int s = 0, T v = 0 ) //容量为c、规模为s、所有元素初始为v &#123; _elem = new T[_capacity = c]; for ( _size = 0; _size &lt; s; _elem[_size++] = v ); &#125; //s&lt;=c Vector ( T const* A, Rank n ) &#123; copyFrom ( A, 0, n ); &#125; //数组整体复制 Vector ( T const* A, Rank lo, Rank hi ) &#123; copyFrom ( A, lo, hi ); &#125; //区间 Vector ( Vector&lt;T&gt; const&amp; V ) &#123; copyFrom ( V._elem, 0, V._size ); &#125; //向量整体复制 Vector ( Vector&lt;T&gt; const&amp; V, Rank lo, Rank hi ) &#123; copyFrom ( V._elem, lo, hi ); &#125; //区间 // 析构函数 ~Vector() &#123; delete [] _elem; &#125; //释放内部空间 // 只读访问接口 Rank size() const &#123; return _size; &#125; //规模 bool empty() const &#123; return !_size; &#125; //判空 Rank find ( T const&amp; e ) const &#123; return find ( e, 0, _size ); &#125; //无序向量整体查找 Rank find ( T const&amp; e, Rank lo, Rank hi ) const; //无序向量区间查找 Rank search ( T const&amp; e ) const //有序向量整体查找 &#123; return ( 0 &gt;= _size ) ? -1 : search ( e, 0, _size ); &#125; Rank search ( T const&amp; e, Rank lo, Rank hi ) const; //有序向量区间查找// 可写访问接口 T&amp; operator[] ( Rank r ); //重载下标操作符，可以类似于数组形式引用各元素 const T&amp; operator[] ( Rank r ) const; //仅限于做右值的重载版本 Vector&lt;T&gt; &amp; operator= ( Vector&lt;T&gt; const&amp; ); //重载赋值操作符，以便直接克隆向量 T remove ( Rank r ); //删除秩为r的元素 int remove ( Rank lo, Rank hi ); //删除秩在区间[lo, hi)之内的元素 Rank insert ( Rank r, T const&amp; e ); //插入元素 Rank insert ( T const&amp; e ) &#123; return insert ( _size, e ); &#125; //默认作为末元素插入 void sort ( Rank lo, Rank hi ); //对[lo, hi)排序 void sort() &#123; sort ( 0, _size ); &#125; //整体排序 void unsort ( Rank lo, Rank hi ); //对[lo, hi)置乱 void unsort() &#123; unsort ( 0, _size ); &#125; //整体置乱 int deduplicate(); //无序去重 int uniquify(); //有序去重// 遍历 void traverse ( void (* ) ( T&amp; ) ); //遍历（使用函数指针，只读或局部性修改） template &lt;typename VST&gt; void traverse ( VST&amp; ); //遍历（使用函数对象，可全局性修改）&#125;; //Vector vector模板类的原理：整个vector结构是被封装起来，能供来自各种应用的用户使用的操作接口就是interface框中vector，~vector，insert，remove等等，它们就相当于vector结构的使用说明书，它告诉我们这里提供了哪些操作渠道、途径，通过这种接口规范直接使用。经过了这样地一个剥离之后，使得我们的应用和实现相互之间可以很好的分工，又同时很好的协作。那么具体内部怎么实现的呢？可以看出其实是开辟了一个名字叫作_elem的数据区，它的容量至少要足以容纳所存放的有效数据，对外而言的每一个元素都通过某种形式转译为内部这段数据区中的，实际上是这个有效的数据区（_size）中的某一个元素，由此实现了对内部数据项的封装。 1.2.1构造与析构作为一种数据结构与所有的类一样，vector也首先需要解决构造和析构的问题。向量的默认的构造实际上只需指始初始的容量就可以了，如果没有指定会按照默认的容量，指定一个数值。在内部的操作其实就是通过new申请一个长度为c，基本类型就是模板参数T的一段连续的数据空间。在创建了这样一个空间之后，我们把这个空间的首地址交给内部的_elem记下来。这个时候虽然它有一定的空间，但是其中有效的数据是没有的，所以这就是为什么_size初始化是0。 12345Vector(int c = DEFAULT_CAPACITY)&#123; _elem = new T[_capacity = c]; _size = 0;&#125; //默认 当然还有其它的一些构造的方法，比如如果已经有一组以数组的形式存放的数据，我们也可以将其中从lo到hi的这段区间中的元素取出来作为初始向量，可以看到它是通过调用一个叫作copyFrom()的内部接口实现的。同样地 它还重载了其它的一些形式，比如被复制的元素可能是来自于一个数组，而是来自于一个本身已经被封装了的向量，我们可以从这个向量的_elem区域中去读取出来，并且同样调用copyFrom()来做这件事。所以这里有区间的复制，也可以有对整个向量的一个克隆。 123456Vector(T const *A, Rank lo, Rank hi) &#123; copyForm(A, lo, hi);&#125; //数组区间复制Vector(Vector&lt;T&gt; const &amp;V, Rank lo, Rank hi) &#123; copyForm(V._elem, lo, hi);&#125; //向量区间复制Vector(Vector&lt;T&gt; const &amp;V) &#123; copyForm(V._elem, 0, V._size);&#125; //向量整体复制 内部操作接口copyForm( )的工作原理以及过程可以通过下图示意，工作原理以及过程，可以通过这个图来示意。一般地我们需要从一个数组A中将介于lo到hi之间的元素整体复制到当前仍然为空的一个向量中，具体的操作大概分为两步，首先在向量内部开辟出足够的空间，接下来再将区间内的元素逐一地复制过来。 这个过程可以描述并且实现为下面的C++代码：首先申请足够多的空间，这里需要再强调一下这个区间的宽度可以直接通过lo和hi之间的一个减法得到，这是因为当我们在描述一个区间的时候往往是用左闭右开的形式，所以换而言之这个lo是在这个区间中最靠左的那个元素，而hi是在右侧第一个不属于这个区间的那个元素，尽管hi这个元素有可能压根就不存在。但是我们不妨把它统一地理解成是一个哨兵，这样的话我们就可以通过，hi减lo直接得到区间的宽度。 这里给计算出的宽度再乘个2，也就是说我们实际开辟的空间是我们需要复制的空间的两倍，而不是恰好那么多。这样做的主要的目的在于预留了一些空间之后，就可以使得我们在接下来足够长的时间之内，不会因为有必要扩容而打断我们的计算过程。 1234567template &lt;typename T&gt; //元素类型void Vector&lt;T&gt;::copyFrom (T const* A, Rank lo, Rank hi)//以数组区间A[lo, hi)为蓝本复制向量&#123; _elem = new T[_capacity = 2 * ( hi - lo ) ]; _size = 0; //分配空间，规模清零 while ( lo &lt; hi ) //A[lo, hi)内的元素逐一 _elem[_size++] = A[lo++]; //复制至_elem[0, hi - lo)&#125; 接下来还需要对这个向量的有效规模进行初始化 把它清为0。 再接下来 就是复制过程也就是说我们对于lo和hi中间的每一个Rank，都要从A这个数组中取出对应的元素，并将它们顺次的存入到_elem，对应的区间里面去。整体循环构成了这个操作的最重要的部分，所以我们也可以看出算法的复杂度主要是来自于这样一个循环。这样一个主体的复杂度是取决于被复制元素的个数，或者说这个复制区间的宽度，也可以认为是这个向量通过复制被创建之后的初始规模。 析构函数只需要把这个曾经动态分配获得的数据区域释放掉，归还给操作系统。 1~Vector() &#123; delete [] _elem; &#125; //释放内部空间 这样的话我们就完成了向量这种最基本的结构作为一种模板类它的最基本的一些接口，接下来会学习功能更为复杂的其它的接口。 2.可扩充向量与所有的数据结构一样，向量也可以认为是一组数据项的集合，换而言之，它首先必须能够自适应地在规模上适应其中所包含的元素个数的变化，这一节集中讨论它的可扩充性能。向量本身并不具有这种性能，我们需要采取一些策略。就目前的设计方案而言，我们的向量并不具备可扩充的性能，究其原因在于它采用的 实际上是所谓的静态空间管理的策略。 2.1.静态空间管理具体来说，它实际上在内部只不过是设置了一个私有的数组，这个数组所占有的那段连续的地址空间会被用来存放若干个对外界而言可见的，或者是有效的元素。这些元素的总数，或者说它们所占用的逻辑空间的数，用_size来表示，而整个物理空间的大小是由_capacity来确定的。 这里的问题是_capacity一旦确定,按照目前的方案它就将一成不变，而这样一种策略显然存在明显的不足。这种不足体现在两个方面：第一 是有可能会出现所谓的上溢overflow，也就是说随着有效元素（个数）的增加，总会出现这样的可能，使得整个_elem所占用的物理空间已经不足以存放需要存放的元素组。尽管这个时候在系统的其它的部分仍然有足够多的空间可以用于存放这些元素，但是限于_capacity是固定的，我们不能直接做到这一点。 另一种情况虽然不是很严重，但是也是会造成一定的空间的效率低下，我们称之为下溢underflow。具体来说就是有可能我们开辟了一个比较大的空间，但是在整个这个数据结构的生命期内真正存放于其中的数据却寥寥无几，从而使得装填因子指标会非常非常的小，这个装填因子其实就是有效元素个数，也就是_size ，去除以可用于存放元素的空间总数_capacity，也可以理解成是空间的利用率有可能不到一半，甚至远远地低于一半，那么在这种时候空间效率非常低下。 很遗憾如果我们坚持采用这样一种固定容量的策略，我们在实际的一般应用环境中，很难在事先就预测到我们需要用多少空间，也就是说这种空间不足以及空间浪费的情况，都有可能发生甚至经常发生。 那么如何使得向量可以自适应地根据实际需要来动态地调整自己的容量呢？而且这种调整的过程既能保证足够同时又不致使得因为开辟的空间过多而导致空间效率的低下。 2.2.动态空间管理为了解决上述的问题，我们需要把刚才所采用的静态空间管理策略改变为所谓的动态空间管理策略，就是如果在某个时刻，某一个向量即将发生上溢，那么我们就适当地扩大内部数组的容量，使之足以容纳新的元素。按照这样一种策略向量的生命期可以大致由下面一组图来表示。 最开始的时候向量所存放的有效元素还不是很多，还不致于出现上溢的情况，这时候可以从容应对。但是剩余的空间有可能会逐步地被占用，直到某一个关键时刻，内部数组有可能已经饱和，这时就存在一个风险也就是说再插入一个元素的话，就会导致上溢。为此我们可以动态的申请另一段存放空间，当然它的大小应该比原来的有所增长。接下来我们要把原先已经存放好的那些有效元素，逐一地按次序地复制过来，从而使得它们对外界而言依然保持原貌。新多出来的这些空间就足够用以存放新需要插入的元素，而原来所占用的空间将在此之后被释放并且归还给系统。上述这样一个完整的调整过程可以描述并且实现为下面的c++的代码： 12345678910template&lt;typename T&gt;void Vector&lt;T&gt;::expand() &#123; //向量空间不足时扩容 if (_size &lt; _capacity) return; //尚未满员时，不必扩容 _capacity = max(_capacity, DEFAULT_CAPACITY); //不低于最小容量 T* oldElem = _elem; _elem = new T[_capacity &lt;&lt;= 1]; //容量加倍 for (int i = 0; i &lt; _size; i++) //复制原向量内容 _elem[i] = oldElem[i]; //T为基本类型，或已重载复制操作符'=' delete[] oldElem; //释放原空间&#125; 首先要判断现在是否处于即将发生上溢的临界状态，它的标志就是_size是否还继续严格地小于_capacity。如果是还不存在上溢的风险，可以直接返回，所以这里隐含着有一个else，即接下来_size虽然不一定大于_capacity，但是至少会出现等于_capacity的情况。 这时我们要做的是将原来的那个数据域做一个备份，接下来以原先的容量（注意这里是左移一位，相当于加倍）加倍的一个新的容量来申请一段动态空间，并且将这段空间交由原来的_elem来指示。接下来是复制，对从原先的那个数据域中逐一地取出各项，并且将其转移至新的这个数据域中对应的位置。在整体赋值完之后，原先的这个空间已经没有任何存在的意义了，所以通过delete操作将它释放。 其实对于尚未封装的数组同样可以采用上述的这样的一个策略，而对于向量而言，这里调整的优势体现在向量整体的封装性上。因为对于一般的数组，如果它经过了动态的重新分配地址，那么原先指向它内部的某些元素的一些指针就有可能会出现无效，即虽然它能指向一个地址但其中并没有存放所需要的数值。但是对于向量而言经过了这样的封装以后就安全了，因为无论是此前此后我们在访问某一个具体的元素的时候，在内部都是通过_elem这个统一的指示器来标识空间的起点。从这一点也可以看出进行封装以后的一个好处。 那么为什么要采用一个容量加倍的策略呢？采用其他策略，比如适当增加背部数组的容量，是否也可行呢？ 2.2.1.容量递增策略实际上情况并不那么简单，我们不妨以其中的一种典型的策略，即容量递增策略，来做一个对比。就是每当发现当前的内部数组即将发生上溢我们并不是对它进行容量的加倍，而只是在原来的容量的基础上追加一个固定的数额，这样看起来并没有什么问题。在代码上只需将原来的_capacity*2变成_capacity追加一个固定的数额，记为INCREMENT，简记作$I$。下面来考虑这个策略的效率。 在即将上溢之前，追加固定大小的容量 12T* oldElem = _elem;_elem = new T[_capacity += INCREMENT]; 最坏情况：在初始容量0的空向量中，连续插入$n = m * I$个元素（远大于2） 于是，在第$1, I+1, 2I+1, 3I+1,……$次插入时都需要扩容 即便不计申请空间操作，各次过程中复制原向量的时间成本依次为：$0,I,2I,\\dots,(m-1)I$（算术级数） 总体耗时 = $I\\times(m-1)\\times m/2=O(n^2)$，每次扩容的分摊成本为$O(n)$。 2.2.2.容量加倍策略 在即将上溢之前，使容量加倍 12T* oldElem = _elem;_elem = new T[_capacity &lt;&lt;= 1]; //容量加倍 最坏情况：在初始容量1的的满向量中，连续插入$n=2^m$个元素 于是，在第$1，2,4,8,16,32，\\dots$次插入时都需要扩容 各次扩容过程中复制原向量的时间成本依次为：$1,2,4,8,\\dots,2^m$ （几何级数） 总耗时 = $O(n)$，每次扩容的分摊成本为$O(1)$。 造成两种方法每次扩容分摊成本的时间复杂度出现很大差别的原因，可以用下图说明。实际上在向量规模不断递增达到某一固定的数值之前，如果采用的是递增式的增容策略，那么所需增容的操作必然是按当时的规模呈算数级数的形式分布。反过来如果是以倍增式的策略来进行的扩容，那么只需要进行其中的少数几次扩容就够了，具体来说就是这些以紫色标明的，可以看到要远远小于原先的数目，而且随着数组规模的增加，这种差异会更加的明显。 我们不妨将这两种策略所对应的性能列成如上面的一张表。在时间方面，在达到一个固定的规模n之前，累计所用的扩容时间：递增策略要多达$O(n^2)$，而倍增策略只需要$O(n)$，如果从分摊的意义上讲分摊到每一次扩容所需要的时间：前者是$O(n)$， 而后者是$O(1)$。可以看到就时间而言，容量加倍策略具有巨大的优势。而在空间方面，前一种策略似乎要非常好，因为它总是每次增加一个固定的数额，所以随着向量规模的增加，整个空间的利用率会越来越接近于百分之百。而加倍策略未必能做到百分之百，但是它至少有个底线，至少是50%，只有在它即将发生上溢，而因此刚刚通过加倍扩容的那个瞬间时才会是50%。所以相对而言，可以理解为倍增策略是通过在空间的效率上做了一个适当的牺牲，来换取在时间方面的巨大的收益，显然收益要远远大于损失。 2.3.平均分析 vs. 分摊分析平均复杂度或期望复杂度（average/expected complexity） 根据数据结构各种操作出现概率的分布，将对应的成本加权平均。 各种可能得操作，作为独立事件分别考查； 割裂了操作之间的相关性和连贯性； 往往不能准确地评判数据结构和算法的真实性能。 分摊复杂度（amortized complexity） 对数据结构连续地实施足够多次操作，所需总体成本分摊至单次操作。 从实际可行的角度，对一系列操作做整体的考量； 更加忠实地刻画了可能出现的操作序列； 可以更为精确地评判数据结构和算法的真实性能 3.无序向量回顾前两节，我们以向量为例给出了数据结构定义的一种通用方法，即模板，大致格式如下： 1template &lt;typename T&gt; Vector &#123; ...... &#125;; 这种方法实际上定义了 一系列的Vector，在使用的时候可以灵活指定它的类型。如果尖括号里是int的，那这个Vector实际上是a Vector of integers，即由一系列的整数组成的向量。更重要的是 在以后我们将利用这种方式来构造更为复杂的数据结构，比如可以把某些数据结构作为基本的组成元素来构成向量，举个例子在后面的学习中会定义二叉树Binary Tree这样一种数据结构，如果把BinTree作为基本的元素来构成Vector，那我们就可以构成一个由一系列的二叉树构成的一个线性序列，也就是A Vector of Binary Trees，取个形象的名字可以叫它forest 森林。在后面介绍霍夫曼编码的时候也会用到这种技巧，通过采用统一的模板式的方法，可以使得数据结构的定义非常的规范，而且更重要的是它们可以互相的融合组合，便捷地搭建更为复杂的数据结构。 12345Vector&lt;int&gt; myVector1;Vector&lt;float&gt; myVector2;Vector&lt;char&gt; myVector3;Vector&lt;BinTree&gt; forest; 这一节我们将围绕向量的最基本的形式，即无序向量来展开。无序向量不一定是说其中的元素没有顺序，甚至有时候其中的元素是根本就不可能排成顺序。在这样的一个前提下我们将研究如何来定义并且实现相应的操作接口。 3.1.循秩访问通过V.get(r)和V.put(r, e)接口，固然可以读，写向量元素，但便捷性远不如数组元素的下标式访问方式A[r]。通过重载下标操作符“ [ ] “，便可沿用数组的下标方式访问向量元素。对于任何一个指定的Rank r，只需在内部数据区中取出对应的第r号元素，此后凡是需要引用向量中的某个特定的比如说Rank为r的这个元素，就可以直接以这样一种类似于数组下标的形式进行引用。 123template&lt;typename T&gt;T&amp; Vector&lt;T&gt;::operator[](Rank r) const //0 &lt;= r &lt; _size &#123; return _elem[r]; &#125; 此后，对外的V[r]即对应于与内部的V._elem[r]。这种引用可以作为右值，以这种类似数组形式进行运算并且将运算的结果，向左侧赋值给某一变量；而反过来计算的结果也可以赋值给向量中某一个元素，也就是作为左值，因为这个接口返回值是一个引用。 右值：T x = V[r] + U[s] * W[t] 左值：V[r] = （T) (2*x + 3) 需要注意的是这里我们对入口参数r并没有做过多的检查，而是简易地在入口处增设了一个断言，用以提醒使用者保证入口参数r能够在合理的范围之内，但在真正的实际应用中，要做更为严格的处理。 3.2.插入 向量的插入算法具体来说就是如何将某一个特定的元素插入到向量的特定位置，在原来向量中因为所有的元素都必须是紧邻排列的，所以为了能够插入新的元素我们需要做一个调整，也就是将对应这个位置之后的所有的那些元素，称作它的后继，整体的构成一个后缀，进行一个整体的右移操作。这个right shift操作效果就是所有的后缀元素都向右移动一个单元，从而空出一个单，此时才可以将指定的那个元素纳入其中，从而完成插入。 整个算法可以描述并且实现如下的C++代码： 123456789template&lt;typename T&gt; //e作为秩为r的元素插入，0 &lt;= r &lt;= _sizeRank Vector&lt;T&gt;::insert(Rank r, T const&amp; e) &#123; expand(); //若有必要，扩容 for (int i = _size; i &gt; r; i--) //自后向前 _elem[i] = _elem[i - 1]; //后继元素顺次后移一个单元 _elem[r] = e; //置入新元素 _size++; //更新容量 return r; //返回秩&#125; 右移操作可以通过for循环完成，每个元素确实都是后移一位，当所有的后移完成之后，再将新的那个元素纳入到rank所指的位置上，当然同时还要更新整个向量的规模。 有两个需要注意的地方：第一，在for循环的方向是从最后一直向前不断地递减，也就是说整个的移动的方向虽然是向右，但是所有元素移动的先后次序却是后优先的，用图来表示也就是最后这个元素先移动，接下来是次后这个元素，再往前一直直到最前面的那个元素。这是必要的，如果把这个次序颠倒过来会有危险，会出现数据在无意中被覆盖的问题。 第二个主要注意的是expand()，即扩容操作，这是有必要的。因为确实在某些时候这个向量可能已经是满载的，所以为了插入新元素，在后移的过程中必然会出现上溢的情况，在这种时候就需要对向量进行扩容处理，比如上节的容量加倍策略，这样一件事情完全由expand()完成。 3.3.删除3.3.1.区间删除我们先考虑一个通用的一个版本，即区间删除，具体来说就是在某个向量中，我们要将介于lo和hi之间的一系列的元素成批地从中剔除掉。因为向量要求所有的元素始终都是彼此紧邻排列的，所以不应该在删除之后留下这个缝隙，换而言之，我们需要将它后继的那些元素（如果有的话）统一地向前或者说向左移动来填补这段空白。其实可以反过来看到如果能够完成这样的一个左移的话，那么实际上也就相当于把这些元素给剔除或者叫覆盖掉了，所以关键的任务在于如何实现这个左移。 这样的一个过程可以实现为下面代码： 12345678template&lt;typename T&gt; //删除区间[lo, hi)，0&lt;=lo&lt;=hi&lt;=_sizeint Vector&lt;T&gt;::remove(Rank lo, Rank hi) &#123; //O(n-hi) if (lo == hi) return 0; //处于效率考虑，单独处理退化情况 while (hi &lt; _size) _elem[lo++] = _elem[hi++]; //[hi, _size)顺次前移hi-lo个单元 _size = lo; shrink(); //更新闺蜜，若有必要则缩容 return hi - lo; //返回被删除元素的数目&#125; 代码中最关键的是while循环，它会遍历整个后缀，并且将其中的每一个元素逐一地取出，向前转移到合适的位置。比如第一个转移的是hi这个位置上的这个元素，它将被转移到lo这个位置，紧接着是hi+1转移到lo+1，hi+2转移到lo+2，直到最后。 同样有两个问题需要强调说明：第一个问题，在整个移动的过程中，所有这些元素参与移动的先后次序，同样也是很敏感的，或者说不能更改的，与插入算法完全颠倒，插入算法是自后向前，而区间删除算法是越往前的元素越优先参与移动，所以我们也可以认为它是一个自前向后的前移操作。如果把这个次序颠倒过来是有风险的，比如两者，即前缀的原来的那个位置和后来的那个位置中间有相互重叠的部分，如果优先移动后面的那个元素，那么就有可能会造成重叠区间的元素在无意中被覆盖掉。 第二点是shrink()这个历程的调用，它是某种意义上讲的缩容，这种操作在实际应用中并不是必须的，我们往往可以忽略它。 3.3.2.单元素删除上一小节中实现了区间的批量删除的接口，所以我们不妨把单元素的删除视作是整个区间操作的特例。具体来说，就是要将任何一个由单个元素构成的区间视作是由 r 到 r+1所定义的左闭右开的那段区间。这样就可以很简明地调用用此前重载的那个remove接口，只不过这里的参数改变为 r 和 r+1，与我们刚才的那种转换相对应。同理算法所进行的操作就是所有的后缀向前移动一个单位。 123456template&lt;typename T&gt; //删除向量中秩为r的元素，0 &lt;=r &lt; _sizeT Vector&lt;T&gt;::remove(Rank r) &#123; //O(n - r) T e = _elem[r]; //备份被删除的元素 remove(r, r + 1); //调用区间删除算法 return e; //返回被删除的元素&#125; 那么反过来，基于remove(r)接口，通过反复的调用，实现remove(lo, hi)是否可行呢。理论上是可行的，对于一个特定的一段从 lo 到 hi的区间，我们可以对其中的每一个元素分别去调用一次单元素删除接口，从而完成整体的删除操作。但是正如我们一直强调的，数据结构更多关注的是效率，而从效率上看这样做是非常差的。 首先考虑单元素删除本身的效率，最重要的实际上是这段区间也就是被删除元素的那些后继们，统一地要向前移动一次，这也是它的复杂度的来源。因此它的时间复杂度是取决于它的后继的个数，即为n-hi，最坏情况下是$O(n)$。如果按这种方式反复调用，有可能会导致$O(n^2)$的复杂度，在效率上是不能接受的。 3.4.查找查找即是按照某种特定的条件，从向量中找出特定的元素。首先我们要明确两个概念：判等与比较，对于任何的两个元素，我们来判断它们是否是相等，或者是比较它们之间谁大谁小，这两个操作并不是所有的类型都天然支持的。所以这里我们做一个假设：向量中元素的类型是基本类型，或者向量元素这个类已经重载了对应的判等的操作符或者是比较的操作符。无序向量可以一般性地认为它只支持判等操作，而对于有序向量，要求要更高一点，它还需要支持其中的元素能够相互比较大小。 无序向量：T为可判等的基本类型，或已重载操作符=或!= 有序向量：T为可比较的基本类型，或已重载操作符&lt;或&gt; 无序向量的查找过程可以描述为下图，如果查找的区间范围是 lo 到 hi 的话，就从 hi 出发逆向地、逐一地取出向量中的各个元素与目标元素进行比对，如果不相等就忽略它，进而考察它的前驱，所以整个的工作会亦步亦趋地逐个地遍历向量中的所有的元素。 经过这样一个逆向地扫描的过程，我们很有可能在中间的某一步找到所需要的那个目标，即查找成功；如果一直持续到最后，在试图越过lo也就是合法的最左侧的边界的时候，就可以断定整个查找是失败的。这个算法可以通过下面的代码实现： 1234567template&lt;typename T&gt; // 0 &lt;= lo &lt; hi &lt;= _sizeRank Vector&lt;T&gt;::find(T const &amp;e, Rank lo, Rank hi) cosnt&#123; //O(hi - lo) = O(n)，在命中多个元素时可返回秩最大者 while ((lo &lt; hi--) &amp;&amp; (e != _elem[hi])); //逆向查找 return hi; // hi &lt; lo 意味着失败，否则hi即命中元素的秩&#125; // Excel::match(e, range, type) 需要注意的是，find函数返回的都是最终停止的那个位置，有可能是合法的一个位置。也可能是刚刚越过左边界的那个非法的位置。而具体判别是否成功可以交给上层的调用者，因为他通过这个秩是否是合法就可以判断查找是否成功，如果是成功的话这样一个秩将可以被高层的算法进一步地利用。 我们也可以看出这个算法的复杂度有很大的变化空间，在最好的情况下，可能在第一个元素位置上就顺利地命中所以这时复杂度是常数$O(1)$；但是在最坏的情况下，比如一直持续到比较后才发现这个元素，甚至一直持续到最终也没有发现我们的目标元素，为此在这个过程中我们需要扫描的元素可能会与向量的规模相当，复杂度就会是$O(n)$。 这样一种在最好和最坏情况下相差极其悬殊的算法，叫作输入敏感算法（input-sensitive），即它的复杂度具体是多少与输入时候数据的配置紧密相关。 输入敏感（input-sensitive）：最好$O(1)$，最差$O(n)$。（对本例而言） 3.5.唯一化问题无序向量的唯一化问题，即是把其中重复的元素都剔除掉，使得每一组重复的元素只保留一个拷贝。在很多实际的应用中都能够找到唯一化的影子，比如在网络搜索的环境中有很多个不同的结点所分工完成的局部的搜索结果，可能会含有大量的重复的元素，我们需要将其中重复的元素剔除掉，从而得到一份记忆完整同时又不冗余的搜索报告。这样一个算法大致可以通过这样的一个图示来表示它的原理： 对于一个向量，我们总是把它分为三个部分，以当前的这个元素为界，当前这个元素自己是一部分，它的前驱所构成的前缀是一部分，以及对称地，所有的后继是一部分。每一次我们遇到一个新的元素，都在它的前缀中去进行查找，这可以通过find操作来完成的，如果能够找到雷同的元素，比如在某个位置上出现了一个x，就可以把这个元素剔除掉。反之，经过查找以后，如果这个元素没有出现，那么我们就可以把它保留下来，同时再去考察它的下一个元素。这个算法可以由下面的代码实现： 12345678910template&lt;typename T&gt; //删除重复元素，返回被删除元素数目int Vector&lt;T&gt;::deduplicate() &#123; int oldSize = _size; //记录原规模 Rank i = 1; //从_elem[1]开始 while (i &lt; _size) //自前向后逐一考查各元素_elem[i] (find(_elem[i], 0, i) &lt; 0) ? //在前缀中寻找雷同者 i++ //若无雷同者则继续考查其后继 : remove(i); //否则删除雷同者（可以是多个） return oldSize - _size; //返回向量规模变化量，即删除元素总数&#125; 3.5.1.正确性那么我们如何给出这个算法正确性的严格证明呢？同样根据第一章学到的知识，我们通过挖掘算法所具有的不变性和单调性，来证明一个算法最终的正确性。 首先来证明不变性，我们发现在这个算法运行的任何一个时刻，如果当前所对应的是第i个元素V[i]的话，那么在它所对应的那个前缀中所有的元素必然是彼此互异，即不包含重复元素。当算法开始时i=1，它的前缀只有V[0]。 其余的一般情况下可以用数学归纳法来予以证明：假设当时的状态是第i个元素e，它的前缀是从0到i的区间。按照数学归纳法我们假设在此前不变性是成立的话，那么接下来，无非两种情况，即当前的这次对应的查找成功或者失败。 如果是失败，即在它的前缀中不含元素e，算法给出的处理方法是直接令i++，也就是我们已经指向了它的下一个元素，而将刚才那个元素e归入了新的这个前缀中。既然e和此前的那些前缀是互不重复的，所以将e归入这样的一个区间以后，这个区间必然是不含重复元素的。 反之如果如果查找成功，e出现在它的前缀中，按照算法流程会将它剔除掉，也就是通过删除操作使得后继的元素整体地向前移动，从而使得原先它的直接后继变为当前的这个元素，并且算法继续地运转下去。经过了这样一次迭代之后当前的这个元素虽然换了，但是它的前缀并没有换，这个前缀所具有的元素互异的性质也依然会保持下来。 算法运行到最终是覆盖整个向量，到那时我们所说的当前的元素其实就是最末尾的那个哨兵元素，而它的前缀其实就是整个向量，那么它的前缀中不包含重复的元素其实也就相当于整体的向量中不包含重复的元素，这正是我们这个算法的功能唯一化所要求的，所以在最终这个不变性必然会转化为我们所需要的正确性 接着我们证明单调性，这个算法的主体是由一个while循环构成的，随着反复的while迭代: 当前元素前缀的长度单调非降，且迟早增至_size 当前元素后缀的长度严格单调下降，且迟早减至0 所以算法待处理元素的个数会严格单调减少，算法必然终止，且至多迭代$O(n)$轮。 3.5.2.复杂度这个算法的主体是while循环，而在while循环中真正能够造成有效复杂度的是find操作和remove操作，其中find操作是对于当前的元素的整个前缀而言的，而remove操作恰好对称是相对于当前这个元素的后继而言的。所以每一次while循环所需要的成本也就是find和remove两类操作的成本，累计起来也不会超过整个向量的长度，即$O(n)$线性步。而while循环最多会迭代$O(n)$轮，所以这个算法累计起来最多不超过$O(n^2)$的时间复杂度，这也是最坏情况。 这个算法也可以进一步的优化。 3.6.遍历遍历就是按照某种事先约定的操作（称之为visit），对向量中的每一个元素逐一地、统一地执行一次。所以这里涉及到两个问题：第一，如何来指定或者来描述这样一个visit操作；第二，如何将它传递到向量内部的每一个具体的元素。 通常有两种方法：第一种是使用函数指针，也就是说可以对于vector这样一个类定义一个traverse接口，作为它的参数visit本身就是一个函数的指针。所以为了兑现这样的一个遍历操作我们只需要逐一地取出向量中由这个i确定的每一个元素通过这个函数指针找到这个函数，并且对这个元素实施这个函数所指定的操作。 12345template&lt;typename T&gt;void Vector&lt;T&gt;::traverse(void(*visit)(T&amp;))&#123; //函数指针 for (int i = 0; i &lt; _size; i++) vist(_elem[i]);&#125; 第二种方式是使用函数对象，也就是说我们指定的这个参数visit，本身就是一个对象，它的作用就是用来模拟一个操作一个函数的一个行为方式。所以同样地，我们也可以对这个向量中的每一个元素都逐一地取出，并且转交给这样一个函数对象，通过它来实施具体地、统一地操作。 12345template&lt;typename T&gt; template&lt;typename VST&gt;void Vector&lt;T&gt;::traverse(VST&amp; visit) &#123; //函数对象 for (int i = 0; i &lt; _size; i++) vist(_elem[i]);&#125; 这两种方法其实是非常接近，但是也有一些重要的区别，相对而言，后一种方式的通用性更强。 下面通过一个实例来了解如何通过函数对象，实现刚才所说的具体地遍历。比如说，我们可以考虑将向量中的所有的元素统一地各自+1。为此我们只需要实现一个对应功能的函数对象，它本身也是以一个类的形式给出来的。这里为了简化起见使用了struct，而没有进行过多的封装。这个对象最重要的一个作用或者说唯一的作用就是重载了它的圆括号操作符()，从而使得它在行为上与一个函数非常的类似，而具体的功能就是把每一个参数e做一个+1操作。 1234template&lt;typename T&gt; //假设T可直接递增或已重载操作符“++”struct Inciease &#123; //函数对象：通过重载操作符\"()\"实现 virtual void operator()(T &amp; e) &#123; e++; &#125; //加一&#125;; 在实现了这样一个对应的类之后，就可以通过调用vector统一遍历接口traverse，将我们刚刚编写的这个函数对象以参数的形式传入就可以实现相应的功能，也就是把向量中的每一个元素统一地加一。 1234template&lt;typename T&gt;void increase(Vector&lt;T&gt; &amp; V) &#123; V.traverse(Increase&lt;T&gt;()); &#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"数据结构与算法（1）","slug":"数据结构与算法（1）","date":"2020-02-05T02:32:06.000Z","updated":"2020-02-07T02:32:06.000Z","comments":true,"path":"2020/02/05/数据结构与算法（1）/","link":"","permalink":"http://nekomoon404.github.io/2020/02/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%881%EF%BC%89/","excerpt":"","text":"计算对象：规律，技巧 目标：高效，低耗 Computer science should be called called computing science, for the same reason why surgery is not called knife science. -E.Dijkstra 计算 = 信息处理 ​ 借助某种工具，遵照一定规则，以明确而机械的形式进行 计算模型 = 计算机 = 信息处理工具 算法所谓算法，即特定计算模型下，旨在解决特定问题的指令序列 ​ 输入：待处理的信息（问题） ​ 输出：待处理的信息（答案） ​ 正确性：的确可以解决指定的问题 ​ 确定性：任一算法都可以描述为一个由基本操作组成的序列 ​ 可行性：每一基本操作都可以实现，且在常数时间内完成 ​ 有穷性：任一算法在执行有限次基本操作之后终止并给出输出 列子：Hailstone序列 \\begin{align*}& 序列Hailstone(n)=\\begin{cases}\\{1\\} &n\\le1\\\\\\{n\\}\\cup Hailstone(n/2) &n为偶数\\\\\\{n\\}\\cup Hailstone(3n+1) &n为奇数\\end{cases}\\\\\\\\&Hailstone(42)=\\{ 42,21,64,32,\\dots,1\\}\\end{align*}12345int hailstone(int n)&#123;//计算序列Hailstone(n)的长度 int length = 1; //从1开始递推 while (n&lt;1) &#123; (n % 2) ? n = 3 * n + 1 : n / = 2; lenth++; &#125; return length; //返回hailstone(n)&#125; 问题：对于任意的n，总有|Hailstone(n)| &lt; ∞ ？ 目前还不能证明 程序不一定是算法 好算法正确：符合语法，能够编译，链接 ​ 能够正确处理简单的，大规模的，一般性的，退化的，任意合法的输入 健壮：能辨别不合法的输入并做适当处理，而不致非正常退出 可读性：结构化 + 准确命名 + 注释 + … 效率：速度尽可能快 ；存储空间尽可能少 （最重要的） ​ Algorithms + Data Structures = Programs -N. Wirth, 1976 ​ (Algorithms + Data Structures) × Efficiency = Computation 计算模型好的数据结构和算法才能有高效的计算，从而有好的应用。 算法分析两个主要方面： ​ 正确性：算法功能与问题要求一致？ ​ 数学证明？并不简单 ​ 成本： 运行时间+所需存储空间 ​ 如何度量？如何比较？ 考察：$T_A(P)$ = 算法A求解问题实例P的计算成本。 ​ 意义不大，因为可能出现的问题实例太多。那么如何归纳概括？ 观察：问题实例的规模，往往是决定计算成本的主要因素。 特定算法 + 不同实例 令$T_A(n)$ = 用算法A求解某一问题规模为n的实例，所需的计算成本。 ​ 讨论特定算法A（及其对应的问题）时，简记作$T(n)$。 然而这一定义仍有问题，同一问题等规模的不同实例，计算成本不尽相同，甚至有实质差别。 稳妥起见，取$T(n) = max\\{ T(P)| |P| = n \\}$，亦即，在规模同为n 的所有实例中，只关注最坏（成本最高）的实例。 特定问题 + 不同算法 同一问题通常有多种算法，如何评判其优劣？ 实验统计是最直接的方法，但足以准确反映算法的真正效率？ 但实验统计还是不足够的，还要考虑： 不同的算法，可能更适应于不同规模的输入 不同的算法，可能更适应 与不同类型的输入 同一算法，可能由不同程序员、用不同程序语言、经不同编译器实现 同一算法，可能实现并运行与不同的体系结构、操作系统 为给出客观的评判，需要抽象出一个理想的平台或模型 不再依赖于上述种种具体的因素 从而直接而准确地描述，测量并评价算法 图灵机 Turing Machine Tape：依次均匀地划分为单元格，各注有某一字符，默认为’#’ Alphabet：字符的种类有限 Head：总是对准某一单元格，并可读取和改写其中的字符；每经过一个节拍，可转向左侧或右侧的邻格 State：TM总是处于有限种状态中的某一种，每经过一个节拍，可（按照规则）转向另一种状态 Transition Function ：（q, c; d, L/R, P) ​ 若当前状态为q且当前字符为c，则将当前字符改写为d；转向左侧/右侧的邻格； ​ 转入p状态，一旦转入特定的状 态’h’，则停机。 RAM: Random Access Machine 与TM模型一样，RAM模型也是一般计算工具的简化与抽象，使我们可以独立于具体的平台，对算法的效率做出可信的比较与评判。 在这些模型中： 算法的运行时间 转化为 算法需要执行的基本操作次数 $T(n)$ = 算法为求解规模为n的问题，所需执行的基本操作次数 大$O$记号渐进分析 回到原先的问题：随着问题规模的增长，计算成本如何增长？ ​ 注意：这里更关心足够大的问题，注重考察成本的增长趋势 渐进分析：在问题规模足够大后，计算成本如何增长？ ​ Asymptotic analysis：当n&gt;&gt;2后，对于规模为n输入，算法 ​ 需执行的基本操作次数：T(n) = ? ​ 需占用的存储单元数：S(n) = ? //通常可不考虑 大$O$记号（big-$O$ notation） \\begin{align*} &T(n)=O(\\,f(n)\\,)\\quad if \\quad \\exists c>0, 当n>>2后，有T(n)","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://nekomoon404.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"Cpp基础（7）类和对象","slug":"Cpp基础（7）类和对象","date":"2020-02-04T08:09:38.000Z","updated":"2020-02-14T08:09:38.000Z","comments":true,"path":"2020/02/04/Cpp基础（7）类和对象/","link":"","permalink":"http://nekomoon404.github.io/2020/02/04/Cpp%E5%9F%BA%E7%A1%80%EF%BC%887%EF%BC%89%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"面向对象程序设计的基本特点抽象：对同一类对象的共同属性和行为进行概括，形成类。 首先注意问题的本质及描述，其次是实现过程或细节。 数据抽象：描述某类对象的属性或状态（对象相互区别的物理量）。 代码抽象：描述某类对象的共有的行为特征或具有的功能。 抽象的实现：类。 12345678class Clock&#123; public: void setTime(int newH, int newM. int newS); void showTiem(); private: int hour, minute, second;&#125; 封装：将抽象出的数据，代码封装在一起，形成类。 目的：增强安全性和简化编程，使用者不必了解具体的实现细节，而只需要通过外部接口，以特定的访问权限，来使用类的成员。 实现封装：类声明中的{ } 继承：在已有类的基础上，进行扩展形成新的类。 多态：同一名称，不同的功能实现方式。达到行为标识统一，减少程序中标识符的个数。 类和对象的定义对象是现实中的对象在程序中的模拟；类是同一类对象的抽象，对象是类的实例。定义类的对象，才可以通过对象使用类中定义的功能。 设计类就是设计类型，需要关注哪些问题： 此类型的“合法值”是什么？ 此类型应该有什么样的函数和操作符？ 新类型的对象该如何被创建和销毁？ 如何进行对象的初始化和赋值？ 对象作为函数的参数如何以值传递？ 谁将使用此类型的对象成员？ 类定义的语法形式123456789class 类名称&#123; public: 公有成员（外部接口） private: 私有成员 protected: 保护型成员&#125; 在定义类时也可以为数据成员设置类内初始值，用于初始化数据成员。 12345678class Clock&#123; public: void setTime(int newH, int newM. int newS); void showTiem(); private: int hour = 0, minute = 0, second = 0;&#125; 类成员的访问控制公有类型成员：在关键字public后面声明，它们是类与外部的接口，任何外部函数都可以访问公有类型数据和函数。 私有类型成员：在关键字private后面声明，只允许本类中的函数访问，而类外部的任何函数都不能访问。如果紧跟在类名称的后面声明私有成员，则关键字private可以省略。如果某个成员前面没有上述关键字，则缺省地被认为是私有成员。 保护类型成员：与private类似，其差别表现在继承与派生时对派生类的影响不同。 类中成员之间直接使用成员名互相访问。 从类外访问成员使用“ 对象名.成员”，来访问公有成员。 类的成员函数在类中声明函数原型： 可以直接在类中给出函数体，形成内联成员函数； 12345678910//定义一个矩形的类class Rectangle&#123; private: int w; int h; public: int getArea() &#123; return w*h; &#125; int getPerimeter() &#123; return 2*(w+h); &#125;&#125;; 也可以在类外给出函数体实现，并在函数名前用类名加以限定； 12345678910111213//定义一个矩形的类class Rectangle&#123; private: int w; int h; public: int getArea(); int getPerimeter();&#125;;int Rectangle::getArea() &#123; return w*h; &#125;int Rectangle::getPerimeter() &#123; return 2*(w+h); &#125; 允许声明重载函数和带默认参数值的函数。 例子：设计一个圆的类，该类的成员变量为圆心的x轴坐标，y轴坐标，半径长度；该类的成员变量对外都是不可见的；该类的成员函数为：设置圆心坐标，设置圆心半径，计算圆的面积，计算圆的周长。 123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;using namespace std;class Circle&#123;private: double x; double y; double r;public: void setM(double _x, double _y) &#123; x = _x; y = _y; &#125; void setR(double _r) &#123; r = _r; &#125; double getArea() &#123; return 3.14 * r * r; &#125; double getPerimeter() &#123; return 2 * 3.14 *r; &#125;&#125;;int main()&#123; Circle myCircle; myCircle.setM(1.7, 3.2); myCircle.setR(4.2); double myArea = myCircle.getArea(); double myPeri = myCircle.getPerimeter(); cout &lt;&lt; \"Area = \" &lt;&lt; myArea &lt;&lt; \", Perimeter = \" &lt;&lt; myPeri &lt;&lt; endl; myCircle.setR(8.4); myArea = myCircle.getArea(); myPeri = myCircle.getPerimeter(); cout &lt;&lt; \"Area = \" &lt;&lt; myArea &lt;&lt; \", Perimeter = \" &lt;&lt; myPeri &lt;&lt; endl;&#125; 构造函数当我们定义对象时，如何对对象进行初始化？在定义基本类型的变量时，是可以直接给定初始值的，但是在定义对象时却不是这么简单，因为一个类是我们自己定义的，对类的对象按照什么规则进行初始化，编译器是不会自动知道的，必须由程序员写程序来规定。为此C++中提供了一种特殊的机制：构造函数，在构造函数中我们可以描述如何对类的对象进行初始化。 基础知识构造函数的作用 在对象被创建时使用特点的值构造对象，将对象初始化为一个特定的初始状态 例如：希望在构造一个Clock类对象时，将初始时间设为0:0:0，就可以通过构造函数来设置 构造函数的形式 函数名与类名相同； 不能定义返回值类型，也不能在函数体有return语句； 可以有形式参数，也可以没有形式参数； 可以是内联函数； 可以是重载； 可以带默认参数值。 构造函数的调用时机 在对象创建时被自动调用，但如果没有定义构造函数就进行初始化，那么编译器就会报错。 1Clockk myClock(0,0,0); 默认构造函数调用时可以不需要实参的构造函数： 参数表为空的构造函数 全部参数都有默认值的构造函数 下面两个都是默认构造函数，如在类中同时出现，将产生编译错误，不是合法的函数重载形式 12Clock();Clock(int newH=0, int newM=0, int newS=0) 隐含生成的构造函数如果在程序中未定义构造函数，编译器将在需要时自动生成一个默认的构造函数： 参数列表为空，不为数据成员设置初始值； 如果类内定义了成员的初始值，则使用内类定义的初始值； 如果没有定义类内的初始值，则以默认方式初始化； 基本类型的数据默认初始化的值是不确定的。 如果定义的类的成员不是基本类型的成员，而是其他类的对象，这个就是类组合的情况，其默认的初始化方式由它所属的类决定。 =default如果程序中已定义构造函数，默认情况下编译器就不会再隐含生成默认构造函数。如果此时依然希望编译器隐含生成的默认构造函数，可以使用=default。 1234567class Clock&#123; public: Clock() = default; Clock(int newH, int newM, int newS); private: int hour, minute, second;&#125; 例子1 1234567891011121314151617181920#include&lt;iostream&gt;using namespace std;class Clock &#123;public: Clock(int newH, int newM, int newS); void setTime(int newH, int newM, int newS); void showTime();private: int hour, minute, second;&#125;;Clock::Clock(int newH, int newM, int newS) :hour(newH), minute(newM), second(newS) &#123;&#125; //初始化列表int main() &#123; Clock c(0, 0, 0); c.showTime(); return 0;&#125; 例子2 1234567891011121314151617class Clock &#123;public: Clock(int newH, int newM, int newS); //构造函数 Clock(); //默认构造函数，如果类要重复使用，一般要提供一个默认构造函数 void setTime(int newH, int newM, int newS); void showTime();private: int hour, minute, second;&#125;;Clock::Clock():hour(0),mintue(0),second(0)&#123;&#125; //默认构造函数Clock::Clock(int newH, int newM, int newS) :hour(newH), minute(newM), second(newS) &#123;&#125; int main()&#123; Clock c1(8,10,0); //调用有参数的构造函数 Clock c2; //调用无参数的默认构造函数&#125; 委托构造函数当我们在一个类中重载多个构造函数的时候，往往发现这些构造函数它们只是形参表不同，初始化列表不同，而其他都是一样的，初始化算法都是相同的，函数体都是相同的。那么在这种情况下，如果我们写多个函数体来重载，往往就显得重复了，为了避免这种重复，C++11新标准提供了一种新的机制：委托构造函数，也就是让一个构造函数可以去委托另一个构造函数去帮它完成初始化功能。 回顾Clock类的两个构造函数，第一个构造函数是有三个参数的，第二个构造函数是默认构造函数，没有参数。实际上，这两个构造函数进行初始化的方式是完全一样的，只不过第一个构造函数是用参数表里的参数进行初始化，第二个构造函数用默认状态全部用0进行初始化。那么我们其实没有必要写两个类似的重复的代码。 12Clock::Clock(int newH, int newM, int newS) :hour(newH), minute(newM), second(newS) &#123;&#125; Clock::Clock():hour(0),mintue(0),second(0)&#123;&#125; //默认构造函数 委托构造函数使用类的其他构造函数执行初始化过程，我们用委托构造函数的方法重写上面的代码，这里第二个构造函数调用了另外一个有参数的构造函数，将默认的三个初始化参数传给有参数表的Clock构造函数，这样就不用把同样的初始化方法再写一遍了。 12Clock::Clock(int newH, int newM, int newS) :hour(newH), minute(newM), second(newS) &#123;&#125;Clock::Clock():Clock(0,0,0)&#123;&#125; 用委托构造函数不仅可以减少重复的工作，其最大的好处是可以保持代码实现的一致性，如果想要修改构造函数的初始化算法时，就只需在一处修改，其他的委托这个构造函数来进行初始化的构造函数的算法也就同步修改了。 复制构造函数当我们在定义一个基本类型的变量时，经常会用一个已经存在的已经有值的变量去初始化这个变量；我们在定义对象时可会有这样的需求，即用一个存在的对象去初始化一个新的对象，这时要如何实现这种初始化呢？C++中提供了一种特殊的构造函数，叫复制构造函数。 在复制构造函数中我们可以规定如何用一个已经存在的对象去初始化一个新对象，可以用这个已经存在的对象的引用作为构造函数的参数。如果在定义类的时候没有定义复制构造函数，编译器也生成一个默认的复制构造函数，它会实现类的两个对象的数据成员之间一一对应复制，这些功能在很多时候已经能满足需求，那么我们就不需要再写复制构造函数了。 复制构造函数定义 复制构造函数是一种特殊的构造函数，其形参为本类的对象引用。作用是用一个已存在的对象去初始化同类型的新对象。 由于复制构造函数的目的不会是将原有的那个形参对象给修改了，所以最好是在形参引用前加上const关键字 123456789class 类名&#123; public: 类名(形参)； //构造函数 类名(const 类名 &amp;对象名); //复制构造函数 //......&#125;;类名::类(const 类名 &amp;对象名) //复制构造函数的实现&#123; 函数体 &#125; 复制构造函数的调用除了在定义新对象时，用已有的对象作为参数去初始化它这种情况以外，共有三种情况是典型的要调用复制构造函数的情况： 定义一个对象时，以本类另一个对象作为初始值，发生复制构造； 如果函数的**形参是类的对象**，调用函数时，将使用实参对象初始化形参对象，发生复制构造； 如果函数的返回值是类的对象，函数执行完成返回主调函数时，将使用return语句中的对象初始化一个临时无名对象，传递给主调函数，此时发生复制构造。这种情况也可以通过移动构造避免不必要的复制。 隐含的复制构造函数 如果程序员没有为类拷贝初始化构造函数，则编译器自己生成一个隐含的复制构造函数； 这个构造函数的功能是：用作为初始值的对象的每个数据成员的值，初始化将要建立的对象的对应数据成员。 如果类的成员中有指针的时候，很多情况下，默认的复制构造函数其浅层的复制功能就不够用了，这是我们就需定义深层的复制构造。 =delete如果我们不希望对象被复制构造，那么可以采用下面的方法: C++98做法：将复制构造函数声明为private，并且不提供函数的实现。 C++11做法：用=delete指示编译器不生成默认复制构造函数。 1234567class Point&#123; public: Point(int xx=0, int yy=0) &#123;x=xx; y=yy&#125; //构造函数，内联 Point(const Point &amp;p) = delete; //指示编译器不生成默认复制构造函数 private: int x, y;&#125; 析构函数当一个对象在存续期间会占用系统资源，当这个对象的生存期结束时，需要进行善后工作将其删除清理掉，C++中提供了这样一种机制：析构函数。当对象被构造时，构造函数会自动调用；当对象要消亡时，其析构函数也会自动调用。 完成对象被删除前的一些清理工作； 在对象的生存期结束的时刻系统自动调用它，然后再释放此对象所属的空间； 如果程序中未声明析构函数，编译器将自动产生一个默认的析构函数，其函数体为空； 析构函数的原型：~类名( ); 析构函数没有参数，没有返回类型 123456789101112class Point&#123; public: Point(int xx, int yy) //构造函数 ~Point(); //析构函数 private: int x, y;&#125;Point::Point(int xx, int yy)&#123; x=xx; y=yy;&#125;Point::~Point()&#123;&#125; 类的组合在制造业多年来都一直使用部件组装的生产方式，与一切手工从头做起相比，部件组装的生产效率肯定是要高，产品的标准化 它的可靠性也都更好。在程序中我们也可以借用这种部件组装的思想，用已经存在的这些类去组装新的类，C++语言支持类的组合。我们在定义一个新类的时候，可以让它的类成员是已有类的对象，也就是说一些类的对象可以作为另外一个类的部件，这就是类的组合。 类组合的基本概念： 类中的成员是另外其他类的对象； 可以在已有抽象的基础上实现更复杂的抽象。 成员对象：一个类的成员变量是另一个类的对象 包含成员对象的类叫封闭类（Enclosing） 类组合的构造函数设计那么组合类的构造函数如何设计呢？每个类的构造函数都是负责自己本类成员初始化的，如果用另外类的对象作为新定义类的成员，那么这个组合类是没有权利去访问部件对象内部的私有成员。因为一个类的私有成员只有这个类内部的函数可以访问，类外任何地方是不可以访问的，而且部件类的设计者、开发者，跟组合类的设计者 开发者可能不是一个人，甚至不是一个团队。因此在写组合类的构造函数时要考虑，由组合类的构造函数负责将部件对象初始化所需要的初始化参数传递给它，然后编译器会自动去调用部件类的构造函数，来初始化这些部件对象。其语法形式如下： 原则：不仅要负责对本类中的基本类型成员数据初始化，也要对对象成员初始化。 声明形式：初始化列表 12345类名::类名(对象成员所需的形参，本类成员形参): 对象1(参数), 对象2(参数),...... &#123; //函数体其他语句 &#125; 构造组合类对象时的初始化次序 首先对构造函数初始化列表中列出的成员（包括基本类型成员和对象成员）进行初始化，初始化次序是成员在类体中定义的次序。 成员对象构造函数调用顺序：按对象成员的定义顺序，先声明者先构造 初始化列表中未出现的成员对象，调用默认构造函数（即无形餐的）初始化 处理完初始化化列表之后，再执行构造函数的函数体 也可以这样理解，构造函数和析构函数的调用顺序： 当封闭类对象生成时： S1：执行所有成员对象的构造函数 S2：执行封闭类的构造函数 成员对象的构造函数调用顺序 和成员对象在类中的说明顺序一致 与在成员初始化列表中出现的顺序无关 当封闭类的对象消亡时 S1：先执行封闭类的析构函数 S2：再执行成员对象的析构函数 析构函数顺序和构造函数的调用顺序相反（先构造的后析构，后构造的先析构） 需要注意的是，我们在写类的构造函数时，最好再写一个无参数的默认构造函数。当这个类的对象被用作其他类的部件成员时，可能组合类中没有写构造函数只使用默认构造函数，这个时候我们上面的操作就显得很必要了。 例子：构造一个Point类，再用Point类构造组合类Line类，通过构造函数和复制构造函数中的“调试信息”（cout&lt;&lt;……）可以更好地理解构造函数和复制构造函数的调用过程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include&lt;iostream&gt;#include&lt;cmath&gt;using namespace std;class Point &#123; //Point类的定义public: Point(int xx = 0, int yy = 0) &#123; x = xx; y = yy; &#125; Point(Point &amp;p); int getX() &#123; return x; &#125; int getY() &#123; return y; &#125;private: int x, y;&#125;;Point::Point(Point &amp;p) &#123; //复制构造函数的实现 x = p.x; y = p.y; cout &lt;&lt; \"Calling the copy constructor of Point\" &lt;&lt; endl;&#125;//类的组合class Line &#123; //Line类的定义public: //外部接口 Line(Point xp1, Point xp2); Line(Line &amp;l); double getLen() &#123; return len; &#125;private: //私有数据成员 Point p1, p2; //Point类的对象p1,p2 double len;&#125;;//组合类的构造函数Line::Line(Point xp1, Point xp2) :p1(xp1), p2(xp2) &#123; cout &lt;&lt; \"Calling constructor of Line\" &lt;&lt; endl; double x = static_cast&lt;double&gt;(p1.getX() - p2.getX()); double y = static_cast&lt;double&gt;(p1.getY() - p2.getY()); len = sqrt(x*x + y * y);&#125;//组合类的复制构造函数Line::Line(Line &amp;l) :p1(l.p1), p2(l.p2) &#123; cout &lt;&lt; \"Calling the copy constructor of Line\" &lt;&lt; endl; len = l.len;&#125;//主函数int main() &#123; Point myp1(1, 1), myp2(4, 5); Line line(myp1, myp2); Line line2(line); cout &lt;&lt; \"The length of the line is: \"; cout &lt;&lt; line.getLen() &lt;&lt; endl; cout &lt;&lt; \"The length of the line2 is: \"; cout &lt;&lt; line2.getLen() &lt;&lt; endl; return 0;&#125; 前向引用声明类应该先声明，后使用，如果需要在某个类的声明之前引用该类，则应进行前向引用声明。前向引用声明只为程序引入一个标识符，但具体声明在其他地方。前向引用声明某个类之后，可在之后的其他类的成员函数中将该类作为参数类型使用。 123456789class B; //前向引用声明class A&#123; public: void f(B b);&#125;;class B&#123; public: void g(A a);&#125; 需要注意的是： 使用前向引用声明虽然可以解决一些问题，但它并不是万能的。 在提供一个完整的类声明之前，不能声明该类的对象，也不能在内联成员函数中使用该类的对象。 当使用前向引用声明时，只能使用被声明的符号，而不能涉及类的任何细节。 1234567class Fred; //前向引用声明class Barney&#123; Fred x; //错误：类Fred的声明尚不完整，不能声明该类的对象&#125;;class Fred&#123; Barney y;&#125; 示例 声明一个CPU类，包含等级（rank）、频率（frequency）、电压（voltage）等属性，有两个公有成员函数run、stop。其中，rank为枚举类型CPU_Rank，声明为enum CPU_Rank {P1=1,P2,P3,P4,P5,P6,P7}，frequency为单位是MHz的整型数，voltage为浮点型的电压值。类似地声明一个RAM类。 声明一个简单的Computer类，有数据成员芯片（cpu）、内存（ram），有两个公有成员函数run、stop。cpu为CPU类的一个对象，ram为RAM类的一个对象。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140#include &lt;iostream&gt;using namespace std;enum CPU_Rank &#123; P1 = 1, P2, P3, P4, P5, P6, P7 &#125;;class CPU&#123;private: CPU_Rank rank; int frequency; float voltage;public: CPU(CPU_Rank r, int f, float v) //构造函数 &#123; rank = r; frequency = f; voltage = v; cout &lt;&lt; \"构造了一个CPU!\" &lt;&lt; endl; &#125; CPU(CPU &amp;c) //复制构造函数 &#123; rank = c.rank; frequency = c.frequency; voltage = c.voltage; cout &lt;&lt; \"复制构造了一个CPU！\" &lt;&lt; endl; &#125; ~CPU() &#123; cout &lt;&lt; \"析构了一个CPU!\" &lt;&lt; endl; &#125; //析构函数 CPU_Rank GetRank() const &#123; return rank; &#125; //外部接口 int GetFrequency() const &#123; return frequency; &#125; float GetVoltage() const &#123; return voltage; &#125; void SetRank(CPU_Rank r) &#123; rank = r; &#125; void SetFrequency(int f) &#123; frequency = f; &#125; void SetVoltage(float v) &#123; voltage = v; &#125; void Run() &#123; cout &lt;&lt; \"CPU开始运行!\" &lt;&lt; endl; &#125; void Stop() &#123; cout &lt;&lt; \"CPU停止运行!\" &lt;&lt; endl; &#125;&#125;;enum RAM_TYPE &#123; DDR2 = 2, DDR3, DDR4 &#125;;class RAM&#123;private: enum RAM_TYPE type; unsigned int frequency; //MHz unsigned int size; //GBpublic: RAM(RAM_TYPE t, unsigned int f, unsigned int s) //构造函数 &#123; type = t; frequency = f; size = s; cout &lt;&lt; \"构造了一个RAM！\" &lt;&lt; endl; &#125; RAM(RAM &amp;c) //复制构造函数 &#123; type = c.type; frequency = c.frequency; size = c.size; cout &lt;&lt; \"复制构造了一个RAM！\" &lt;&lt; endl; &#125; ~RAM() &#123; cout &lt;&lt; \"析构了一个RAM！\" &lt;&lt; endl; &#125; //析构函数 RAM_TYPE GetType() const &#123; return type; &#125; unsigned int GetFrequency() const &#123; return frequency; &#125; unsigned int GetSize() const &#123; return size; &#125; void SetType(RAM_TYPE t) &#123; type = t; &#125; void SetFrequency(unsigned int f) &#123; frequency = f; &#125; void SetSize(unsigned int s) &#123; size = s; &#125; void Run() &#123; cout &lt;&lt; \"RAM开始运行!\" &lt;&lt; endl; &#125; void Stop() &#123; cout &lt;&lt; \"RAM停止运行!\" &lt;&lt; endl; &#125;&#125;;//COMPUTER类class COMPUTER&#123;private: CPU my_cpu; RAM my_ram; unsigned int storage_size; //GB unsigned int bandwidth; //MBpublic: COMPUTER(CPU c, RAM r,unsigned int s, unsigned b); //构造函数 ~COMPUTER() &#123; cout &lt;&lt; \"析构了一个COMPUTER！\" &lt;&lt; endl; &#125; //析构函数 void Run() &#123; my_cpu.Run(); my_ram.Run(); cout &lt;&lt; \"COMPUTER开始运行!\" &lt;&lt; endl; &#125; void Stop() &#123; my_cpu.Stop(); my_ram.Stop(); cout &lt;&lt; \"COMPUTER停止运行!\" &lt;&lt; endl; &#125;&#125;;//COMPUTER类的构造函数，内嵌对象采用初始化列表初始化//一共会调用两次复制构造函数，形实结合调用依次，初始化列表调用依次//当COMPUTER构造函数结束以后，形实结合那个形参的生命周期就结束，于是执行析构函数COMPUTER::COMPUTER(CPU c, RAM r, unsigned int s, unsigned int b) :my_cpu(c), my_ram(r)&#123; storage_size = s; bandwidth = b; cout &lt;&lt; \"构造了一个COMPUTER！\" &lt;&lt; endl;&#125;int main()&#123; CPU a(P6, 300, 2.8); a.Run(); a.Stop(); cout &lt;&lt; \"***********************\\n\"; RAM b(DDR3, 1600, 8); b.Run(); b.Stop(); cout &lt;&lt; \"***********************\\n\"; COMPUTER my_computer(a, b, 128, 10); cout &lt;&lt; \"***********************\\n\"; my_computer.Run(); my_computer.Stop(); cout &lt;&lt; \"***********************\\n\"; //return之前会执行析构函数，先析构my_computer，和它的两个内嵌成员，然后析构CPU a和RAM b return 0;&#125; PS：结构体，联合体，枚举类的内容在上一篇文章Cpp基础（6）中。","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"Cpp基础（6）结构体与链表","slug":"Cpp基础（6）结构体与链表","date":"2020-02-02T09:03:07.000Z","updated":"2020-02-03T09:03:07.000Z","comments":true,"path":"2020/02/02/Cpp基础（6）结构体与链表/","link":"","permalink":"http://nekomoon404.github.io/2020/02/02/Cpp%E5%9F%BA%E7%A1%80%EF%BC%886%EF%BC%89%E7%BB%93%E6%9E%84%E4%BD%93%E4%B8%8E%E9%93%BE%E8%A1%A8/","excerpt":"","text":"结构体结构体是一种特殊形态的类，与类的唯一区别是：类的缺省访问权限是private，结构体的缺省访问权限是public。 那么什么时候用结构体而不用类：定义主要用来保存数据，没没有什么操作的类型。人们习惯将结构体的数据成员设为公有，这时使用结构体更方便。 结构体相当于构造了一个新的数据类型，用一组变量描述同一个“事物”。 123456789struct stduent&#123; int id; char name[20]; char sex; int age; float score; char addr[30];&#125;; //注意大括号后的\" ; \" 定义结构体变量的方式： 直接用已声明的结构体类型定义变量名 1student student1, student2; 在声明类型的同时定义变量 123456789struct stduent&#123; int id; char name[20]; char sex; int age; float score; char addr[30];&#125;student1,student2; 结构体数据类型的特性与普通数据类型的特性是一致的，可以赋值，做函数参数，有指向结构体的指针，结构体数组等等。 定义结构体类型的变量12345678910111213141516#include&lt;iostream&gt;using namespace std;struct student&#123; int id_num; char name[10];&#125;;int main()&#123; student mike = &#123; 123,\"mike\" &#125;; mike.id_num = 2123000 + mike.id_num; for (int i = 0; mike.name[i] != '\\0'; i++) mike.name[i] = toupper(mike.name[i]); cout &lt;&lt; mike.id_num &lt;&lt; \" \" &lt;&lt; mike.name &lt;&lt; endl; return 0;&#125; 结构体变量赋值12345678910111213141516171819#include&lt;iostream&gt;using namespace std;struct student&#123; int id_num; char name[10];&#125;;int main()&#123; student mike1 = &#123; 123,\"mike\" &#125;; student mike2; mike2 = mike1; mike2.id_num = 2123000 + mike2.id_num; for (int i = 0; mike2.name[i] != '\\0'; i++) mike2.name[i] = toupper(mike2.name[i]); cout &lt;&lt; mike1.id_num &lt;&lt; \" \" &lt;&lt; mike1.name &lt;&lt; endl; cout &lt;&lt; mike2.id_num &lt;&lt; \" \" &lt;&lt; mike2.name &lt;&lt; endl; return 0;&#125; 结构体做函数参数1234567891011121314151617181920#include&lt;iostream&gt;using namespace std;struct student&#123; int id_num; char name[10];&#125;;void renew(student one)&#123; one.id_num = 2123000 + one.id_num; for (int i = 0; one.name[i] != '\\0'; i++) one.name[i] = toupper(one.name[i]); cout &lt;&lt; one.id_num &lt;&lt; \" \" &lt;&lt; one.name &lt;&lt; endl;&#125;int main()&#123; student mike = &#123; 123,\"mike\" &#125;; renew(mike); return 0;&#125; 指向结构体的指针1234567891011121314#include&lt;iostream&gt;using namespace std;struct student&#123; int id_num; char name[10];&#125;;int main()&#123; student mike = &#123; 123,\"mike\" &#125;; student *one = &amp;mike; cout &lt;&lt; one-&gt;id_num &lt;&lt; \" \" &lt;&lt; one-&gt;name &lt;&lt; endl; return 0;&#125; 结构体数组123456789101112131415161718#include&lt;iostream&gt;using namespace std;struct student&#123; int id_num; char name[10];&#125;;int main()&#123; student myclass[3] = &#123; 123,\"mike\",133,\"tom\", 143,\"jack\"&#125;; student *one = myclass; cout &lt;&lt; one-&gt;id_num &lt;&lt; \" \" &lt;&lt; one-&gt;name &lt;&lt; endl; one++; cout &lt;&lt; one-&gt;id_num &lt;&lt; \" \" &lt;&lt; one-&gt;name &lt;&lt; endl; one++; cout &lt;&lt; one-&gt;id_num &lt;&lt; \" \" &lt;&lt; one-&gt;name &lt;&lt; endl; return 0;&#125; 枚举类型枚举：如果一个变量只有几种可能的取值，则可以将该变量定义为枚举类型。 枚举类型的定义 1234567//声明一个枚举数据类型weekdayenum weekday&#123;sun,mon,tue,wed,thu,fri,sat&#125;; //花括号内sun,mon,...,sat等称为枚举元素//定义枚举变量enum weekday workday,weekend;weekday workday,weekend//枚举变量赋值workday = sun; weekend = moon; 需要注意的是： 枚举类型按常量处理，不能对它们赋值。sun = mon; （错误） 枚举类型不能直接输出元素的名字。enum color{red,green,white,black}; color cloth = red; cout&lt;&lt;cloth; //结果为0。 枚举类型可以比较。if(cloth &gt; white) count++ 一个整型不能直接赋给一个枚举变量。workday = 2; （错误） 枚举元素有值： 定义时枚举元素如未指定值，编译系统按定义顺序取默认值依次为0,1,2,3,…. 也可以给枚举元素指定对应的值，enum day {sun=7,mon=1, tue, wed, thu, fri, sat}; 这时有sun=7, mon=1, tue=2, wed=3,...... 若要把整数赋给枚举变量应先进行强制类型转换，workday = (enum weekday) 2; 12345678910111213141516171819#include&lt;iostream&gt;using namespace std;enum color&#123;red,yellow, green=3,blue&#125;;enum color cl;int main()&#123; cl = blue; cout &lt;&lt; \"red=\" &lt;&lt; red &lt;&lt; \" yellow=\" &lt;&lt; yellow &lt;&lt; \" green=\" &lt;&lt; green &lt;&lt; endl; cout &lt;&lt; \"blue=\" &lt;&lt; blue &lt;&lt; \" cl=\" &lt;&lt; cl &lt;&lt; endl; //输出枚举类型的内容 switch (cl) &#123; case red: cout &lt;&lt; \"red\\n\"; break; case yellow: cout &lt;&lt; \"yellow\\n\"; break; case green: cout &lt;&lt; \"green\\n\"; break; case blue: cout &lt;&lt; \"blue\\n\"; break; &#125; return 0;&#125; 例子：计算工资 123456789101112131415161718192021222324#include&lt;iostream&gt;using namespace std;int main()&#123; enum day&#123;Mon,Tue,Wed,Thu,Fri,Sat,Sun&#125;; day workDay; double times, wages = 0, hourlyPay, hours; cout &lt;&lt; \"Enter the hourly wages rate.\" &lt;&lt; endl; cin &gt;&gt; hourlyPay; cout &lt;&lt; \"Enter hours worked daily\" &lt;&lt; endl; for (int i = 0; i &lt; 7; i++) &#123; cin &gt;&gt; hours; switch ((day)i) &#123; case Sat:times = 1.5*hours; break; case Sun:times = 2 * hours; break; default:times = hours; &#125; wages = wages + times * hourlyPay; &#125; cout &lt;&lt; \"The wages for the week are \" &lt;&lt; wages &lt;&lt; endl; return 0;&#125; 共用体共用体：为了节省内存空间，可以将几种不同类型的变量存放到同一段内存单元中，这段内存单元所对应的数据结构称为共用体。 共用体的定义：uniom 共用体名{ 成员列表; }变量列表; 12345678union data&#123; int i; char ch; float f;&#125;a,b,c; // 直接定义data a,b,c; //分开定义 共用体的引用：不能引用共用体变量，只能引用共用体变量中的成员。 共用体类型数据的特点： 同一内存段可以存放几种不同类型的成员，但在同一时刻时只能存放其中一种。 共用体变量中起作用的成员是最后一次存放的成员，在存入一个新的成员后原有的成员就失去作用。 共用体变量的地址和它的各成员的地址都是同一地址，如&amp;a, &amp;a.i, &amp;a.ch, &amp;a.f都是同一地址值。 共用体不能初始化，不能对整个共用体赋值。 在函数中，可以使用共用体的指针，但不能使用名字做函数参数。 共用体的空间是所有成员中最大的一个。 例子： 123456789101112struct&#123; int num; char name[10]; char sex; char job; union &#123; int Class; char position[10]; &#125;category;&#125;preson[2]; 链表链表是一种非常常用的动态数据结构，可以用来表示顺序访问的线性群体： 链表头：指向第一个链表结点的指针； 链表结点：链表中的每一个元素，包括：当前结点的数据，下一个结点的地址； 链表尾：不再指向其他结点的结点，其地址部分放一个NULL，表示链表到此结束。 关于new &amp; deletenew：C++运算符，动态地分配内存空间，并将所分配的内存的地址赋给指针变量。 delete：C++运算符，将动态分配的内存空间归还给系统。 用法一： &lt;指针变量&gt; = new&lt;类型&gt;; ​ 分配某种类型大小的一片连续内存空间，并将内存空间的首地址赋给指针变量。 delete&lt;指针变量&gt;; 123456789101112#include&lt;iostream&gt;using namespace std;int main()&#123; int *p = new int; cout &lt;&lt; *p &lt;&lt; endl; *p = 10; cout &lt;&lt; *p &lt;&lt; endl; delete p; return 0;&#125; 用法二： &lt;指针变量&gt; = new&lt;类型&gt;(初值); ​ 分配空间，并将初始值存入所分配的空间中。 delete&lt;指针变量&gt;; 123456789int main()&#123; int *p = new int(10); cout &lt;&lt; *p &lt;&lt; endl; delete p; cout &lt;&lt; *p &lt;&lt; endl; return 0;&#125; 用法三： &lt;指针变量&gt; = new&lt;类型&gt;[&lt;常量表达式&gt;]; ​ 分配指定类型的数组空间，并将数组的首地址赋给指针变量。 delete[ ]&lt;指针变量&gt;; ​ 将指针变量所指向一维数组内存空间归还给系统。 123456789101112#include&lt;iostream&gt;using namespace std;int main()&#123; int *p = new int[5]; memset(p, 0, 20); for(int i=0;i&lt;5;i++) cout &lt;&lt; *(p+i) &lt;&lt; endl; delete p; return 0;&#125; 当new &amp; delete 用于结构体 1234567891011121314#include&lt;iostream&gt;using namespace std;struct Node&#123; int n; Node *next;&#125;;int main()&#123; Node *p = new Node; cout &lt;&lt; p-&gt;n &lt;&lt; endl; cout &lt;&lt; p-&gt;next &lt;&lt; endl; return 0;&#125; 逐步建立链表 123456789101112131415161718192021222324252627282930313233343536373839#include&lt;iostream&gt;using namespace std;struct student&#123; int id; student *next;&#125;;student *create()&#123; student *head, *temp; int num, n = 0; head = new student; temp = head; cin &gt;&gt; num; while (num != -1) //-1作为结束符？ &#123; n++; temp-&gt;id = num; temp-&gt;next = new student; temp = temp-&gt;next; cin &gt;&gt; num; &#125; if (n == 0) head = NULL; else temp-&gt;next = NULL; return head;&#125;int main()&#123; student *pointer = create(); while (pointer-&gt;next != NULL) //遍历链表的元素 &#123; cout &lt;&lt; pointer-&gt;id &lt;&lt; endl; pointer = pointer-&gt;next; &#125; return 0;&#125; 删除结点 例子：在链表中将值为n的结点删掉 1234567891011121314151617181920212223242526student *dele(student *head, int n)&#123; student *temp, *follow; temp = head; if (head == NULL) //head为空时，说明链表为空表 return(head); if (head-&gt;id == n) //若第一个节点是要删除的目标 &#123; head = head-&gt;next; delete temp; return(head); &#125; while (temp != NULL &amp;&amp; temp-&gt;id != n) //寻到要删除的目标 &#123; follow = temp; temp = temp-&gt;next; &#125; if (temp == NULL) //若没到找到要删除的目标 cout &lt;&lt; \"not found\"; else &#123; follow-&gt;next = temp-&gt;next; //删除目标结点 delete temp; &#125; return(head);&#125; 插入结点1.将结点unit插入链表的最前面 2.将结点unit插入链表的中间 3.将结点unit插入链表的最后 例子：插入结点值为n的结点（按大小顺序） 12345678910111213141516171819202122232425262728293031323334student *insert(student *head, int n)&#123; student *temp, *unit, *follow; temp = head; unit = new student; unit-&gt;id = n; unit-&gt;next = NULL; if (head == NULL) //如果链表为空，直接插入 &#123; head = unit; return(head); &#125; while ((temp-&gt;next != NULL) &amp;&amp; (temp-&gt;id &lt; n)) //寻找第一个不小于n的结点temp &#123; follow = temp; temp = temp-&gt;next; &#125; if (temp == head) //如果temp为第一个结点 &#123; unit-&gt;next = head; head = unit; &#125; else &#123; if (temp-&gt;next == NULL) //如果temp为最后一个结点 temp-&gt;next = unit; else //如果temp为一个中间结点 &#123; follow-&gt;next = unit; unit-&gt;next = temp; &#125; &#125; return(head);&#125; 单向链表 双向链表 删除结点temp 将结点unit插入到temp之后 例子：约瑟夫环问题问题描述：n个孩子围坐成一圈，并按顺时针编号为1,2,3, ……,n，从编号为p的小孩顺时针依次报数，由1报到m，当报到m时，该小孩从圈中出去，然后下一个小孩再从1报数，当报到m时再出去。如此反复，直至所有的小孩都从圈中出去。请按出去的先后顺序输出小孩的编号（假设小孩的个数不多于300个）。 关于输入：n,p,m的值在1行内输入，以空格间隔 关于输出：按出圈的顺序输出编号，编号之间以逗号间隔。 思路： 首先定义结点的结构体，列出需要的函数的，然后再考虑每个函数需要完成的功能 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include&lt;iostream&gt;using namespace std;struct Node&#123; int num; Node *next; Node *ahead;&#125;;Node *Create(int N);Node *Search(Node *head, int P);Node *Release(Node *head, int M);int main()&#123; int N, P, M = 0; cout &lt;&lt; \"请输入人数N，从几号开始P，报到哪个数M：\" &lt;&lt; endl; cin &gt;&gt; N &gt;&gt; P &gt;&gt; M; Node *head = Create(N); //创建N个结点的环 head = Search(head, P); //找到第P个结点 while (head-&gt;next != head) //不断释放第M个元素，直到只剩一个元素 &#123; head = Release(head, M); &#125; cout &lt;&lt; head-&gt;num; return 0;&#125;Node *Create(int N) //创建包含N个结点的双向循环链表&#123; int n = 1; Node *node = new Node; node-&gt;num = n; Node *head = node; //指向第一节点 Node *tail = head; //指向最后一个节点 while (n++ &lt; N) &#123; node = new Node; //创建新节点 node-&gt;num = n; //赋值 tail-&gt;next = node; //插入新节点 node-&gt;ahead = tail; tail = tail-&gt;next; //尾巴后移一个 &#125; tail-&gt;next = head; head-&gt;ahead = tail; return head;&#125;Node *Search(Node *head, int P) //从head开始寻找第P个节点&#123; while (head-&gt;num != P) &#123; head = head-&gt;next; &#125; return head;&#125;Node *Release(Node *head, int M) //释放Head开始的第M个节点&#123; int count = 1; Node *temp = head; while (count &lt; M) //寻找第M个节点 &#123; temp = temp-&gt;next; count++; &#125; temp-&gt;ahead-&gt;next = temp-&gt;next; //移除第M个节点 temp-&gt;next-&gt;ahead = temp-&gt;ahead; //移除第M个节点 cout &lt;&lt; temp-&gt;num &lt;&lt; \", \"; head = temp-&gt;next; //释放第M个节点所占的内存空间 delete temp; return head;&#125;","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"Cpp基础（5）函数","slug":"Cpp基础（5）函数","date":"2020-01-30T11:41:22.000Z","updated":"2020-02-02T11:41:22.000Z","comments":true,"path":"2020/01/30/Cpp基础（5）函数/","link":"","permalink":"http://nekomoon404.github.io/2020/01/30/Cpp%E5%9F%BA%E7%A1%80%EF%BC%885%EF%BC%89%E5%87%BD%E6%95%B0/","excerpt":"","text":"函数基础函数的定义和声明函数是C++程序的基本构成单元，一个C++程序由一个或多个源文件组成，一个源程序文件可以由一个或多个函数组成。一个典型的函数（function）定义包括：返回类型（return type）、函数名字、由0个或多个形参（parameter）组成的列表以及函数体。函数执行的操作在语句块，称为函数体。 12345678//计算阶乘int fact(int val)&#123; int ret = 1; while (val &gt; 1) ret* = val--; return ret;&#125; 函数的类型是指函数返回值的数据类型，若函数类型与return语句中表达式的值不一致，则以函数类型为准，系统自动进行类型转换。 1234int bigger(float x, float y)&#123; return (x &gt; y ? x : y); //返回时会转换为整数&#125; 函数的名字也必须在使用之前声明，函数只能定义一次，但可以声明多次。函数的声明不包含函数体，所以也就无须形参的名字，但是加上便于理解。函数声明也称作函数原型（function prototype）。 建议变量在头文件中声明，在源文件中定义。与之类似，函数也该在头文件中声明而在源文件中定义。这样可以确保同一函数的所有声明保持一致。定义函数的源文件应该把含有函数声明的头文件包含进来，编译器负责验证函数的定义和声明是否匹配。 需要注意的是：函数不能嵌套定义，函数间可以互相调用，但不能调用main函数。 函数的调用函数的调用完成两项工作：一是用实参初始化函数对应的形参，二是将控制权转移给被调用函数。此时主调函数（calling function）的执行被暂时中断，被调函数（called function）开始执行。执行函数的第一步是（隐式地）定义并初始化它的形参。当遇到一条return语句时函数结束执行过程，return语句也完成两项工作：一是返回return语句中的值（如果有的话），二是将控制权从被调函数转移回主调函数。 123456int main()&#123; int j = fact(5); cout &lt;&lt; \"5! is\" &lt;&lt; j &lt;&lt; endl; return 0;&#125; 一个函数调用的执行过程可以分为3个阶段： 首先把实参值传入被调用函数形参的对应单元中，中断主调函数当前的执行，并且保存返回地点（称为断点）。 执行被调用函数语句，直到return语句返回。若被调用函数中没有return语句，则直到其全部语句执行完毕后自动返回到位于主调函数中的断点处。 从保存的断点处，主调函数继续执行其他剩余语句。 形参和实参实参是形参的初始值，编译器能以任意可行的顺序对实参求值。实参的类型必须与对应的形参类型匹配。实参与形参具有不同的存储单元，实参与形参变量的数据传递是“值传递”（passed by value）；函数调用时，系统给形参分配存储单元，并将实参对应的值传递给形参。 函数的形参列表可以为空，但是不能省略，其中每个形参都是含有一个声明符的声明，即使两个形参的类型一样，也必须把两个类型都写出来，且任意两个形参都不能同名， 函数的返回类型不能是数组类型或函数类型，但可以是指向数组或函数的指针。一种特殊的返回类型是void，它表示函数不返回任何值。 变量的作用范围根据变量在程序中作用范围的不同，可以将变量分为： 局部变量：在函数内或块内定义，只在这个函数或块内起作用的变量； 全局变量：在所有函数外定义的变量，它的作用域是从定义变量的位置开始到本程序文件结束。 当全局变量与局部变量同名时，局部变量将在自己作用域内有效，它将屏蔽同名的全局变量，即在局部变量的作用范围内，全局变量不起作用。 需要注意的是，不在必要时不要使用全局变量。因为全局变量在程序的全部指向过程中都占用存储单元；过多地使用全局变量，程序的可读性变差；会增加函数之间的“关联性”，降低了函数的独立性，使函数可移植性降低。 自动对象与局部静态对象对于普通局部变量对应的对象来说，当函数的控制路径经过变量定义语句时创建该对象，当到达定义所在的块末尾时销毁它，把只存在于块执行期间的对象称为自动对象（automatic object）。 形参是一种自动对象，我们用传递给函数的实参初始化形参对应的自动对象。对于局部变量对应的自动对象，分为两种情况：如果变量定义本身含有初始值，就用这个初始值进行初始化；否则，如果变量定义本身不含初始值，执行默认初始化。 有时局部变量的生命周期贯穿函数调用及之后的时间，可以将局部变量定义为static类型。局部静态对象（local static object）在程序的执行路径第一次经过对象定义语句时初始化，并且直到程序终止才被销毁。 123456789101112//下面的函数统计它自己被调用了多少次size_t count_calls()&#123; static size_t ctr = 0; return ++ctr;&#125;int main()&#123; for (size_t i=0; i!=10; ++i) cout &lt;&lt; cout_calls() &lt;&lt; endl; return 0;&#125; 指针形参指针用做函数参数，在函数内部改变指针的值只能改变局部变量，不会影响实参原来的值；在函数内部通过解引用操作改变指针所指内容的值，即实参指针所指内容的值也发生了改变。 例子：编写一个函数，使用指针形参交换两个整数的值。 123456789101112131415161718#include&lt;iostream&gt;using namespace std;void mySWAP(int *p, int *q)&#123; int tmp = *p; *p = *q; *q = tmp;&#125;int main()&#123; int a = 5, b = 10; int *r = &amp;a, *s = &amp;b; cout &lt;&lt; \"交换前：a=\" &lt;&lt; a &lt;&lt; \"，b=\" &lt;&lt; b &lt;&lt; endl; mySWAP(r, s); cout &lt;&lt; \"交换后：a=\" &lt;&lt; a &lt;&lt; \"，b=\" &lt;&lt; b &lt;&lt; endl; return 0;&#125; 需要注意的是，下面的函数并不能满足要求，因为在函数内部改变指针的值（改变指针所指的地址）只能改变局部变量。 123456void mySWAP(int *p, int *q)&#123; int *tmp = p; p = q; q = tmp;&#125; 引用形参我们知道对于引用的操作实际上是作用在引用所引的对象上。引用形参的行为与之类似。 与值传递（实参的值被拷贝给形参，形参和实参是两个相互独立的变量）不同的是，引用形参是传引用的方式，形参是对应的实参的别名，形参绑定到初始化它的对象，如果改变了形参的值，也就是改变了对应实参的值。 用引用形参重写上面例子中的程序，引用形参绑定初始化它的对象，p绑定我们传给函数的int对象a，改变p的值也就是改变p所引对象的值。 1234567891011121314151617#include&lt;iostream&gt;using namespace std;void mySWAP(int &amp;p, int &amp;q)&#123; int tmp = p; p = q; q = tmp;&#125;int main()&#123; int a = 5, b = 10; cout &lt;&lt; \"交换前：a=\" &lt;&lt; a &lt;&lt; \"，b=\" &lt;&lt; b &lt;&lt; endl; mySWAP(a, b); cout &lt;&lt; \"交换后：a=\" &lt;&lt; a &lt;&lt; \"，b=\" &lt;&lt; b &lt;&lt; endl; return 0;&#125; 使用引用形参避免拷贝拷贝大类类型对象或者容器对象比较低效，甚至有的类类型不支持拷贝。当某种类型不支持拷贝操作时，函数只能通过引用形参访问该类型的对象。 如果函数无须改变引用形参的值，最好将其声明为常量引用。把函数不会改变的形参定义成（普通的）引用是一种比较常见的错误，这么做有几个缺陷：一是容易给使用者一种误导，即程序允许修改变量s的内容；二是限制了该函数所能接受的实参类型，我们无法把const对象、字面值常量或者需要进行类型转换的对象传递给普通的引用形参。 123456789101112131415161718192021//比较两个string 对象的长度bool isShorter(const string &amp;s1, const string &amp;s2)&#123; return s1.size() &lt; s2.size();&#125;//判断一个string对象是否含有大写字母bool HasUpper(const string &amp;str) //无须修改参数的内容，设为常量引用类型&#123; for (auto c : str) if(isupper(c)) return true; return false;&#125;//把字符串的所有大写字母转成小写void ChangeToLower(string &amp;str)&#123; for (auto &amp;c : str) c = tolower(c);&#125; 使用引用形参返回额外信息一个函数只能返回一个值，然而有时函数需要同时返回多个值，引用形参为一次返回多个结果提供了有效的途径。（对于引用的操作实际上是作用在引用所引的对象上） 例子：定义一个名为find_char的函数，返回string对象中某个指定字符第一次出现的位置，同时能“返回”该字符出现的次数。 一种思路是定义一个新的数据类型，包含位置和数量两个成员，显然比较复杂；另一种更简单的方法是，给函数传入一个额外的引用实参。 123456789101112131415string::size_type find_char(const string &amp;s, char c, string::size_type &amp;occurs)&#123; auto ret = s.size(); occurs = 0; for (decltyoe(ret) i = 0; i!=s.size(); i++) &#123; if(s[i] == c) &#123; if(ret == s.size()) ret = i; //记录c第一次出现的位置 ++occurs; &#125; &#125; return ret; //出现次数通过occurs隐式地返回&#125; 数组形参数组有两个特殊性质：不允许拷贝数组，以及使用数组时通常会将其转换成指针。所以我们不能以值传递的方式使用数组参数，当我们为函数传递一个数组时，实际上传递的是指向数组首元素的指针。 尽管不能以值传递的方式传递数组，但是我们可以把形参写成类似数组的形式： 1234//这三个print函数是等价的void print(const int*);void print(const int[]);void print(const int[10]); 当编译器处理对print函数的调用时，只检查传入的参数是否是const int*类型；如果我们传给print函数的是一个数组，则实参自动地转换成指向数组首元素的指针，数组的大小对函数的调用没有影响。以数组为形参的函数也必须确保使用数组时不会越界。 1234567891011121314151617#include&lt;iostream&gt;using namespace std;void sum(int *p, int n)&#123; int total = 0; for (int i=0; i&lt;n; i++) &#123; total+=*p++; &#125; cout &lt;&lt; total &lt;&lt; endl;&#125;int main()&#123; int a[10] = &#123;1,2,3,4,5,6,7,8,9,10&#125;; sum(a,10); return 0;&#125; 多维数组名做函数参数当将多维数组传递给函数时，真正传递的是指向数组首元素的指针，而多维数组的首元素是一个数组，指针就是一个指向数组的指针。数组第二维（以及后面所有维度）的大小都是数组类型的一部分，不能省略。 123//这两个print等价void print(int (*matrix)[10], int rowSize); //（*matrix)的括号不能少void print(int matrix[][10], int rowSize); 例子：求一个$3\\times 4$的矩阵的所以元素中的最大值。 123456789101112131415int maxvalue(int (*p)[4])&#123; int max = p[0][0]; for(int i=0; i&lt;3; i++) for(int j=0; j&lt;4; j++) if(p[i][j] &gt; max) max = p[i][j]; return max;&#125;int main()&#123; int a[3][4] = &#123;&#123;1,3,5,7&#125;,&#123;9,11,13,15&#125;,&#123;2,4,6,8&#125;&#125;; cout &lt;&lt; \"The Max value is\" &lt;&lt; maxvalue(a); return 0;&#125; 数组引用形参形参也可以是数组的引用，此时引用形参绑定到对应的实参上，也就是绑定到数组上。但此时函数只能作用于固定大小的数组。 12345void print(int (&amp;arr)[10]) //只能将函数作用于大小为10的数组，(&amp;arr)的括号不能少&#123; for(auto elem : arr) cout &lt;&lt; elem &lt;&lt;endl;&#125; 函数的递归什么是递归我们已经知道：函数不能嵌套定义，函数可以嵌套调用。那么一个函数能调用“自己”嘛？答案是可以的 例子：已知 n，求n的阶乘$n!$ \\begin{align*} n!&=(n-1)!*n \\\\ (n-1)!&=(n-2)!*(n-1) \\\\ &\\dots \\\\ 2!&=1!*2 \\\\ 1!&=1 \\end{align*}1234567891011121314#include&lt;iostream&gt;using namespace std;int fact(int n)&#123; if(n==1) return 1; else return n*fact(n-1); //每次调用，数据规模缩小&#125;int main()&#123; cout &lt;&lt; fact(4) &lt;&lt;endl; return 0;&#125; 深入理解递归的过程递归调用与普通调用在实质上是一样的。 通过下面的两个例子来理解递归的过程。 12345678910111213141516#include&lt;iostream&gt;using namespace std;int recur()&#123; char c; c = cin.get(); if (c != '\\n') recur(); cout &lt;&lt; c; return 0;&#125;int main()&#123; recur(); return 0;&#125; 12345678910111213141516#include&lt;iostream&gt;using namespace std;int recur()&#123; char c; c = cin.get(); cout &lt;&lt; c; if (c != '\\n') recur(); return 0;&#125;int main()&#123; recur(); return 0;&#125; 递归的作用用递归来完成递推递归的关注点放在求解目标上，重在表现第i次与第i+1次的关系，让程序变得简明。必须要确定第1次的返回结果。 例子：斐波那契数列 \\begin{align*} fab(n)&=fab(n-1)+fab(n-2) \\\\ fab(1)&=1,\\, fab(2)=1 \\end{align*}123456789int f(int n)&#123; if(n == 1) return 1; if(n == 2) return 1; else return(f(n-1)+f(n-2)); &#125; 模拟连续发生的动作主要是搞清楚连续发生的动作是什么；搞清楚不同动作之间的关系；搞清楚边界条件是什么。 例子1：将一个十进制整数转换成二进制数 12345678910111213141516171819#include&lt;iostream&gt;using namespace std;void convert(int x)&#123; if ((x / 2) != 0) &#123; convert(x / 2); cout &lt;&lt; x % 2; &#125; else cout &lt;&lt; x;&#125;int main()&#123; int x; cin &gt;&gt; x; convert(x); return 0;&#125; 例子2：汉诺塔问题 相传在古代印度有位僧人整天把三根柱子上的金盘倒来倒去，他想把64个一个比一个小的金盘从一根柱子上移到另一个柱子上去。移动过程中恪守下述规则：每次只允许移动一只盘，且大盘不得落在小盘上面。 123456789101112131415161718192021222324#include&lt;iostream&gt;using namespace std;void move(int m, char A, char B, char C) //表示将m个盘子从A经过B移动到C&#123; if (m == 1) &#123; cout &lt;&lt; \"move 1# from\" &lt;&lt; A &lt;&lt; \"to\" &lt;&lt; C &lt;&lt; endl; //直接可解结点 &#125; else //如果m不为1，则要调用move(m-1) &#123; move(m - 1, A, C, B); cout &lt;&lt; \"move 1# from\" &lt;&lt; A &lt;&lt; \"to\" &lt;&lt; C &lt;&lt; endl; move(m - 1, B, A, C); &#125;&#125;int main()&#123; int n; cout &lt;&lt; \"请输入盘数n=\" &lt;&lt; endl; cin &gt;&gt; n; cout &lt;&lt; \"在3根柱子上移\" &lt;&lt; n &lt;&lt; \"个盘子的步骤为：\" &lt;&lt; endl; move(n, 'A', 'B', 'C'); return 0;&#125; 进行“自动的分析”先假设有一个函数能给出答案，再利用这个函数分析如何解决问题；搞清楚最简单的情况下答案是什么。 例子：放苹果 把m个同样的苹果放在n个同样的盘子里，允许有的盘子空着不放，问共有多少种不同的分法？注意：5,1,1和1,5,1是同一种分法。 思路： 假设有一个函数f(m,n)能解决这个问题，那么最简单的情况是m&lt;=1||n&lt;=1，此时只有1种分法。 当n&gt;m时，必有盘子会空着，空着的盘子不影响结果，那么有f(m,n)=f(m,m)。 当n&lt;=m时，分两种情况： (1)如果有盘子空着，那么减少一个盘子也不会影响结果，有f(m,n)=f(m,n-1)。 (2)如果盘子全满，那么每个盘子至少有1个苹果，那么只需考虑剩下m-n个苹果在n个盘子中的分法，则有 f(m,n)=f(m-n,n)。 12345678910111213141516#include&lt;iostream&gt;using namespace std;int count(int m, int n)&#123; if (m &lt;= 1 || n &lt;= 1) return 1; if (m &lt; n) return count(m, m); else return count(m, n - 1) + count(m - n, n);&#125;int main()&#123; int m, n; cin &gt;&gt; m &gt;&gt; n; cout &lt;&lt; count(m, n) &lt;&lt; endl;&#125; 递归问题解法小结面对一个问题时： 假设有一个函数f()可以解决问题；接下来考虑这个函数是什么样的？ 找到f^n()与f^n-1()之间的关系； 确定f()的参数； 分析并写出边界条件。 例子1：组合问题 用递归法计算从n个人中选择k个人组成一个委员会，求不同的组合的个数一共是多少？ 思路： 由n个人里选k个人的组合数=由n-1个人里选k个人的组合数+由n-1个人里选k-1个人的组合数； 当n = k或k = 0时，组合数为1。 123456789101112131415161718192021#include&lt;iostream&gt;using namespace std;int comm(int n, int k)&#123; if (k &gt; n) return 0; else if (n == k || k == 0) return 1; else return comm(n - 1, k) + comm(n - 1, k - 1);&#125;int main() &#123; int n, k; cout &lt;&lt; \"Please enter two integers n and k: \"; cin &gt;&gt; n &gt;&gt; k; cout &lt;&lt; \"C(n,k) = \" &lt;&lt; comm(n, k) &lt;&lt; endl; return 0;&#125; 探索式递归例子1：下楼问题 从楼上走到楼下共有h个台阶，每一步有3种走法：走1个台阶；走2个台阶；走3个台阶。问可以走出多少种方案？将所有的方案输出。 思路： 既然要列出所有方案，所以需要用一个数组存放每步走的步数，可设为take[99]，步数存放在take[ ]中，满足条件就打印出来； 假设有一个函数Try( )能解决问题，接着寻找Try^n^( )与Try^n+1^( )的关系； Try^n^( )代表走完第n步的状态，即已经填完第n个take[ ]； Try^n( )与Try^n+1( )的关系：在走完第n步后，再走第n+1步时，有三种选择（走1、2、3步），每个选择下有三种可能性： 如果剩下的台阶数小于想要走的步数：返回 如果剩下的台阶数恰好等于要走的步数：打印输出 如果剩下的台阶数大于想要走的步数：走下去 Try( )的参数如何确定：Try^n^( )与Try^n+1^( )之间哪些数据是不一样的？而且是需要由Try^n( )传递给Try^n+1^( )的？ ​ Try^n^( )代表走完第n步的情况，Try^n+1^( )代表走完第n+1步的情况； ​ Try^n^( )需要将走完第 n步后剩余的台阶数传递 给Try^n+1^( )。 ​ 因此可以将Try^n^( )定义为：Try(i, s)，i表示剩余的台阶数，s表示步数。 123456789101112131415161718192021222324252627282930313233#include&lt;iostream&gt;using namespace std;int take[99]; //记录每一个走的台阶数int num = 0; //num记录解决方案的个数void Try(int i,int s)&#123; for (int j = 1; j &lt;= 3; j++) &#123; if (i &lt; j) continue; take[s] = j; if (i == j) &#123; num++; cout &lt;&lt; \"solution\" &lt;&lt; num &lt;&lt; \": \"; for (int k = 1; k &lt;= s; k++) cout &lt;&lt; take[k]; cout &lt;&lt; endl; &#125; else Try(i - j, s + 1); //take[s]=0; &#125;&#125;int main()&#123; int h = 0; cout &lt;&lt; \"how many stairs:\"; cin &gt;&gt; h; Try(h, 1); cout &lt;&lt; \"There are \" &lt;&lt; num &lt;&lt; \" solutions.\" &lt;&lt; endl; return 0;&#125; 例子2：字母全排列 从键盘读入一个英文单词（全部字母小写，且该单词中各个字母均不相同），输出该单词英文字母的所有全排列。 如输入abc，则打印出abc, acb, bac, bca, cab, cba。 思路： 需要反复做的事情是：选择第n个位置的字母，依次检查每个字母，如果某个字母没被选择过，则进行： 将该字母放第n个位置； 标记该字母已经被选择； 如果全部位置都已选完，打印输出；否则，为下一个位置选择字母； 把刚刚标记过的字母重新标记为“未选择”； 假设一个函数ranker( )能够完成上述事情，每次调用之间的区别在于位置n，ranker(1)—&gt;ranker(2)—&gt;ranker(3)……—&gt;ranker(n)。 12345678910111213141516171819202122232425262728293031323334#include&lt;iostream&gt;using namespace std;char in[30] = &#123; 0 &#125;; //存放输入的单词char out[30] = &#123; 0 &#125;; //存放准备输出的字符串int used[30] = &#123; 0 &#125;; //记录第i个字母是否已经使用过int length = 0; //记录输入的单词的长度void ranker(int n)&#123; if(n==length) //如果全部字母已经被选择完，则打印输出 &#123; cout &lt;&lt; out &lt;&lt; endl; return; &#125; for (int i = 0; i &lt; length; i++) //依次查看每个字母 &#123; if (!used[i]) //如果某个字母没有被选用 &#123; out[n] = in[i]; //选入该字母 used[i] = 1; //标记该字母已经被选择 ranker(n + 1); //为下一个位置寻找字母 used[i] = 0; //回溯，标记字母未被使用，让其可重新被选择 &#125; &#125;&#125;int main()&#123; cout &lt;&lt; \"Input the word: \"; cin &gt;&gt; in; length = strlen(in); ranker(0); //从第一个字母开始 return 0;&#125; 例子3：分书问题 有编号分别为1, 2, 3, 4, 5的五本书，准备分给A，B，C，D，E五个人，每个人阅读兴趣用一个二维数组加以描述。请写一个程序，输出所有分书方案，让人人都能拿到喜欢的书。 思路： 假设函数trybook( )可以解决问题，从第0个人开始分书，函数trybook(i)应该要完成： 试着给第i个人分书，从0号书开始试，当第i个人喜欢第j个书，且j书还没被选走时（因此要建一个数组记录书被选走的状态），那么第i个人就得到第j本书； 如果不满足上述条件，则什么也不做，返回循环条件； 若满足条件，则做三件事情： 做事：将第j个书分给第i个人，同时记录j书已被选用； 判断：查看是否将所有5个人所要的书分完，若分完，则输出每个人所得之书；若未分完，去寻找其他解决方案； 回溯：让第i个人退回j书，恢复j书尚未被选用的状态。 123456789101112131415161718192021222324252627282930313233343536#include&lt;iostream&gt;using namespace std;int like[5][5] = &#123; &#123;0,0,1,1,0&#125;,&#123;1,1,0,0,1&#125;,&#123;0,1,1,0,1&#125;,&#123;0,0,0,1,0&#125;,&#123;0,1,0,0,1&#125; &#125;;int book[5] = &#123; 0 &#125;; //book[5]记录书是否被选用，选用记为1int take[5] = &#123; 0 &#125;; //take[5]记录第i个人领到那本书int num; //num记录分书方案的个数void trybook(int i) //第i个人&#123; for (int j = 0; j &lt;=4; j++) //第j本书 &#123; if ((like[i][j] &gt; 0) &amp;&amp; (book[j] == 0)) //若第i个人喜欢第j本书，且第j本书还没被选用 &#123; take[i] = j; //把第j本书分给第i个人 book[j] = 1; //记录第j本书已经被选用 if (i == 4) //如果第5个人已经拿到书，即书已分完，则输出方案 &#123; num++; cout &lt;&lt; \"第\" &lt;&lt; num &lt;&lt; \"个方案\" &lt;&lt; endl; for (int k = 0; k &lt;= 4; k++) cout &lt;&lt; take[k] &lt;&lt; \"号书给\" &lt;&lt; char(k + 65)&lt;&lt;\" \"; cout &lt;&lt; endl; &#125; else //如果书没分完，则继续给下一个人分书 trybook(i + 1); //take[i] = -1; 把第i个人的书退回，实际上可以不加这一条 book[j] = 0; //回溯，把第j本书标记为未选用 &#125; &#125;&#125;int main()&#123; int n = 0; trybook(0); return 0;&#125; 探索式递归问题的解法第n步需要做什么？对于面前的每种选择： 把该做的事情做了； 判定是否得到解； 递归（调用第n+1步）； 看是否需要回溯。","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"Cpp基础（4）字符数组与字符串","slug":"Cpp基础（4）字符数组与字符串","date":"2020-01-29T03:08:57.000Z","updated":"2020-01-29T05:08:57.000Z","comments":true,"path":"2020/01/29/Cpp基础（4）字符数组与字符串/","link":"","permalink":"http://nekomoon404.github.io/2020/01/29/Cpp%E5%9F%BA%E7%A1%80%EF%BC%884%EF%BC%89%E5%AD%97%E7%AC%A6%E6%95%B0%E7%BB%84%E4%B8%8E%E5%AD%97%E7%AC%A6%E4%B8%B2/","excerpt":"","text":"字符数组与字符串定义字符型用于表示单个字符 只占用1个字节，将对应的ASCII码放入存储单元语法：char ch = &#39;a&#39;需要注意：1.要用单引号将字符括起来2.单引号内只能有一个字符，不可以是字符串 字符串型用于表示一串字符两种风格： C风格字符串： char 变量名[] = “字符串值” 要用双引号 C++风格字符串： string 变量名 = “字符串值” 需要加入头文件 #include&lt;string&gt; 转义字符用于表示一些不能显示出来的ASCII字符常用的转义字符：/n 换行，将当前位置移到下一行开头 /t 水平制表，跳到下一个TAB位置，/t和其前面的内容一共占8个字符 // 代表一个反斜杠字符 初始化与赋值只可以在数组定义并初始化的时候才可以使用字符串字面值对字符数组初始化，一定要注意字符串字面值的结尾处还有一个空字符。不能用赋值语句将一个字符串常量或字符数组直接赋给另一个数组。 1234567891011char a1[] = &#123;'C', '+', '+'&#125;; //列表初始化，没有空字符char a2[] = &#123;'C', '+', '+', '\\0'&#125;; //列表初始化，含有显式的空字符char a3[] = \"C++\"; //用字符换字面值初始化，自动添加表示字符串结束的空字符const char a4[6] = \"Daniel\" //错误：没有空间放空字符str1[] = \"China\"; //错误str1 = \"China\"; //错误str2 = str1; //错误//利用二维数组存储多个字符串char weekday[7][11] = &#123;\"Sunday\", \"Monday\",\"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\",\"Ssturday\"&#125;; 字符/字符数组/字符串的输入与输出一个字符的输入直接用cin输入字符。cin这一输入操作，遇到结束符（Space, Tab, Enter）就会结束，且对于结束符，并不保存到变量中，但最后一个Enter会在缓冲区。 123456789#include&lt;iostream&gt;using namespace std;int main()&#123; char c; cout&lt;&lt;\"enter a sentence\"&lt;&lt;endl; while(cin&gt;&gt;c) //abc def g cout&lt;&lt;c; //abcdefg return 0;&#125; 用cin.get()函数输入可以用于读入一个字符；有2中形式： 无参数cin.get()，可用于舍弃输入流中的不需要的字符，或者舍弃回车，弥补三参数的cin.get(ch, 10, &#39;/n&#39;)的不足。 1234char c;cout&lt;&lt;\"enter a sentence\"&lt;&lt;endl; while( (c=cin.get())!= EOF ) //abc def gcout &lt;&lt; c; //abc def g 一个参数cin.get(char) 12345char c;cout&lt;&lt;\"enter a sentence\"&lt;&lt;endl; //读取一个字符赋给字符变量cwhile( cin.get(c) ) //abc def gcout &lt;&lt; c; //abc def g 要注意的是，cin.get()遇到结束符停止读取，但并不会将结束符从缓冲区丢弃。 123456char ch1,ch2; cout&lt;&lt;\"请输入两个字符：\"&lt;&lt;endl; cin.get(ch1);//或ch1 = cin.get(); cin.get(ch2); cout&lt;&lt;ch1&lt;&lt;\" \"&lt;&lt;ch2&lt;&lt;endl; cout&lt;&lt;(int)ch1&lt;&lt;\" \"&lt;&lt;(int)ch2&lt;&lt;endl; 输入a[Enter]，读取到结束符&#39;/n&#39;，其仍在缓冲区中被存入ch2，在输出a之后，第二次输出&#39;/n&#39;即换行，而输出的第二个ASCII码值为10，即&#39;/n&#39;的ASCII值，说明cin.get()遇到结束符并不会将之删除。 用getchar()输入字符1234char c;cout&lt;&lt;\"enter a sentence\"&lt;&lt;endl; while( c = getchar() ) //abc def g 不跳过任何字符cout &lt;&lt; c; //abc def g 字符串的输入直接用cin输入字符123456789#include&lt;iostream&gt;using namespace std;int main() &#123; char str[10]; cout &lt;&lt; \"enter a sentence\" &lt;&lt; endl; while (cin &gt;&gt; str) cout &lt;&lt; str &lt;&lt; endl; return 0;&#125; 用cin.get()函数输入有三个参数的cin.get()函数：cin.get(ch, 10, &#39;/n&#39;) 读取10-1（10减1=9，最后一个为&#39;/0&#39;）个字符（包括空格），赋给指定的字符数组，；如果在读取9个字符之前，遇到指定的终止字符&#39;/n&#39;，则提前结束读取（如果第3个参数没有指定，则默认为&#39;/n&#39;），而结束符仍在缓冲区中；读取成功返回非0值（真），如失败（遇到文件结束符）则返回0值（假）。 还要一点要注意，cin.get(ch, 10, &#39;/n&#39;)，当第一个输入字符为结束符时，缓冲区将无该结束符。 1234567891011#include&lt;iostream&gt;using namespace std;int main() &#123; char ch1[20]，ch2[20]; cout &lt;&lt; \"enter a sentence\" &lt;&lt; endl; cin.get(ch1,10,'o'); //指定终止符为'o' cin.get(ch2,10); cout &lt;&lt; ch1 &lt;&lt; endl; cout &lt;&lt; ch2 &lt;&lt; endl; return 0;&#125; 输入：we are good friends[Enter]，由于遇到结束符 &#39;o&#39; ，首先读入we are g到ch1，此时ood friends仍在缓冲区，当执行cin.get(ch2,10)会直接从缓冲区读入ood frien（只能读入9个字符），而不需要申请从键盘输入。 用cin.getline()函数输入用法与上面的cin.get()类似，但也有区别： cin.get()当输入的字符串在结束符之前的长度超过接收长度时，不会引起cin函数的错误，剩余的字符会留在缓冲区，后面若有cin操作，会继续从缓冲区读取；当cin.getline()输入超长时，会引起cin函数的错误，后面的cin操作将不再执行。 cin.get()每次读取一整行并把由Enter键生成的换行符&#39;/n&#39;留在输入队列中，然而cin.getline()每次读取一整行并把由Enter键生成的换行符抛弃。 cin.get()遇到结束符是停止读取，缓冲区指针不移动；cin.getline()遇到结束符时，缓冲区指针移到终止标志字符之后。 123456char ch1[20]，ch2[20];cout &lt;&lt; \"enter a sentence\" &lt;&lt; endl;cin.getline(ch1,10,'o'); //指定终止符为'o' cin.getline(ch2,10); cout &lt;&lt; ch1 &lt;&lt; endl; cout &lt;&lt; ch2 &lt;&lt; endl; //注意与上节的区别 一个需要注意的地方：cin这一输入操作，遇到结束符（Space, Tab, Enter）就会结束，且对于结束符，并不保存到变量中，但最后一个Enter会在缓冲区。而无参数cin.get()，可用于舍弃输入流中的不需要的字符，或者舍弃回车，弥补三参数的cin.get(ch, 10, &#39;/n&#39;)的不足。 123456789101112#include&lt;iostream&gt;using namespace std;int main() &#123; char a[10][10]; int n = 0; cin &gt;&gt; n; //输入7[Enter]，7存入n，[Enter]仍在缓冲区， for (int i = 0; i &lt; n; i++) cin.getline(a[i], 10); //当执行cin.getline(a[1],10)时，[Enter]被读取到a[1] for (int i = 0; i &lt; n; i++) cout &lt;&lt; a[i] &lt;&lt; endl; return 0;&#125; 12345678910111213#include&lt;iostream&gt;using namespace std;int main() &#123; char a[10][10]; int n = 0; cin &gt;&gt; n; //输入7[Enter]，7存入n，[Enter]仍在缓冲区 cin.get(); //用cin.get来舍弃缓冲区的[Enter]，就不会出现上面的情况了 for (int i = 0; i &lt; n; i++) cin.getline(a[i], 10); for (int i = 0; i &lt; n; i++) cout &lt;&lt; a[i] &lt;&lt; endl; return 0;&#125; 字符串的输出用cout输出字符数组12345678#include&lt;iostream&gt;using namespace std;int main()&#123; char a[10] = \"Computer\"; cout &lt;&lt; a; return 0;&#125; 需要注意的是，cout输出有终止条件（一般是碰到&#39;\\0&#39;），当用字面值初始化字符数组时，会自动在串尾加上&#39;\\0&#39;；当用列表初始化字符数组时，若不在串尾加上’\\0’，用cout输出时就不知道何时停止，可能读到内存里其他随机的位置，显示就会在字符串后面出现乱码。 12char a[8] = &#123; 'C','o','m','p','u','t','e','r' &#125;;cout &lt;&lt; a; 例子1.字符串加密：输入一个字符串，把每个字符变成它后续字符，如果是’Z’或者’z’，则对应变成’A’或者’a’，空格则不变。然后将变换后的字符串输出；要求能够接受连续输入。 1234567891011121314151617181920212223#include&lt;iostream&gt;using namespace std;int main() &#123; char str[200]; while (cin.getline(str, 200)) &#123; for (int i = 0; str[i] != '\\0'; i++) &#123; if (str[i] == 'Z')&#123; str[i] = 'A'; continue; &#125; if (str[i] == 'z')&#123; str[i] == 'a'; continue; &#125; if (str[i] == ' ') continue; str[i]++; &#125; cout &lt;&lt; str &lt;&lt; endl; &#125; return 0;&#125; 2.字符串连接：输入两个字符串，将其中较短的串接到较长的串的后面。不使用系统函数strcat，每个输入的串的长度不超过20。 12345678910111213141516171819202122232425#include&lt;iostream&gt;using namespace std;int main() &#123; int len1, len2; char str1[40], str2[40]; cin.getline(str1,20); cin.getline(str2, 20); for (len1 = 0; str1[len1] != '\\0'; len1++); for (len2 = 0; str2[len2] != '\\0'; len2++); if (len1 &gt;= len2) &#123; for (len2 = 0; str2[len2] != '\\0'; len2++) str1[len1++] = str2[len2]; //后置++，先执行表达式，再len+1 str1[len1] = '\\0'; //必须要加，要不然cout&lt;&lt;str1会出错 &#125; else &#123; for (len1 = 0; str1[len1] != '\\0'; len1++) str2[len2++] = str1[len1]; str2[len2] = '\\0'; &#125; cout &lt;&lt; str1 &lt;&lt; endl; cout &lt;&lt; str2 &lt;&lt; endl; return 0;&#125; 3.统计单词数：输入一个英文句子（不超过80个字母），统计其中有多少个单词，单词之间用空格分开。 123456789101112131415161718#include&lt;iostream&gt;using namespace std;int main() &#123; char str[80]; int num = 0, flag = 0; cin.getline(str, 80); for (int i = 0; str[i] != '\\0'; i++) &#123; if (str[i] == ' ') flag = 0; else if (flag == 0) &#123; flag = 1; num++; &#125; &#125; cout &lt;&lt; \"字符串中有\" &lt;&lt; num &lt;&lt; \"个单词\" &lt;&lt; endl; return 0;&#125; 当输入不是很严格时，比如有数字，或输入不规范，比如标号后不加空格等等，上面的程序会多算单词数。可以改成如下的程序，直接判断是不是字母。 123456789101112131415161718#include&lt;iostream&gt;using namespace std;int main() &#123; char str[80]; int num = 0, flag = 0; cin.getline(str, 80); for (int i = 0; str[i] != '\\0'; i++) &#123; if ((str[i] &gt;= 'A'&amp;&amp; str[i] &lt;= 'Z') || (str[i] &gt;= 'a'&amp;&amp;str[i] &lt;= 'z')) flag = 0; else if (flag == 0) &#123; flag = 1; num++; &#125; &#125; cout &lt;&lt; \"字符串中有\" &lt;&lt; num &lt;&lt; \"个单词\" &lt;&lt; endl; return 0;&#125; 对于char数组char s[100]，要读入一整行可以用cin.getline(s, 100);，或者fgets(s, 100, stdin); 要输出可以用printf(&quot;%s\\n&quot;, s);，等价于puts(s); char数组常用的函数有（要引入头文件#include&lt;string.h&gt;）： strlen(str)，求字符串的长度（&#39;\\0&#39;不计入其中）； strcmp(a, b)，比较两个字符串的大小，a &lt; b 返回-1，a == b 返回0，a &gt; b返回1。这里的比较方式是字典序（字典序一般和贪心相关）； strcpy(a, b)，将字符串b复制给从a开始的字符数组。 C++标准库类型stringC++标准库类型string字符串是可变长的字符序列，比字符数组更加好用。需要引入头文件#include&lt;string&gt;，读入string字符串可以用getline(cin, s);读入一整行；使用cin &gt;&gt; s;读入，遇到结束符（Space, Tab, Enter）就会终止。 定义和初始化： 123456789101112#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;int main()&#123; string s1; //默认初始化，s1是一个空字符串 string s2 = s1; //s2是s1的副本 string s3 = \"hiya\"; //s3是该字符串字面值的副本 string s4(10, 'c'); //s4的内容是cccccccccc return 0;&#125; string的读写：（不能用printf直接输出string，需要写成：printf(“%s”, s.c_str());） 1234567891011121314#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;int main()&#123; string s1, s2; cin &gt;&gt; s1; getline(cin, s2); //读入一行 cout &lt;&lt; s1 &lt;&lt; \" \" &lt;&lt; s2 &lt;&lt; endl; //printf(\"%s\", s1.c_str()); //puts(s1.c_str()); return 0;&#125; 常用函数 / 操作： .empty()用来判断字符串是否为空，返回一个布尔值； .size()操作返回字符串的长度，其时间复杂度为$O(1)$（注意size是无符号整数，因此 s.size() &lt;= -1一定成立）； string的比较：直接使用关系符&lt;, &lt;=, &gt;, &gt;=, ==, !=，按字典顺序比较； 123456789101112#include&lt;iostream&gt;#include&lt;string&gt;using namespace std;int main()&#123; string s1, s2 = 'abc'; cout &lt;&lt; s1.empty() &lt;&lt; endl; //1 cout &lt;&lt; s2.empty() &lt;&lt; endl; //0 cout &lt;&lt; s2.size() &lt;&lt; endl; //3 return 0;&#125; 为string对象赋值：s1 = s2;，用s2的副本替换s1的副本； 为string对象赋值： 123string s1(10, ‘c’), s2; // s1的内容是 cccccccccc；s2是一个空字符串s1 = s2; // 赋值：用s2的副本替换s1的副本 // 此时s1和s2都是空字符串 两个string对象相加： 123string s1 = “hello, ”, s2 = “world\\n”;string s3 = s1 + s2; // s3的内容是 hello, world\\ns1 += s2; // s1 = s1 + s2 字面值和string对象相加：做加法运算时，字面值和字符都会被转化成string对象，因此直接相加就是将这些字面值串联起来；当把string对象和字符字面值及字符串字面值混在一条语句中使用时，必须确保每个加法运算符的两侧的运算对象至少有一个是string。 12345678string s1 = “hello”, s2 = “world”; // 在s1和s2中都没有标点符号string s3 = s1 + “, “ + s2 + ‘\\n’; // s3的内容是 hello, world\\nstring s4 = s1 + “, “; // 正确：把一个string对象和有一个字面值相加string s5 = “hello” +”, “; // 错误：两个运算对象都不是stringstring s6 = s1 + “, “ + “world”; // 正确，每个加法运算都有一个运算符是stringstring s7 = “hello” + “, “ + s2; // 错误：不能把字面值直接相加，运算是从左到右进行的 处理string对象中的字符 ： 可以将string对象当成字符数组来处理： 12string s = \"hello world\";for(int i = 0; i &lt; s.size(); i++) cout &lt;&lt; s[i] &lt;&lt; endl; 或者使用基于范围的for语句： 123string s = \"hello world\";for(char c : s) cout &lt;&lt; c &lt;&lt; endl;for(char &amp;c : s) c = 'a'; //改变s中的每一个字符","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"Cpp基础（3）数组","slug":"Cpp基础（3）数组","date":"2020-01-28T03:06:06.000Z","updated":"2020-01-28T07:06:06.000Z","comments":true,"path":"2020/01/28/Cpp基础（3）数组/","link":"","permalink":"http://nekomoon404.github.io/2020/01/28/Cpp%E5%9F%BA%E7%A1%80%EF%BC%883%EF%BC%89%E6%95%B0%E7%BB%84/","excerpt":"","text":"数组数组是一种类似于标准库类型vector的数据结构，与vector相似的是，数组也是存放类型相同的对象的容器，这些对象需要通过其所在位置访问；与vector不同的是，数组的大小确定不变，不能随意向数组中增加元素。 定义和初始化内置数组数组的声明形如 a[d] ，其中a是数组的名字，d是数组的维度。维度必须是一个常量表达式。 123456constexpr unsigned sz = 42; //常量表达式int arr[10];int *parr[sz];unsigned cnt = 42; // 不是常量表达式string bad[cnt]; // 错误：cnt不是常量表达式 默认情况下，数组的元素被默认初始化。定义数组的时候必须指定数组的类型，不能用auto关键字由初始值的列表推断类型。数组的元素应为对象，因此不存在引用的数组。 显式初始化数组元素可以对数组的元素进行列表初始化，如果没有指明维度，编译器会根据初始值的数量计算并推测出来；若指明了维度，那么初始值的总数量不应该超出指定的大小；如果维度比提供的初始值数量大，则剩下的元素被初始化成默认值。 123456const unsigned sz = 3;int ial[sz] = &#123;0, 1, 2&#125;;int a2[] = &#123;0, 1, 2&#125;;int a3[5] = &#123;0, 1, 2&#125;;string a4[3] = &#123;\"hi\", \"bye\"&#125;;int a5[2] = &#123;0, 1, 2&#125;; //错误 字符数组的特殊性当使用字符串字面值对字符数组初始化（只可以在数组并初始化的时候）时，一定要注意字符串字面值的结尾处还有一个空字符。不能用赋值语句将一个字符串常量或字符数组直接赋给另一个数组。 1234567891011char a1[] = &#123;'C', '+', '+'&#125;; //列表初始化，没有空字符char a2[] = &#123;'C', '+', '+', '\\0'&#125;; //列表初始化，含有显式的空字符char a3[] = \"C++\"; //用字符换字面值初始化，自动添加表示字符串结束的空字符const char a4[6] = \"Daniel\" //错误：没有空间放空字符str1[] = \"China\"; //错误str1 = \"China\"; //错误str2 = str1; //错误//利用二维数组存储多个字符串char weekday[7][11] = &#123;\"Sunday\", \"Monday\",\"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\",\"Ssturday\"&#125;; 不允许拷贝和赋值不能将数组的内容拷贝给其他数组作为初始值，也不能用数组为其他数组赋值。 复杂的数组声明数组能存放大多数类型的对象，可以定义一个存放指针的数组；又因为数组本身是对象，所以允许定义数组的指针及数组的引用。默认情况下，类型修饰符从右向左依次绑定。就数组而言，从数组的名字开始由内向外阅读更容易理解。 12345int *ptrs[10]; //ptrs是含有10个整型指针的数组int &amp;refs[10] = /* ？ */ //错误：不存在引用的数组int (*Parray)[10] = &amp;arr; //Parray是一个指针，指向一个含有10个整数的数组int (&amp;arrRef)[10] = arr; //arrRef是一个引用，引用一个含有10个整数的数组int *(&amp;arry)[10] = ptrs; //arry是一个引用，引用一个含有10个指针的数组 练习3.27 设txt_size是一个无参数的函数，它的返回值是int。下列哪些定义是非法的？为什么？ 12345unsigned buf_size = 1024;int ia[buf_size]; //非法的，因为buf_size不是一个常量表达式int ia[4*7-14]; //正确，因为4*7-14是一个常量表达式int ia[txt_size()]; //非法的，因为txt_size没有被定义为常量表达式 constexprchar st[11] = \"fundamental\" //非法的，因为用字符串字面值初始化，没有空间存放空字符 访问数组元素与标准库类型vector 和string 一样，数组的元素也能使用范围for 语句或下标运算符来访问。数组的索引从0开始。 数组下标通常定义为size_t类型，size_t是一种机器相关的无符号类型，在cstddef头文件中定义。 1234567//以10分为一个分段统计成绩的数量：0~9.10~19，...，90~99,100unsigned scores[11] = &#123;&#125;; //列表初始化，初值为0；若不初始化，在函数内不执行默认初始化。unsigned grade;while (cin &gt;&gt; grade) &#123; if (grade &lt;= 100) ++scores[grade/10];&#125; 与vector 和string 一样，当需要遍历数组的所有元素时，最好的办法是使用范围for语句。 1234//对于scores中的每个计数值输出当前的计数值for (auto i : scores) cout &lt;&lt; i &lt;&lt; \" \";cout&lt;&lt;endl; 必须要检查数组下标的值在合理范围内，下标越界会产生缓冲区溢出。 练习3.31编写一段程序，定义一个含有10个int的数组，令每个元素的值就是其下标值。 123456789101112131415include&lt;iostream&gt;using namespace std;int main()&#123; const int sz = 10; int a[sz]; for(int i = 0; i &lt; sz; i++) a[i] = i; for(auto val: a) cout &lt;&lt; val &lt;&lt; \" \"; cout&lt;&lt;endl; return 0;&#125; 3.32 将上一题创建的数组拷贝给另外一个数组，利用vector重写程序，实现类似的功能。 //如果要把数组的内容拷贝给另外一个数组，不能直接对数值使用赋值运算符，而应该逐一拷贝数组的元素。 1234567891011121314include&lt;iostream&gt;using namespace std;int main()&#123; const int sz = 10; int a[sz], b[sz]; for(int i = 0; i &lt; sz; i++) a[i] = i; for(int j = 0; j &lt; sz: j++) b[j] = a[j]; return 0;&#125; //用vector重写 123456789101112131415161718#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int main()&#123; const int sz = 10; vector&lt;int&gt; vInt, vInt2; for (int i = 0; i &lt; sz; i++) vInt.push_back(i); for (int j = 0; j &lt; sz; j++) vInt2.push_back(vInt[j]); for (auto val: vInt2) cout &lt;&lt; val &lt;&lt; \" \"; cout&lt;&lt;endl; return 0;&#125; 例子：输出100以内的所有素数。 一种思路：让2,3,4,5，…，c中的每个数自我相加多次，来获得100之内的所有合数，筛掉合数之后就得到素数。若n为合数，则n的最小正因数c满足： 1","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"Cpp基础（2）引用与指针","slug":"Cpp基础（2）引用与指针","date":"2020-01-27T03:02:17.000Z","updated":"2020-01-27T04:02:17.000Z","comments":true,"path":"2020/01/27/Cpp基础（2）引用与指针/","link":"","permalink":"http://nekomoon404.github.io/2020/01/27/Cpp%E5%9F%BA%E7%A1%80%EF%BC%882%EF%BC%89%E5%BC%95%E7%94%A8%E4%B8%8E%E6%8C%87%E9%92%88/","excerpt":"","text":"引用与指针复合类型（compound type）是指基于其他类型定义的类型，引用和指针是其中的两种。 一般地，一条声明语句由一个基本数据类型（base type）和紧随其后的一个声明符（declarator）列表组成。 引用当我们使用“引用（reference）”时，指的其实是“左值引用（lvalue reference）”。 引用为对象起了另外一个名字，引用类型引用另外一种类型。通过将声明符写成$d的形式来定义引用类型。在定义引用时，程序把引用和它的初始值绑定（bind）在一起，而不是将初始值拷贝给引用。引用将和它的初始值对象一直绑定在一起，无法令其绑定到另外一个对象。 为引用赋值，实际上是把值赋给了与引用绑定的对象。获取引用的值，实际上是获取了与引用绑定的对象的值。以引用作为初始值，实际上是以引用绑定的对象作为初始值。 允许在一条语句中定义多个引用，其中每个引用标识符都必须以符号&amp;开头。 引用的类型都要和与之绑定的对象严格匹配。引用只能绑定在对象上，而不能与字面值或某个表达式的计算结果绑定在一起。 12345678910int ival = 1024;int &amp;refVal = ival; //refVal指向ivalint &amp;refVal2; //错误。引用必须被初始化refVal = 2; //把值赋给了ivalint &amp;refVal3 = refVal; //正确：refVal3绑定到了那个与refVal绑定的对象上，即ivalint i = refVal; //相当于i = ivalint i = 1024, i2 = 2048;int &amp;r1 = i, &amp;r2 = i2; 指针指针（pointer）是“指向”另外一种类型的复合类型。指针也实现了对对象的间接访问，但与引用相比也有不同： （1）指针本身就是一个对象，允许对指针赋值和拷贝，在指针的生命周期内它可以先后指向几个不同的对象。 （2）指针无须再定义时赋初值。如果没有被初始化，将拥有一个不确定的值。 指针运算符*，取地址符&amp; 指针的类型都要和它所指向的对象严格匹配。因为引用不是对象，没有实际地址，所以不能定义指向引用的指针。 指针变量：专门用于存放指针（某个变量的地址）的变量 1234int c = 76;int *pointer; //定义名字为pointer的指针变量pointer = &amp;c; //将变量c的地址赋值给指针变量pointer；赋值后称指针变量pointer指向了变量c//pointer = c; //错误：因为pointer是存放地址的变量，所以只能存放地址 通过指针变量可以访问“它所指向的变量”。指针变量也是变量，是变量就有地址。 12345int c = 76;int *pointer = &amp;c; //*pointer为“pointer所指向的存储单元的内容\"，即是变量ccout &lt;&lt; &amp;c &lt;&lt;endl; //取变量c的地址cout &lt;&lt; &amp;pointer &lt;&lt; endl; //取指针变量c的地址 赋值和指针：记住赋值永远改变的是等号左侧的对象，就能分清一条赋值语句是改变了指针还是改变了指针所指的对象的值。 1234int i = 42;int *pi = 0; //pi被初始化pi = &amp;ival; //pi所存的地址改变，指向ival*pi = 0; //ival的值被改变，指针pi所存的地址没有改变 &amp;与*`的运算优先级：同级 空指针（null pointer）不指向任何对象。 123int *p1 = nullptr //C++11int *p2 = 0; //直接将p2初始化为字面常量0int *p3 = NULL; //需要首先#include&lt;cstdlib&gt; 使用未经初始化的指针是引发运行时错误的一大原因。因此建议初始化所有的指针，尽量等定义了对象之后再定义指向它的指针。 void*指针：可用于存放任意对象的地址。利用void*指针可以：拿它和别的指针比较，作为函数的输入或输出，或者赋给另外一个void*指针。不能访问其内存空间中所存的对象。 123double obj = 3.14, *pd = &amp;obj;void *pv = &amp;obj;pv = pd;","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"Cpp基础（1）基础语法","slug":"Cpp基础（1）基础语法","date":"2020-01-27T02:59:54.000Z","updated":"2020-01-27T13:59:54.000Z","comments":true,"path":"2020/01/27/Cpp基础（1）基础语法/","link":"","permalink":"http://nekomoon404.github.io/2020/01/27/Cpp%E5%9F%BA%E7%A1%80%EF%BC%881%EF%BC%89%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/","excerpt":"","text":"基础语法Hello world1234567891011121314#include&lt;iostream&gt;using namespace std;int main() &#123; cout &lt;&lt; \"Hello world\" &lt;&lt; endl; //变量创建的语法： 数据类型 变量名 = 变量初始值 int a = 10; cout &lt;&lt; \"a=\" &lt;&lt; a &lt;&lt; endl; system(\"pause\"); return 0;&#125; 变量创建的语法： 数据类型 变量名 = 变量初始值 定义常量的两种方法： #define 宏常量名 常量值 const 修饰的变量 const 数据类型 常量名 = 常量值 定义变量或常量时不要用C++中已经使用的关键字 C++中给标识符（变量，常量）命名时，要注意：1.标识符不可以是关键字2.标识符只能由字母，数字，下划线组成3.标识符的第一个字母只能是字母或者下划线4.标识符中区分大小写5.给变量命名时最好能做到见名知意，方便阅读 数据类型数据类型存在的意义：给不同类型的变量分配合适的内存空间 整型的几种类型：1.短整型 short 2字节 -2^15 - 2^15-12.整型 int 4字节 -2^31 - 2^31-1 //int最常用3.长整型 long 4字节 -2^31 - 2^31-14.长长整型 long long 8字节 -2^63 - 2^63-1 sizeof 关键字可以得到数据类型所占的内存的大小语法： sizeof（数据类型/变量名） 实型/浮点型：用于表示小数1.单精度 float 4字节 7位有效数字2.双精度 double 8字节 15-16位有效数字 字符型：用于表示单个字符 只占用1个字节，将对应的ASCII码放入存储单元语法：char ch = &#39;a&#39;需要注意：1.要用单引号将字符括起来2.单引号内只能有一个字符，不可以是字符串 字符串型：用于表示一串字符两种风格： C风格字符串： char 变量名[] = “字符串值” 要用双引号 C++风格字符串： string 变量名 = “字符串值” 需要加入头文件 #include&lt;string&gt; 转义字符： 用于表示一些不能显示出来的ASCII字符常用的转义字符：/n 换行，将当前位置移到下一行开头 /t 水平制表，跳到下一个TAB位置，/t和其前面的内容一共占8个字符 // 代表一个反斜杠字符 布尔类型bool：代表真或假的值 占用1个字节bool类型只有两个值：true（本质是1） false（本质是0） 数据的输入：用于从键盘获取数据语法：cin &gt;&gt; 变量 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#include&lt;iostream&gt;#include&lt;iomanip&gt;#include&lt;string&gt;using namespace std;int main1() &#123; short num1 = 10; cout &lt;&lt; \"short所占的内存空间为\" &lt;&lt; sizeof(num1) &lt;&lt; endl; float f1 = 3.14f; //编译器会把小数默认为双精度数，所以要在3.14后加上f cout &lt;&lt; \"f1=\" &lt;&lt; f1 &lt;&lt; endl; double d1 = 3.1415926; cout &lt;&lt; \"d1=\" &lt;&lt; d1 &lt;&lt; endl; //显示小数默认是6位，若要改变可用&lt;iomanip&gt;库中的 setprecision( ) const double value = 12.3456789; cout &lt;&lt; value &lt;&lt; endl; // 默认以6精度，所以输出为 12.3457 cout &lt;&lt; setprecision(4) &lt;&lt; value &lt;&lt; endl; // 改成4精度，所以输出为12.35 cout &lt;&lt; setprecision(8) &lt;&lt; value &lt;&lt; endl; // 改成8精度，所以输出为12.345679 cout &lt;&lt; fixed &lt;&lt; setprecision(4) &lt;&lt; value &lt;&lt; endl; // 加了fixed意味着是固定点方式显示，所以这里的精度指的是小数位，输出为12.3457 cout &lt;&lt; value &lt;&lt; endl; // fixed和setprecision的作用还在，依然显示12.3457 cout.unsetf(ios::fixed); // 去掉了fixed，所以精度恢复成整个数值的有效位数，显示为12.35 cout &lt;&lt; value &lt;&lt; endl; cout.precision(6); // 恢复成原来的样子，输出为12.3457 cout &lt;&lt; value &lt;&lt; endl; //科学计数法 float f2 = 3e2; cout &lt;&lt; \"f2=\" &lt;&lt; f2 &lt;&lt; endl; //查看字符型变量对应的ASCII码 a-97 A-65 char ch = 'a'; cout &lt;&lt; (int)ch &lt;&lt; endl; ch = 97; cout &lt;&lt; ch &lt;&lt; endl; //字符串 char str1[] = \"hello world\"; cout &lt;&lt; str1 &lt;&lt; endl; string str2 = \"hello world\"; cout &lt;&lt; str2 &lt;&lt; endl; //布尔类型 bool flag = true; cout &lt;&lt; flag &lt;&lt; endl; flag = false; cout &lt;&lt; flag &lt;&lt; endl; //数据的输入 string str; cout &lt;&lt; \"请输入字符串变量：\" &lt;&lt; endl; cin &gt;&gt; str; cout &lt;&lt; str &lt;&lt; endl; system(\"pause\"); return 0;&#125; 运算符算术运算符：加+ 减- 乘* 除/ 取模（取余）% 只有整型变量可以进行取模运算需要注意：1.进行运算的变量类型，如两个整型相除结果仍是整数2.除数不能为0，取模运算除数也不能为03.只有整型变量可以进行取模运算，小数不可以 赋值运算符：用于将表达式的值赋给变量赋值= 加等于+= 减等于-= 乘等于*= 除等于/= 模等于%= 比较运算符：用于表达式的比较，并返回一个真值或假值相等于== 不等于!= 小于&lt; 大于&gt; 小于等于&lt;= 大于等于&gt;= 逻辑运算符：用于根据表达式的值返回真值或假值非! 与&amp;&amp; 或|| 要注意运算符的优先级 1234567891011121314151617181920212223242526272829303132333435363738#include&lt;iostream&gt;using namespace std;int main2() &#123; //后置递增 int a = 10; a++; //等价于a = a + 1 cout &lt;&lt; a &lt;&lt; endl; // 11 //前置递增 int b = 10; ++b; cout &lt;&lt; b &lt;&lt; endl; // 11 //区别 //前置递增先对变量进行++，再计算表达式 int a2 = 10; int b2 = ++a2 * 10; cout &lt;&lt; b2 &lt;&lt; endl; // a2=11, b2=110 //后置递增先计算表达式，后对变量进行++ int a3 = 10; int b3 = a3++ * 10; cout &lt;&lt; b3 &lt;&lt; endl; // a3=11, b3=100 //比较运算符 a = 10; b = 20; cout &lt;&lt; (a == b) &lt;&lt; endl; // 0 cout &lt;&lt; (a != b) &lt;&lt; endl; // 1 cout &lt;&lt; (a &gt; b) &lt;&lt; endl; // 0 cout &lt;&lt; (a &lt; b) &lt;&lt; endl; // 1 cout &lt;&lt; (a &gt;= b) &lt;&lt; endl; // 0 cout &lt;&lt; (a &lt;= b) &lt;&lt; endl; // 1 system(\"pause\"); return 0;&#125; 程序流程结构C++支持的三种基本程序运行结构：顺序结构，选择结构，循环结构1.顺序结构：程序按顺序执行，不发生跳转2.选择结构：依据条件是否满足，有选择的执行相应功能3.循环结构：依据条件是否满足，循环多次执行某段代码 选择结构if语句：1.单行格式if语句：if (条件) { 条件满足执行的语句}2.多行格式if语句：if (条件) { 条件满足执行的语句} else{条件不满足执行的语句}3.多条件的if语句：if (条件1) {条件1满足执行的语句} else if(条件2) {条件2满足执行的语句} …….else {条件都不满足执行的语句}4.嵌套if语句注意：if条件后面不要加分号 三目运算符：实现简单的判断语法：表达式1 ? 表达式2 : 表达式3解释：如果表达式1的值为真，执行表达式2，并返回表达式2的结果 如果表达式1的值为假，执行表达式3，并返回表达式3的结果注意：C++中三目运算符返回的是变量，可以继续赋值 switch语句：执行多条件分支语句语法：switch(表达式){ case 结果1：执行语句;break; case 结果2：执行语句;break; … default:执行语句;break;}注意：1. switch语句中的表达式的数据类型只能是整型或字符型 2. case里如果没有break，那么程序会一直向下执行 3.对于多条件判断，switch的结构清晰，执行效率高 循环结构while循环结构：满足循环条件，执行循环结构 语法：while (循环条件) {循环结构} 注意：在执行循环语句时，程序必须提供跳出循环的窗口，否则出现死循环 do … while循环语句：满足循环条件，执行循环语句 语法：do {循环语句} while (循环条件) 注意：do…while先执行一次循环语句，再判断循环条件 for 循环语句：满足循环条件 语法： for (起始表达式; 条件表达式; 末尾循环体) {循环语句} 注意： 先判断条件表达式，然后执行循环语句，然后再执行末尾循环体 嵌套循环 跳转语句break 语句：用于跳出选择结构或者循环结构 使用的时机：出现在switch条件语句中，作用是终止case或者跳出switch 出现在循环语句中，作用是跳出当前的循环语句 出现在嵌套循环中，跳出最近的内层循环结构 continue语句：在循环语句中，跳过本次循环中余下尚未执行的语句，继续执行下一次循环 注意：continue不会使循环终止，而break会跳出循环 goto语句：可以无条件跳转语句 语法：goto 标记 如果标记的名称存在，执行到goto语句时，会跳转到标记的位置 注意：在程序中不建议使用goto语句，以免造成程序流程混乱 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include&lt;iostream&gt;using namespace std;int main()&#123; //嵌套if语句 int score = 0; cout &lt;&lt; \"请输入考试分数：\" &lt;&lt; endl; cin &gt;&gt; score; if (score &gt; 600) &#123; cout &lt;&lt; \"我考上了一本大学\" &lt;&lt; endl; if (score &gt; 700) &#123; cout &lt;&lt; \"我考上了北大\" &lt;&lt; endl; &#125; else if (score &gt; 650) &#123; cout &lt;&lt; \"我考上了清华\" &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; \"我考上了人大\" &lt;&lt; endl; &#125; &#125; else if (score &gt; 500) &#123; cout &lt;&lt; \"我考上了二本大学\" &lt;&lt; endl; &#125; else if (score &gt; 400) &#123; cout &lt;&lt; \"我考上了三本大学\" &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; \"我未考上本科\" &lt;&lt; endl; &#125; //三目运算符 int a = 10, b = 20, c = 0; c = a &gt; b ? a : b; cout &lt;&lt; \"c = \" &lt;&lt; c &lt;&lt; endl; //C++中三目运算符返回的是变量,可以继续赋值 (a &gt; b ? a : b) = 100; //b=100 cout &lt;&lt; \"a = \" &lt;&lt; a &lt;&lt; endl; //a=10 cout &lt;&lt; \"b = \" &lt;&lt; b &lt;&lt; endl; //b=100 cout &lt;&lt; \"c = \" &lt;&lt; c &lt;&lt; endl; //嵌套循环体 //外层循环执行1次，内层循环执行1轮 for (int i = 0; i &lt; 10; i++) &#123; for (int j = 0; j &lt; 10; j++) &#123; cout &lt;&lt; \"*\" &lt;&lt; \" \"; &#125; cout &lt;&lt; endl; &#125; //continue语句 for (int i = 0; i &lt; 100; i++)&#123; if (i % 2 == 0) continue; cout &lt;&lt; i &lt;&lt; endl; &#125; system(\"pause\"); return 0;&#125;","categories":[{"name":"C++基础","slug":"C-基础","permalink":"http://nekomoon404.github.io/categories/C-%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"搭建自己的博客","slug":"搭建自己的博客","date":"2020-01-18T03:08:31.000Z","updated":"2020-01-18T05:42:11.186Z","comments":true,"path":"2020/01/18/搭建自己的博客/","link":"","permalink":"http://nekomoon404.github.io/2020/01/18/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2/","excerpt":"","text":"1.安装GitGIt是目前世界上最先进的分布式版本控制系统，其开发者就是Linux的作者Linus Torvalds。可以到Git的官网下载：https://gitforwindows.org/，在安装选项设置页面中勾选Git Bash Here和Git GUI Here。 2.安装node.js在官网选择LTS版本下载安装即可：https://nodejs.org/en/download/。 3.安装Hexo在本地磁盘中建立一个文件夹来存储博客的本地文件，如D:\\blog。进入blog文件夹，单机右键，菜单中选择Git Bash Here。在命令框中依次执行以下命令安装Hexo。 1npm install -g hexo-cli 初始化博客 1hexo init 启动服务：依次执行以下命令 12345npm stallhexo shexo chexo ghexo d 浏览器打开本地访问页面 ，使用ctrl+c可关闭服务。若不能访问可尝试用以下命令解决： 1npm install hexo-deployer-git --save 之后每次更新博客内容后都要执行 hexo c —&gt;hexo g —&gt;hexo d，来重新部署。 4.将Hexo部署到GitHub在GitHub中创建一个新的repository，repository name必须是 用户名.github.io 的格式 回到Git Bash中，部署用户名和邮箱，其中yourname输入GitHub用户名，youremail输入GitHub的登陆邮箱 12git config --global user.name \"yourname\"git config --global user.email \"youremail\" 创建SSH 1ssh-keygen -t rsa -C \"youremail\" 打开提示已经创建好的.ssh文件夹，其中id_rsa是私人密钥，id_rsa.pub是公共密钥。在GitHub中打开setting —&gt; New SSH key，将id_rsa.pub里面的信息复制进去。 在blog文件夹中打开博客配置文件_config.yml，将最后几行修改为： 1234deploy: type: git repo: https://github.com/yourname/yourname.github.io.git branch: master 安装deploy-git 1npm install hexo-deployer-git --save 最后重新部署博客 123hexo chexo ghexo d 这样就可以在 http://yourname.github.io 访问自己的博客了。 5.设置主题我安装的是Next主题，网上有很多教程可以参考，我直接安装最新的v6+版本。在blog文件夹下右键GIt Bash Here，下载主题： 1git clone https://github.com/theme-next/hexo-theme-next themes/next 打开blog目录下的博客站点配置文件_config.yml，在开头的Site中添加自己博客的信息，注意将language改为zh-CN： 123456title: # 标题subtitle: # 副标题description: # 站点描述author: # 作者language: zh-CNtimezone: 往下拉，将theme改为next： 1theme: next 重新部署博客即可看到主题已经替换。 接着就是博客的一些具体细节方面的设置，这一部分有很多东西可以学习，我目前只学了一些基础操作。 设置scheme在\\blog\\themes\\next文件夹中打开主题配置文件_config.yml，找到外观scheme，想使用哪一种就把前面的注释符#去掉，我选用的是Mist。 1234#scheme: Musescheme: Mist#scheme: Pisces#scheme: Gemini 设置菜单在主题配置文件_config.yml中找到菜单menu，将需要的项前的#去掉，我保留了about关于，categories分类，tags标签，archives归档。 123456789menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 创建菜单需要的页面： 123hexo new page abouthexo new page tagshexo new page categories 在\\blog\\source下就会生成对应的文件夹，打开其中的index.md文件，将type设置为相应的内容，以about为例： 1234title: aboutdate: 2020-01-17 20:16:07type: \"about\" comments: false 设置背景动画在主题配置文件_config.yml中找到Animation Settings，我使用的是Canvas-nest，将Canvas-nest下的enable状态改为true即可： 123canvas_nest: enable: true onmobile: true # Display on mobile or not —————————————————————————— 到这里个人博客就算搭建起来了，但还是比较简陋，之后可以学习将博客完善美化起来。这也是我在这里写下的第一篇文章，使用Typora编辑。 2020年希望自己能成为一个自律的人，也希望这个博客不会只是自己一时兴起的产物。","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-01-12T02:15:57.000Z","updated":"2020-01-12T03:15:57.000Z","comments":true,"path":"2020/01/12/hello-world/","link":"","permalink":"http://nekomoon404.github.io/2020/01/12/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}