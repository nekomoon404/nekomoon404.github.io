<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://nekomoon404.github.io').hostname,
    root: '/',
    scheme: 'Muse',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="Unsupervised learning（无监督学习）大致可以分为Clustering（聚类），Dimension Reduction（降维）和Generation。本文先简单介绍了Clustering，然后主要介绍了Dimension Redunction中常用的一种方法PCA(Principe Component Analysis，主成分分析）。">
<meta property="og:type" content="article">
<meta property="og:title" content="ML笔记（11）Unsupervised Learning-PCA">
<meta property="og:url" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/index.html">
<meta property="og:site_name" content="nekomoon的个人小站">
<meta property="og:description" content="Unsupervised learning（无监督学习）大致可以分为Clustering（聚类），Dimension Reduction（降维）和Generation。本文先简单介绍了Clustering，然后主要介绍了Dimension Redunction中常用的一种方法PCA(Principe Component Analysis，主成分分析）。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725214439.png">
<meta property="og:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725214453.png">
<meta property="og:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725214456.png">
<meta property="og:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725214500.png">
<meta property="og:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725214449.png">
<meta property="og:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725214503.png">
<meta property="og:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725214506.png">
<meta property="og:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725224416.png">
<meta property="og:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725224420.png">
<meta property="og:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725224424.png">
<meta property="og:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725224427.png">
<meta property="og:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725224431.png">
<meta property="article:published_time" content="2020-07-25T13:24:52.000Z">
<meta property="article:modified_time" content="2020-07-25T15:24:51.306Z">
<meta property="article:author" content="nekomoon">
<meta property="article:tag" content="K-means">
<meta property="article:tag" content="HAC">
<meta property="article:tag" content="PCA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ%E5%9B%BE%E7%89%8720200725214439.png">

<link rel="canonical" href="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>ML笔记（11）Unsupervised Learning-PCA | nekomoon的个人小站</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="nekomoon的个人小站" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">nekomoon的个人小站</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://nekomoon404.github.io/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="nekomoon">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="nekomoon的个人小站">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ML笔记（11）Unsupervised Learning-PCA
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-07-25 21:24:52 / 修改时间：23:24:51" itemprop="dateCreated datePublished" datetime="2020-07-25T21:24:52+08:00">2020-07-25</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.3k</span>
            </span>
            <div class="post-description">Unsupervised learning（无监督学习）大致可以分为Clustering（聚类），Dimension Reduction（降维）和Generation。本文先简单介绍了Clustering，然后主要介绍了Dimension Redunction中常用的一种方法PCA(Principe Component Analysis，主成分分析）。</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="1-Unsupervised-Learning"><a href="#1-Unsupervised-Learning" class="headerlink" title="1.Unsupervised Learning"></a>1.Unsupervised Learning</h3><p>无监督学习(Unsupervised Learning)可以做的事大致分为两种：</p>
<ul>
<li>“化繁为简”<ul>
<li>聚类(Clustering)</li>
<li>降维(Dimension Reduction)</li>
</ul>
</li>
<li>“无中生有”：Generation</li>
</ul>
<p>对于无监督学习(Unsupervised Learning)来说，我们通常只会拥有$(x,\hat y)$中的$x$或$\hat y$，其中：</p>
<ul>
<li><strong>化繁为简</strong>就是把复杂的input变成比较简单的output，比如把一大堆没有打上label的树图片转变为一棵抽象的树，此时training data只有input $x$，而没有output $\hat y$；</li>
<li><strong>无中生有</strong>就是随机给function一个数字，它就会生成不同的图像，此时training data没有input $x$，而只有output $\hat y$。</li>
</ul>
<p><img src="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ图片20200725214439.png" style="zoom:67%;"></p>
<p>下面我们先简单介绍下Clustering，然后focus在dimension reduction上，而且只focus在linear dimension reduction上。</p>
<h3 id="2-Clustering"><a href="#2-Clustering" class="headerlink" title="2. Clustering"></a>2. Clustering</h3><p>Clustering（聚类），顾名思义，就是把相近的样本划分为同一类，比如对下面这些没有标签的image进行分类，手动打上cluster 1、cluster 2、cluster 3的标签，这个分类过程就是化繁为简的过程。那有一个很critical的问题：我们到底要分几个cluster？</p>
<p><img src="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ图片20200725214453.png" style="zoom:67%;"></p>
<h4 id="2-1-K-means"><a href="#2-1-K-means" class="headerlink" title="2.1. K-means"></a>2.1. K-means</h4><p>最常用的聚类方法是<strong>K-means</strong>：</p>
<ul>
<li>我们有一大堆的unlabeled data $\{x^1,…,x^n,…,x^N\}$，我们要把它划分为K个cluster；</li>
<li>对每个cluster都要找一个center $c^i,i\in \{1,2,…,K\}$，initial的时候可以从training data里随机挑K个object $x^n$出来作为K个center $c^i$的初始值；</li>
<li>遍历所有的object $x^n$，并判断它属于哪一个cluster，如果$x^n$与第i个cluster的center $c^i$最接近，那它就属于该cluster，我们用$b_i^n=1$来表示第n个object属于第i个cluster，$b_i^n=0$表示不属于；</li>
<li>更新center：把每个cluster里的所有object取平均值作为新的center值，即$c^i=\sum\limits_{x^n}b_i^n x^n/\sum\limits_{x^n} b_i^n$；</li>
<li>重复进行以上的操作；</li>
</ul>
<p>注：如果不是从原先的data set里取center的初始值，可能会导致部分cluster没有样本点。</p>
<h4 id="2-2-HAC"><a href="#2-2-HAC" class="headerlink" title="2.2. HAC"></a>2.2. HAC</h4><p><strong>HAC（Hierarchical Agglomerative Clustering，层次聚类）</strong>。假设现在我们有5个样本点，想要做clustering：</p>
<ul>
<li><p>build a tree:</p>
<p>（整个过程类似建立<a href="https://www.jianshu.com/p/0b476f861bdc" target="_blank" rel="noopener">Huffman Tree</a>，只不过Huffman是依据词频，而HAC是依据相似度建树）</p>
<ul>
<li>对5个样本点两两计算相似度，挑出最相似的一对，比如样本点1和2；</li>
<li>将样本点1和2进行merge (可以对两个vector取平均)，生成代表这两个样本点的新结点；</li>
<li>此时只剩下4个结点，再重复上述步骤进行样本点的合并，直到只剩下一个root结点。</li>
</ul>
</li>
<li><p>pick a threshold：</p>
<p>选取阈值，形象来说就是在构造好的tree上横着切一刀，相连的叶结点属于同一个cluster；</p>
<p>下图中，不同颜色的横线和叶结点上不同颜色的方框对应着切法与cluster的分法，比如按红色的线切就会分成2个cluster，按蓝色的线切就会分成3个cluster。</p>
</li>
</ul>
<p><img src="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ图片20200725214456.png" style="zoom:67%;"></p>
<p>HAC和K-means最大的区别在于如何决定cluster的数量，在K-means里，K的值是要你直接决定的；而在HAC里，你并不需要直接决定分多少cluster，而是去决定“这一刀切在树的哪里”</p>
<h3 id="3-Dimension-Reduction"><a href="#3-Dimension-Reduction" class="headerlink" title="3. Dimension Reduction"></a>3. Dimension Reduction</h3><p>clustering的缺点是<strong>以偏概全</strong>，它强迫每个object都要属于某个cluster。但实际上某个object可能拥有多种属性，或者多个cluster的特征，如果把它强制归为某个cluster，就会失去很多信息；或许我们应该用一个vector来描述该object，这个vector的每一维都代表object的某种属性（这个vector的dimension肯定要比object原来的feature数目少），这种做法就叫做<strong>Distributed Representation</strong>。如果原先的object是high dimension的，比如image，那现在用计算出来的它的某些attribute（属性，特征）来描述它，就可以使之从高维空间转变为低维空间，这就是所谓的<strong>降维(Dimension Reduction)</strong>。Distribution Representation和Dimension Reduction其实是一样的事情，只是叫法不同。</p>
<p>比如哟下图中动漫“全职猎人”中小杰的念能力分布，从表中可以看出我们不能仅仅把他归为强化系，而应该要用一个vector来表示他的attribute。</p>
<p><img src="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ图片20200725214500.png" style="zoom:67%;"></p>
<h4 id="3-1-Why-Dimension-Reduction-Help"><a href="#3-1-Why-Dimension-Reduction-Help" class="headerlink" title="3.1 Why Dimension Reduction Help?"></a>3.1 Why Dimension Reduction Help?</h4><p>接下来我们从另一个角度来看为什么Dimension Reduction可能是有用的。假设data为下图左侧中的3D螺旋式分布，你会发现用3D的空间来描述这些data其实是很浪费的，因为我们完全可以把这个卷摊平，此时只需要用2D的空间就可以描述这个3D的信息。</p>
<p><img src="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ图片20200725214449.png" style="zoom: 80%;"></p>
<p>如果以MNIST(手写数字集)为例，每一张image都有28*28的dimension，但我们反过来想，大多数28*28 dimension的vector转成image，看起来都不会像是一个数字，所以描述数字所需要的dimension可能远比28*28要来得少。</p>
<p>举一个极端的例子，下面这几张表示“3”的image，我们完全可以用中间这张image旋转$\theta$角度来描述，也就是说，我们只需要用$\theta$这一个dimension就可以描述原先28*28 dimension的图像。你只要抓住角度的变化就可以知道28维空间中的变化，这里的28*28维pixel就是之前提到的樊一翁的胡子，而1维的角度则是他的头，也就是“去芜存菁，化繁为简”的思想</p>
<p><img src="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ图片20200725214503.png" style="zoom:67%;"></p>
<h4 id="3-2-How-to-do-Dimension-Reduction？"><a href="#3-2-How-to-do-Dimension-Reduction？" class="headerlink" title="3.2. How to do Dimension Reduction？"></a>3.2. How to do Dimension Reduction？</h4><p>那怎么去做Dimension Reduction呢，在Dimension Reduction里，我们要找一个function，这个function的input是原始的$x$，output是经过降维之后的$z$。最简单的方法是<strong>Feature Selection</strong>，即直接从原有的dimension里删掉一些直观上就对结果没有影响的dimension，就做到了降维，比如下图中从$x_1,x_2$两个维度中直接拿掉$x_1$；但这个方法不总是有用，因为很多情况下任何一个dimension其实都不能被拿掉，就像下图中的螺旋卷。</p>
<p><img src="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ图片20200725214506.png" style="zoom: 67%;"></p>
<h3 id="4-PCA"><a href="#4-PCA" class="headerlink" title="4. PCA"></a>4. PCA</h3><p>另一个常见的方法叫做<strong>PCA</strong>（Principe Component Analysis，主成分分析）。PCA认为降维就是一个很简单的linear function，它的input x和output z之间是linear transform，即$z=Wx$，PCA要做的，就是根据training data的$x$<strong>把W给找出来</strong>。</p>
<h4 id="4-1-PCA-for-1-Dimension"><a href="#4-1-PCA-for-1-Dimension" class="headerlink" title="4.1 PCA for 1-Dimension"></a>4.1 PCA for 1-Dimension</h4><p>我们先考虑一个简单的case，假设$z$是1维的vector，也就是把$x$投影到一维空间，此时$W$是一个row vector。$z_1=w^1\cdot x$，其中$w^1$表示$w$的第一个row vector，我们<strong>假设$w^1$的长度为1，即$||w^1||_2=1$，此时$z_1$就是$x$在$w^1$方向上的投影</strong>。</p>
<p><img src="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ图片20200725224416.png" style="zoom:67%;"></p>
<p>那我们到底要找什么样的$w^1$呢？假设我们现在已有的宝可梦样本点分布如下，横坐标代表宝可梦的攻击力，纵坐标代表防御力，我们的任务是把这个二维分布投影到一维空间上。我们希望选这样一个$w^1$，它使得$x$经过投影之后得到的$z_1$分布越大越好，也就是说经过这个投影后，不同样本点之间的区别，应该仍然是可以被看得出来的。即：</p>
<ul>
<li><p>我们希望找一个projection（投影）的方向，它可以让$x$经过projection后的variance越大越好；</p>
</li>
<li><p>我们不希望projection使这些data point通通挤在一起，导致点与点之间的奇异度消失；</p>
</li>
<li>要去maximize的对象是$z_1$的variance，其中variance的计算公式：$Var(z_1)=\frac{1}{N}\sum\limits_{z_1}(z_1-\bar{z_1})^2, ||w^1||_2=1$，$\bar {z_1}$是$z_1$的平均值。</li>
</ul>
<p>如下图给出了所有样本点在两个不同的方向上投影之后的variance比较情况，从这个图上，你可以看出$w^1$或许是代表宝可梦的强度，宝可梦可能有一个隐藏的factor代表它的强度，这个隐藏的factor同时影响了它的防御力跟攻击力，所以防御力跟攻击力是会同时上升的。</p>
<h4 id="4-2-PCA-for-n-D"><a href="#4-2-PCA-for-n-D" class="headerlink" title="4.2 PCA for n-D"></a>4.2 PCA for n-D</h4><p>当然我们不可能只投影到一维空间，我们还可以投影到更高维的空间</p>
<p>对$z=Wx$来说：</p>
<ul>
<li>$z_1=w^1\cdot x$，表示$x$在$w^1$方向上的投影</li>
<li>$z_2=w^2\cdot x$，表示$x$在$w^2$方向上的投影</li>
<li>…</li>
</ul>
<p>$z_1,z_2,…$串起来就得到$z$，而$w^1,w^2,…$分别是$W$的第1,2,…个row，需要注意的是，<strong>这里的$w^i$必须相互正交，此时$W$是正交矩阵(orthogonal matrix)</strong>，如果不加以约束，则算出来的的$w^1,w^2,…$实际上是相同的值 。</p>
<p><img src="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ图片20200725224420.png" style="zoom:67%;"></p>
<h4 id="4-3-Lagrange-multiplier"><a href="#4-3-Lagrange-multiplier" class="headerlink" title="4.3. Lagrange multiplier"></a>4.3. Lagrange multiplier</h4><p>求解PCA，实际上已经有现成的函数可以调用，此外你也可以把PCA描述成neural network，然后用gradient descent的方法来求解，这里主要介绍用<strong>Lagrange multiplier（拉格朗日乘数法）</strong>求解PCA的数学推导过程。</p>
<p>（注：$w^i$和$x$均为列向量，下文中类似$w^i\cdot x$表示的是矢量内积，而$(w^i)^T\cdot x$表示的是矩阵相乘。）</p>
<p><strong>Step1: calculate $w^1$</strong></p>
<p>目标：maximize $(w^1)^TSw^1 $，条件：$(w^1)^Tw^1=1$</p>
<ul>
<li><p>首先计算出$\bar{z_1}$：</p>
<script type="math/tex; mode=display">
\begin{split}
&z_1=w^1\cdot x\\
&\bar{z_1}=\frac{1}{N}\sum z_1=\frac{1}{N}\sum w^1\cdot x=w^1\cdot \frac{1}{N}\sum x=w^1\cdot \bar x
\end{split}</script></li>
<li><p>然后计算maximize的对象$Var(z_1)$：</p>
<p>（其中$Cov(x)=\frac{1}{N}\sum(x-\bar x)(x-\bar x)^T$）</p>
<script type="math/tex; mode=display">
\begin{split}
Var(z_1)&=\frac{1}{N}\sum\limits_{z_1} (z_1-\bar{z_1})^2\\
&=\frac{1}{N}\sum\limits_{x} (w^1\cdot x-w^1\cdot \bar x)^2\\
&=\frac{1}{N}\sum (w^1\cdot (x-\bar x))^2\\
&=\frac{1}{N}\sum(w^1)^T(x-\bar x)(x-\bar x)^T w^1\\
&=(w^1)^T\frac{1}{N}\sum(x-\bar x)(x-\bar x)^T w^1\\
&=(w^1)^T Cov(x)w^1
\end{split}</script></li>
<li><p>当然这里想要求$Var(z_1)=(w^1)^TCov(x)w^1$的最大值，还要加上$||w^1||_2=(w^1)^Tw^1=1$的约束条件，否则$w^1$可以取无穷大。</p>
</li>
<li><p>令$S=Cov(x)$，它是：</p>
<ul>
<li>对称的(symmetric)</li>
<li>半正定的(positive-semidefine)，即所有特征值(eigenvalues)是非负的(non-negative)</li>
</ul>
<p>（看来是要复习一下研一上学的矩阵理论了(￣ω￣;)）</p>
</li>
<li><p>使用拉格朗日乘数法，利用目标和约束条件构造函数：</p>
<script type="math/tex; mode=display">
g(w^1)=(w^1)^TSw^1-\alpha((w^1)^Tw^1-1)</script></li>
<li><p>对$w^1$这个vector里的每一个element做偏微分：</p>
<script type="math/tex; mode=display">
\partial g(w^1)/\partial w_1^1=0\\
\partial g(w^1)/\partial w_2^1=0\\
\partial g(w^1)/\partial w_3^1=0\\
...</script></li>
<li><p>整理上述推导式，可以得到：</p>
<p>其中，$w^1$是$S$的特征向量(eigenvector)</p>
<script type="math/tex; mode=display">
Sw^1=\alpha w^1</script></li>
<li><p>注意到满足$(w^1)^Tw^1=1$的特征向量$w^1$有很多，我们要找的是可以maximize $(w^1)^TSw^1$的那一个，于是利用上一个式子：</p>
<script type="math/tex; mode=display">
(w^1)^TSw^1=(w^1)^T \alpha w^1=\alpha (w^1)^T w^1=\alpha</script></li>
<li><p>此时maximize $(w^1)^TSw^1$就变成了maximize $\alpha$，也就是$S$的最大的特征值$\alpha$对应的那个特征向量，就是我们要找的$w^1$</p>
</li>
<li><p>结论：<strong>$w^1$是$S=Cov(x)$这个matrix中的最大的特征值$\lambda_1$对应的特征向量</strong></p>
</li>
</ul>
<p><img src="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ图片20200725224424.png" style="zoom:67%;"></p>
<p><strong>Step2: calculate $w^2$</strong></p>
<p>在推导$w^2$时，相较于$w^1$，多了一个限制条件：$w^2$必须与$w^1$正交(orthogonal)。</p>
<p>目标：maximize $(w^2)^TSw^2$，条件：$(w^2)^Tw^2=1,(w^2)^Tw^1=0$</p>
<p>结论：<strong>$w^2$也是$S=Cov(x)$这个matrix第二大的特征值$\lambda_2$对应的特征向量</strong></p>
<ul>
<li><p>同样是用拉格朗日乘数法求解，先写一个关于$w^2$的function，包含要maximize的对象，以及两个约束条件</p>
<script type="math/tex; mode=display">
g(w^2)=(w^2)^TSw^2-\alpha((w^2)^Tw^2-1)-\beta((w^2)^Tw^1-0)</script></li>
<li><p>对$w^2$的每个element做偏微分：</p>
<script type="math/tex; mode=display">
\partial g(w^2)/\partial w_1^2=0\\
\partial g(w^2)/\partial w_2^2=0\\
\partial g(w^2)/\partial w_3^2=0\\
...</script></li>
<li><p>整理后得到：</p>
<script type="math/tex; mode=display">
Sw^2-\alpha w^2-\beta w^1=0</script></li>
<li><p>上式两侧同乘$(w^1)^T$，得到：</p>
<script type="math/tex; mode=display">
(w^1)^TSw^2-\alpha (w^1)^Tw^2-\beta (w^1)^Tw^1=0</script></li>
<li><p>其中$\alpha (w^1)^Tw^2=0,\beta (w^1)^Tw^1=\beta$，</p>
<p>而由于$(w^1)^TSw^2$是vector×matrix×vector=scalar，因此在外面套一个transpose（转置）不会改变其值，因此该部分可以转化为：</p>
<p>（注：$S$是symmetric matrix（对称矩阵）的，既有$S^T=S$。）</p>
<script type="math/tex; mode=display">
\begin{split}
(w^1)^TSw^2&=((w^1)^TSw^2)^T\\
&=(w^2)^TS^Tw^1\\
&=(w^2)^TSw^1
\end{split}</script><p>我们已经知道$w^1$满足$Sw^1=\lambda_1 w^1$，代入上式：</p>
<script type="math/tex; mode=display">
\begin{split}
(w^1)^TSw^2&=(w^2)^TSw^1\\
&=\lambda_1(w^2)^Tw^1\\
&=0
\end{split}</script></li>
<li><p>因此有$(w^1)^TSw^2=0$，$\alpha (w^1)^Tw^2=0$，$\beta (w^1)^Tw^1=\beta$，又根据</p>
<script type="math/tex; mode=display">
(w^1)^TSw^2-\alpha (w^1)^Tw^2-\beta (w^1)^Tw^1=0</script><p>可以推得$\beta=0$</p>
</li>
<li><p>此时$Sw^2-\alpha w^2-\beta w^1=0$就转变成了$Sw^2-\alpha w^2=0$，即</p>
<script type="math/tex; mode=display">
Sw^2=\alpha w^2</script></li>
<li><p>由于$S$是symmetric的，因此在不与$w_1$冲突的情况下，这里$\alpha$选取第二大的特征值$\lambda_2$时，可以使$(w^2)^TSw^2$最大</p>
</li>
<li><p>结论：<strong>$w^2$也是$S=Cov(x)$这个matrix中的特征向量，对应第二大的特征值$\lambda_2$</strong></p>
</li>
</ul>
<p>（实对称矩阵的不同特征值对应的特征向量是正交的，这个在线性代数或者矩阵理论课程中都有讲过，或者可以参考<a href="https://ccjou.wordpress.com/2011/02/09/實對稱矩陣可正交對角化的證明/" target="_blank" rel="noopener">實對稱矩陣可正交對角化的證明</a>，这是我上学期末在复习矩阵理论的时候发现了一个台湾线代老师的博客，里面有非常多讲解线代知识的文章，思路和内容都要比SJTU用的教材好太多，简直救我期末于水火ヾ(ｏ･ω･)ﾉ）</p>
<p><img src="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ图片20200725224427.png" style="zoom:67%;"></p>
<h4 id="4-4-PCA-decorrelation"><a href="#4-4-PCA-decorrelation" class="headerlink" title="4.4. PCA-decorrelation"></a>4.4. PCA-decorrelation</h4><p>$z=W\cdot x$的神奇之处在于$Cov(z)=D$，即$z$的covariance是一个diagonal matrix，推导过程如下图所示。PCA可以让不同dimension之间的covariance变为0，即不同new feature之间是没有correlation的，这样做的好处是，<strong>减少feature之间的联系从而减少model所需的参数量</strong>。</p>
<p>如果你把原来的input data通过PCA之后再给其他model使用，那这些model就可以使用简单的形式，而无需考虑不同dimension之间类似$x_1\cdot x_2,x_3\cdot x_5^3,…$这些交叉项，此时model得到简化，参数量大大降低，相同的data量可以得到更好的训练结果，从而可以避免overfitting的发生。</p>
<p><img src="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8811%EF%BC%89Unsupervised-Learning-PCA/QQ图片20200725224431.png" style="zoom:67%;"></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/K-means/" rel="tag"># K-means</a>
              <a href="/tags/HAC/" rel="tag"># HAC</a>
              <a href="/tags/PCA/" rel="tag"># PCA</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/07/25/ML%E7%AC%94%E8%AE%B0%EF%BC%8810%EF%BC%89Semi-supervised-Learning/" rel="prev" title="ML笔记（10）Semi-supervised Learning">
      <i class="fa fa-chevron-left"></i> ML笔记（10）Semi-supervised Learning
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Unsupervised-Learning"><span class="nav-text">1.Unsupervised Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Clustering"><span class="nav-text">2. Clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-K-means"><span class="nav-text">2.1. K-means</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-HAC"><span class="nav-text">2.2. HAC</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Dimension-Reduction"><span class="nav-text">3. Dimension Reduction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Why-Dimension-Reduction-Help"><span class="nav-text">3.1 Why Dimension Reduction Help?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-How-to-do-Dimension-Reduction？"><span class="nav-text">3.2. How to do Dimension Reduction？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-PCA"><span class="nav-text">4. PCA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-PCA-for-1-Dimension"><span class="nav-text">4.1 PCA for 1-Dimension</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-PCA-for-n-D"><span class="nav-text">4.2 PCA for n-D</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-Lagrange-multiplier"><span class="nav-text">4.3. Lagrange multiplier</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-PCA-decorrelation"><span class="nav-text">4.4. PCA-decorrelation</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="nekomoon"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">nekomoon</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/nekomoon404" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;nekomoon404" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:nekomoon404@163.com" title="E-Mail → mailto:nekomoon404@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i>RSS</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2020.1.12 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">nekomoon</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">498k</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.0
  </div>

        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  


   
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20200112,"YYYYMMDD"));
    ages = ages.replace(/years?/, "年");
    ages = ages.replace(/months?/, "月");
    ages = ages.replace(/days?/, "天");
    ages = ages.replace(/hours?/, "小时");
    ages = ages.replace(/minutes?/, "分");
    ages = ages.replace(/seconds?/, "秒");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `我已在此等候你 ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


 
<script>
  var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });
</script>

</body>
</html>
